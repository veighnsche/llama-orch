#!/usr/bin/env bash
# llorch-remote: Remote testing CLI for llama-orch
# Created by: TEAM-018
# 
# A flexible CLI tool for managing remote builds, tests, and inference
# across different backends (CPU, CUDA, Metal) via SSH.

set -euo pipefail

VERSION="0.2.0"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_URL="${LLORCH_REPO_URL:-https://github.com/veighnsche/llama-orch.git}"
REMOTE_PATH="${LLORCH_REMOTE_PATH:-~/Projects/llama-orch}"
LOCAL_SCRIPTS_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}ℹ${NC} $*"
}

log_success() {
    echo -e "${GREEN}✓${NC} $*"
}

log_warn() {
    echo -e "${YELLOW}⚠${NC} $*"
}

log_error() {
    echo -e "${RED}✗${NC} $*" >&2
}

log_section() {
    echo ""
    echo -e "${CYAN}════════════════════════════════════════════════════════════════${NC}"
    echo -e "${CYAN}$*${NC}"
    echo -e "${CYAN}════════════════════════════════════════════════════════════════${NC}"
}

# SSH wrapper with fail-fast
ssh_exec() {
    local host="$1"
    shift
    ssh -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no "$host" "$@"
}

# Show usage
usage() {
    cat <<EOF
${CYAN}llorch-remote${NC} v${VERSION} - Remote testing CLI for llama-orch

${YELLOW}USAGE:${NC}
    llorch-remote <HOST> <ACTION> [BACKEND] [OPTIONS]

    ${GREEN}HOST${NC}        Remote host (e.g., mac.home.arpa, workstation.home.arpa)
    ${GREEN}ACTION${NC}      Action to perform (see below)
    ${GREEN}BACKEND${NC}     Backend type: cpu, cuda, metal (required for build/test/inference actions)

${YELLOW}ACTIONS:${NC}
    ${MAGENTA}Git Management (calls llorch-git remotely):${NC}
    status          Show git status and system info
    pull            Pull latest changes and update submodules
    sync            Sync to origin/main (hard reset)
    
    ${MAGENTA}Model Management (calls llorch-models remotely):${NC}
    models-list     List all models on remote
    models-download Download a model on remote
    models-info     Show model info on remote
    
    ${MAGENTA}Build & Test (requires BACKEND):${NC}
    build           Build backend binary (release mode)
    test            Run all tests for backend
    smoke           Run smoke tests only
    unit            Run unit tests only
    integration     Run integration tests only
    
    ${MAGENTA}Inference (requires BACKEND):${NC}
    inference       Generate a test story with actual model
    debug-inference Run inference with detailed logging
    logs            Show worker logs from last run
    info            Show backend and hardware info
    
    ${MAGENTA}Utilities:${NC}
    clean           Clean build artifacts
    all             Run: pull → build → test → inference

${YELLOW}OPTIONS:${NC}
    --model PATH    Model path for inference (default: \$LLORCH_TEST_MODEL_PATH)
    --port PORT     Port for worker (default: 8080)
    --device ID     Device ID for GPU backends (default: 0)
    --help, -h      Show this help message
    --version, -v   Show version

${YELLOW}ENVIRONMENT VARIABLES:${NC}
    LLORCH_REPO_URL         Repository URL (default: github.com/veighnsche/llama-orch)
    LLORCH_REMOTE_PATH      Remote path (default: ~/Projects/llama-orch)
    LLORCH_TEST_MODEL_PATH  Model path for inference tests

${YELLOW}EXAMPLES:${NC}
    # Git operations
    llorch-remote mac.home.arpa status
    llorch-remote mac.home.arpa pull
    llorch-remote mac.home.arpa sync

    # Model management
    llorch-remote mac.home.arpa models-list
    llorch-remote mac.home.arpa models-download tinyllama
    llorch-remote mac.home.arpa models-info tinyllama

    # Build and test
    llorch-remote workstation.home.arpa build cuda
    llorch-remote mac.home.arpa test metal

    # Inference
    llorch-remote mac.home.arpa inference metal
    llorch-remote mac.home.arpa debug-inference metal
    llorch-remote mac.home.arpa logs metal

    # Full workflow
    llorch-remote mac.home.arpa all metal

${YELLOW}BACKEND-SPECIFIC BINARIES:${NC}
    cpu   → llorch-cpu-candled
    cuda  → llorch-cuda-candled
    metal → llorch-metal-candled

EOF
}

# Parse arguments
if [[ $# -lt 2 ]]; then
    usage
    exit 1
fi

HOST="$1"
ACTION="$2"
shift 2

# Backend is optional - will be validated later if needed
BACKEND=""
if [[ $# -gt 0 ]] && [[ "$1" =~ ^(cpu|cuda|metal)$ ]]; then
    BACKEND="$1"
    shift
fi

# Parse options
MODEL_PATH="${LLORCH_TEST_MODEL_PATH:-}"
PORT="8080"
DEVICE_ID="0"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_PATH="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --device)
            DEVICE_ID="$2"
            shift 2
            ;;
        --help|-h)
            usage
            exit 0
            ;;
        --version|-v)
            echo "llorch-remote v${VERSION}"
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            usage
            exit 1
            ;;
    esac
done

# Actions that require backend
BACKEND_REQUIRED_ACTIONS=("build" "test" "smoke" "unit" "integration" "inference" "debug-inference" "logs" "info" "all")

# Parse model argument for model actions
MODEL_ARG=""
if [[ "$ACTION" =~ ^models- ]]; then
    if [[ -n "$BACKEND" ]]; then
        MODEL_ARG="$BACKEND"
        BACKEND=""
    fi
fi

# Validate backend if required
requires_backend() {
    local action="$1"
    for required_action in "${BACKEND_REQUIRED_ACTIONS[@]}"; do
        if [[ "$action" == "$required_action" ]]; then
            return 0
        fi
    done
    return 1
}

if requires_backend "$ACTION"; then
    if [[ -z "$BACKEND" ]]; then
        log_error "Action '$ACTION' requires a backend argument"
        log_error "Usage: llorch-remote $HOST $ACTION <cpu|cuda|metal> [OPTIONS]"
        exit 1
    fi
    case "$BACKEND" in
        cpu|cuda|metal)
            BINARY_NAME="llorch-${BACKEND}-candled"
            ;;
        *)
            log_error "Invalid backend: $BACKEND"
            log_error "Valid backends: cpu, cuda, metal"
            exit 1
            ;;
    esac
fi

# Action implementations
action_status() {
    log_section "📊 Repository Status"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-git status"
    
    echo ""
    echo "System Info:"
    echo "─────────────────────────────────────────────────────────"
    ssh_exec "$HOST" "uname -a"
    
    echo ""
    echo "Rust Toolchain:"
    echo "─────────────────────────────────────────────────────────"
    ssh_exec "$HOST" "rustc --version && cargo --version"
    
    log_success "Status retrieved"
}

action_pull() {
    log_section "🔄 Pulling Latest Changes"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-git pull"
    
    log_success "Pull complete"
}

action_sync() {
    log_section "🔄 Syncing to Origin/Main"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-git sync --force"
    
    log_success "Sync complete"
}

action_models_list() {
    log_section "📋 Listing Models"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-models list"
    
    log_success "List complete"
}

action_models_download() {
    if [[ -z "$MODEL_ARG" ]]; then
        log_error "Model name required"
        log_error "Usage: llorch-remote $HOST models-download <model>"
        exit 1
    fi
    
    log_section "📥 Downloading Model: $MODEL_ARG"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-models download $MODEL_ARG"
    
    log_success "Download complete"
}

action_models_info() {
    if [[ -z "$MODEL_ARG" ]]; then
        log_error "Model name required"
        log_error "Usage: llorch-remote $HOST models-info <model>"
        exit 1
    fi
    
    log_section "ℹ️  Model Info: $MODEL_ARG"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && ./scripts/llorch-models info $MODEL_ARG"
    
    log_success "Info retrieved"
}

action_build() {
    log_section "🔨 Building $BACKEND Backend"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Binary: $BINARY_NAME"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Building release binary..."
cargo build --release --features $BACKEND --bin $BINARY_NAME

echo ""
echo "Build Artifacts:"
echo "─────────────────────────────────────────────────────────"
ls -lh ../../target/release/$BINARY_NAME
EOF
    
    log_success "Build complete"
}

action_test() {
    log_section "🧪 Running Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running all tests..."
cargo test --features $BACKEND -- --nocapture || true
EOF
    
    log_success "Tests complete"
}

action_smoke() {
    log_section "💨 Running Smoke Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running smoke tests..."
cargo test --features $BACKEND --test team_009_smoke -- --nocapture || true
EOF
    
    log_success "Smoke tests complete"
}

action_unit() {
    log_section "🔬 Running Unit Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running unit tests..."
cargo test --features $BACKEND --lib -- --nocapture || true
EOF
    
    log_success "Unit tests complete"
}

action_integration() {
    log_section "🔗 Running Integration Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running integration tests..."
cargo test --features $BACKEND --test team_011_integration -- --nocapture || true
EOF
    
    log_success "Integration tests complete"
}


action_debug_inference() {
    log_section "🐛 Debug Inference Test ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Mode: Detailed logging enabled"
    
    local remote_model_path="${MODEL_PATH:-.test-models/tinyllama}"
    log_info "Model: $remote_model_path"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running debug inference with detailed logging..."
echo "─────────────────────────────────────────────────────────"

# Check if model exists
MODEL_FULL_PATH="\$HOME/Projects/llama-orch/$remote_model_path"
if [[ ! -d "\$MODEL_FULL_PATH" ]]; then
    echo "❌ Model directory not found: \$MODEL_FULL_PATH"
    echo ""
    echo "Download model first:"
    echo "  llorch-remote $HOST $BACKEND download-model"
    exit 1
fi

# Start worker with debug logging
echo "Starting $BACKEND worker with debug logging..."
RUST_LOG=debug ../../target/release/llorch-${BACKEND}-candled \\
    --worker-id debug-test \\
    --model "\$MODEL_FULL_PATH" \\
    --port 9876 \\
    --callback-url http://localhost:9999 > /tmp/${BACKEND}_debug.log 2>&1 &

WORKER_PID=\$!
echo "Worker PID: \$WORKER_PID"

# Wait for worker
echo "Waiting for worker to be ready..."
for i in {1..30}; do
    sleep 1
    if curl -s http://localhost:9876/health > /dev/null 2>&1; then
        echo "Worker ready after \${i}s"
        break
    fi
    if ! kill -0 \$WORKER_PID 2>/dev/null; then
        echo "❌ Worker died during startup"
        echo "Last 30 lines of log:"
        tail -30 /tmp/${BACKEND}_debug.log
        exit 1
    fi
done

# Send inference request
echo ""
echo "Sending inference request..."
curl -s -N -X POST http://localhost:9876/execute \\
    -H "Content-Type: application/json" \\
    -d '{"job_id":"debug-test","prompt":"Once upon a time","max_tokens":20,"temperature":0.7,"seed":42}' \\
    > /tmp/${BACKEND}_response.txt 2>&1 &

CURL_PID=\$!
sleep 10
kill \$CURL_PID 2>/dev/null || true
kill \$WORKER_PID 2>/dev/null || true
wait \$WORKER_PID 2>/dev/null || true

# Analyze results
echo ""
echo "═══════════════════════════════════════════════════════════"
if grep -q '"t":' /tmp/${BACKEND}_response.txt; then
    echo "✅ Inference SUCCESS"
    echo ""
    echo "Generated tokens:"
    grep '"t":' /tmp/${BACKEND}_response.txt | head -5
    echo ""
    echo "Full response saved to: /tmp/${BACKEND}_response.txt"
    echo "Full worker log saved to: /tmp/${BACKEND}_debug.log"
else
    echo "❌ Inference FAILED"
    echo ""
    echo "Response:"
    cat /tmp/${BACKEND}_response.txt
    echo ""
    echo "Worker log (last 30 lines):"
    tail -30 /tmp/${BACKEND}_debug.log
    echo ""
    echo "Full logs at: /tmp/${BACKEND}_debug.log"
    exit 1
fi
EOF
    
    log_success "Debug inference complete"
}

action_logs() {
    log_section "📋 Viewing Worker Logs ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail

echo "Recent worker logs:"
echo "═══════════════════════════════════════════════════════════"

if [[ -f /tmp/${BACKEND}_debug.log ]]; then
    echo "Debug log (last 50 lines):"
    echo "─────────────────────────────────────────────────────────"
    tail -50 /tmp/${BACKEND}_debug.log
    echo ""
fi

if [[ -f /tmp/${BACKEND}_response.txt ]]; then
    echo "Last response:"
    echo "─────────────────────────────────────────────────────────"
    cat /tmp/${BACKEND}_response.txt
    echo ""
fi

if [[ -f /tmp/worker.log ]]; then
    echo "Worker log (last 50 lines):"
    echo "─────────────────────────────────────────────────────────"
    tail -50 /tmp/worker.log
    echo ""
fi

if [[ ! -f /tmp/${BACKEND}_debug.log ]] && [[ ! -f /tmp/${BACKEND}_response.txt ]] && [[ ! -f /tmp/worker.log ]]; then
    echo "No logs found. Run inference first:"
    echo "  llorch-remote $HOST $BACKEND inference"
    echo "  llorch-remote $HOST $BACKEND debug-inference"
fi
EOF
    
    log_success "Logs retrieved"
}

action_inference() {
    log_section "📖 Running Inference Test ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    # Determine model path (expand ~ on remote) - use directory for SafeTensors
    local remote_model_path="${MODEL_PATH:-.test-models/tinyllama}"
    log_info "Model: $remote_model_path"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running inference with actual model..."
echo "─────────────────────────────────────────────────────────"

# Check if model exists (use absolute path from REMOTE_PATH)
MODEL_FULL_PATH="\$HOME/Projects/llama-orch/$remote_model_path"
if [[ ! -d "\$MODEL_FULL_PATH" ]]; then
    echo "❌ Model directory not found: \$MODEL_FULL_PATH"
    echo ""
    echo "Download model first:"
    echo "  llorch-remote $HOST $BACKEND download-model"
    exit 1
fi

# Start the worker in background with output redirection
echo "Starting $BACKEND worker..."
../../target/release/llorch-${BACKEND}-candled \\
    --worker-id test-inference \\
    --model "\$MODEL_FULL_PATH" \\
    --port 9876 \\
    --callback-url http://localhost:9999 > /tmp/worker.log 2>&1 &

WORKER_PID=\$!
echo "Worker PID: \$WORKER_PID"

# Wait for worker to be ready (longer for model loading)
echo "Waiting for worker to load model..."
for i in {1..30}; do
    sleep 1
    if curl -s http://localhost:9876/health > /dev/null 2>&1; then
        echo "Worker is ready!"
        break
    fi
    if ! kill -0 \$WORKER_PID 2>/dev/null; then
        echo "❌ Worker process died"
        echo "Last 20 lines of worker log:"
        tail -20 /tmp/worker.log
        exit 1
    fi
done

# Send inference request (SSE stream)
echo ""
echo "Sending inference request..."

# Capture SSE stream
curl -s -N -X POST http://localhost:9876/execute \\
    -H "Content-Type: application/json" \\
    -d '{"job_id":"test-story","prompt":"Once upon a time","max_tokens":50,"temperature":0.7,"seed":42}' \\
    > /tmp/sse_response.txt

# Kill worker
kill \$WORKER_PID 2>/dev/null || true
wait \$WORKER_PID 2>/dev/null || true

# Parse SSE response and extract tokens
if [[ -f /tmp/sse_response.txt ]] && [[ -s /tmp/sse_response.txt ]]; then
    echo ""
    echo "📖 Generated Story:"
    echo "─────────────────────────────────────────────────────────"
    
    # Extract token events and concatenate text
    STORY=\$(grep '^data:' /tmp/sse_response.txt | sed 's/^data: //' | jq -r 'select(.t != null) | .t' | tr -d '\\n')
    
    if [[ -n "\$STORY" ]]; then
        echo "Once upon a time\$STORY"
    else
        echo "❌ No tokens generated"
        echo "Raw SSE response:"
        cat /tmp/sse_response.txt
    fi
    
    echo ""
    echo "─────────────────────────────────────────────────────────"
    echo ""
    echo "Backend: $BACKEND"
    echo "Status: ✅ Inference test complete"
    rm -f /tmp/sse_response.txt
else
    echo "❌ No response from worker"
    echo "Worker log:"
    tail -30 /tmp/worker.log
    exit 1
fi
EOF
    
    log_success "Inference test complete"
}

action_clean() {
    log_section "🧹 Cleaning Build Artifacts"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH

echo "Cleaning Cargo build artifacts..."
cargo clean

echo "Removing target directory..."
rm -rf target/

echo "Done."
EOF
    
    log_success "Clean complete"
}

action_info() {
    log_section "ℹ️  Backend & Hardware Info"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail

echo "System Information:"
echo "─────────────────────────────────────────────────────────"
uname -a
echo ""

case "$BACKEND" in
    cuda)
        echo "CUDA Information:"
        echo "─────────────────────────────────────────────────────────"
        nvcc --version 2>/dev/null || echo "nvcc not found"
        echo ""
        nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv 2>/dev/null || echo "nvidia-smi not available"
        ;;
    metal)
        echo "macOS & Metal Information:"
        echo "─────────────────────────────────────────────────────────"
        sw_vers 2>/dev/null || echo "Not macOS"
        echo ""
        sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        echo ""
        system_profiler SPDisplaysDataType 2>/dev/null | grep -A 2 "Metal" || echo "Metal info not available"
        ;;
    cpu)
        echo "CPU Information:"
        echo "─────────────────────────────────────────────────────────"
        lscpu 2>/dev/null || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        ;;
esac

echo ""
echo "Rust Toolchain:"
echo "─────────────────────────────────────────────────────────"
rustc --version
cargo --version
EOF
    
    log_success "Info retrieved"
}

action_all() {
    log_section "🚀 Running Full Workflow"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Steps: pull → build → test → inference"
    
    action_pull
    action_build
    action_test
    action_inference
    
    log_section "✅ Full Workflow Complete"
}

# Execute action
case "$ACTION" in
    status)
        action_status
        ;;
    pull)
        action_pull
        ;;
    sync)
        action_sync
        ;;
    models-list)
        action_models_list
        ;;
    models-download)
        action_models_download
        ;;
    models-info)
        action_models_info
        ;;
    build)
        action_build
        ;;
    test)
        action_test
        ;;
    smoke)
        action_smoke
        ;;
    unit)
        action_unit
        ;;
    integration)
        action_integration
        ;;
    inference)
        action_inference
        ;;
    debug-inference)
        action_debug_inference
        ;;
    logs)
        action_logs
        ;;
    clean)
        action_clean
        ;;
    info)
        action_info
        ;;
    all)
        action_all
        ;;
    *)
        log_error "Unknown action: $ACTION"
        echo ""
        usage
        exit 1
        ;;
esac
