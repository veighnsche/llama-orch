#!/usr/bin/env bash
# llorch-remote: Remote testing CLI for llama-orch
# Created by: TEAM-018
# 
# A flexible CLI tool for managing remote builds, tests, and inference
# across different backends (CPU, CUDA, Metal) via SSH.

set -euo pipefail

VERSION="0.1.0"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_URL="${LLORCH_REPO_URL:-https://github.com/veighnsche/llama-orch.git}"
REMOTE_PATH="${LLORCH_REMOTE_PATH:-~/Projects/llama-orch}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}ℹ${NC} $*"
}

log_success() {
    echo -e "${GREEN}✓${NC} $*"
}

log_warn() {
    echo -e "${YELLOW}⚠${NC} $*"
}

log_error() {
    echo -e "${RED}✗${NC} $*" >&2
}

log_section() {
    echo ""
    echo -e "${CYAN}════════════════════════════════════════════════════════════════${NC}"
    echo -e "${CYAN}$*${NC}"
    echo -e "${CYAN}════════════════════════════════════════════════════════════════${NC}"
}

# SSH wrapper with fail-fast
ssh_exec() {
    local host="$1"
    shift
    ssh -o BatchMode=yes -o ConnectTimeout=10 "$host" "$@"
}

# Show usage
usage() {
    cat <<EOF
${CYAN}llorch-remote${NC} v${VERSION} - Remote testing CLI for llama-orch

${YELLOW}USAGE:${NC}
    llorch-remote <HOST> <BACKEND> <ACTION> [OPTIONS]

    ${GREEN}HOST${NC}        Remote host (e.g., mac.home.arpa, workstation.home.arpa)
    ${GREEN}BACKEND${NC}     Backend type: cpu, cuda, metal
    ${GREEN}ACTION${NC}      Action to perform (see below)

${YELLOW}ACTIONS:${NC}
    ${MAGENT### Actions

| Action | Description |
|--------|-------------|
| `clone` | Clone repository to remote host |
| `status` | Show git status and system info |
| `build` | Build backend binary (release mode) |
| `test` | Run all tests for backend |
| `smoke` | Run smoke tests only |
| `unit`        Run unit tests only |
| `integration`    Run integration tests only |
| `download-model` | Download test model (tinyllama by default) |
| `inference`   | Generate a test story with actual model |
| `clean`       | Clean build artifacts |
| `info` | Show backend and hardware info |
| `all` | Run: pull → build → test → download-model → inference |

${YELLOW}OPTIONS:${NC}
    --model PATH    Model path for inference (default: \$LLORCH_TEST_MODEL_PATH)
    --port PORT     Port for worker (default: 8080)
    --device ID     Device ID for GPU backends (default: 0)
    --help, -h      Show this help message
    --version, -v   Show version

${YELLOW}ENVIRONMENT VARIABLES:${NC}
    LLORCH_REPO_URL         Repository URL (default: github.com/veighnsche/llama-orch)
    LLORCH_REMOTE_PATH      Remote path (default: ~/Projects/llama-orch)
    LLORCH_TEST_MODEL_PATH  Model path for inference tests

${YELLOW}EXAMPLES:${NC}
    # Clone repo to Mac
    llorch-remote mac.home.arpa metal clone

    # Build CUDA backend on workstation
    llorch-remote workstation.home.arpa cuda build

    # Run tests on Mac Metal backend
    llorch-remote mac.home.arpa metal test

    # Generate test story (inference)
    llorch-remote mac.home.arpa metal inference --model /path/to/model

    # Full workflow: pull, build, test, inference
    llorch-remote mac.home.arpa metal all

    # Check system info
    llorch-remote mac.home.arpa metal info

${YELLOW}BACKEND-SPECIFIC BINARIES:${NC}
    cpu   → llorch-cpu-candled
    cuda  → llorch-cuda-candled
    metal → llorch-metal-candled

EOF
}

# Parse arguments
if [[ $# -lt 3 ]]; then
    usage
    exit 1
fi

HOST="$1"
BACKEND="$2"
ACTION="$3"
shift 3

# Parse options
MODEL_PATH="${LLORCH_TEST_MODEL_PATH:-}"
PORT="8080"
DEVICE_ID="0"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_PATH="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --device)
            DEVICE_ID="$2"
            shift 2
            ;;
        --help|-h)
            usage
            exit 0
            ;;
        --version|-v)
            echo "llorch-remote v${VERSION}"
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            usage
            exit 1
            ;;
    esac
done

# Validate backend
case "$BACKEND" in
    cpu|cuda|metal)
        BINARY_NAME="llorch-${BACKEND}-candled"
        ;;
    *)
        log_error "Invalid backend: $BACKEND"
        log_error "Valid backends: cpu, cuda, metal"
        exit 1
        ;;
esac

# Action implementations
action_clone() {
    log_section "📥 Cloning Repository"
    log_info "Host: $HOST"
    log_info "Repo: $REPO_URL"
    log_info "Path: $REMOTE_PATH"
    
    ssh_exec "$HOST" "git clone $REPO_URL $REMOTE_PATH"
    
    log_success "Repository cloned successfully"
}

action_pull() {
    log_section "🔄 Pulling Latest Changes"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && git fetch --all && git reset --hard origin/main"
    
    log_success "Repository updated to latest main"
}

action_status() {
    log_section "📊 Repository Status"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH

echo "Git Status:"
echo "─────────────────────────────────────────────────────────"
git log -1 --oneline
echo "Branch: \$(git branch --show-current)"
echo "Remote: \$(git remote get-url origin)"
echo ""

echo "System Info:"
echo "─────────────────────────────────────────────────────────"
uname -a
echo ""

echo "Rust Toolchain:"
echo "─────────────────────────────────────────────────────────"
rustc --version
cargo --version
EOF
    
    log_success "Status retrieved"
}

action_build() {
    log_section "🔨 Building $BACKEND Backend"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Binary: $BINARY_NAME"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Building release binary..."
cargo build --release --features $BACKEND --bin $BINARY_NAME

echo ""
echo "Build Artifacts:"
echo "─────────────────────────────────────────────────────────"
ls -lh ../../target/release/$BINARY_NAME
EOF
    
    log_success "Build complete"
}

action_test() {
    log_section "🧪 Running Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running all tests..."
cargo test --features $BACKEND -- --nocapture || true
EOF
    
    log_success "Tests complete"
}

action_smoke() {
    log_section "💨 Running Smoke Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running smoke tests..."
cargo test --features $BACKEND --test team_009_smoke -- --nocapture || true
EOF
    
    log_success "Smoke tests complete"
}

action_unit() {
    log_section "🔬 Running Unit Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running unit tests..."
cargo test --features $BACKEND --lib -- --nocapture || true
EOF
    
    log_success "Unit tests complete"
}

action_integration() {
    log_section "🔗 Running Integration Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running integration tests..."
cargo test --features $BACKEND --test team_011_integration -- --nocapture || true
EOF
    
    log_success "Integration tests complete"
}

action_download_model() {
    log_section "📥 Downloading Test Model"
    log_info "Host: $HOST"
    log_info "Model: TinyLlama 1.1B Chat (Q4_K_M)"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH

# Run the download script
bash .docs/testing/download_tinyllama.sh
EOF
    
    log_success "Model download complete"
}

action_inference() {
    log_section "📖 Running Inference Test ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    # Determine model path (expand ~ on remote) - use directory for SafeTensors
    local remote_model_path="${MODEL_PATH:-.test-models/tinyllama}"
    log_info "Model: $remote_model_path"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running inference with actual model..."
echo "─────────────────────────────────────────────────────────"

# Check if model exists (use absolute path from REMOTE_PATH)
MODEL_FULL_PATH="\$HOME/Projects/llama-orch/$remote_model_path"
if [[ ! -f "\$MODEL_FULL_PATH" ]]; then
    echo "❌ Model not found: \$MODEL_FULL_PATH"
    echo ""
    echo "Download model first:"
    echo "  llorch-remote $HOST $BACKEND download-model"
    exit 1
fi

# Start the worker in background
echo "Starting $BACKEND worker..."
../../target/release/llorch-${BACKEND}-candled \\
    --worker-id test-inference \\
    --model "\$MODEL_FULL_PATH" \\
    --port 9876 \\
    --callback-url http://localhost:9999 &

WORKER_PID=\$!
echo "Worker PID: \$WORKER_PID"

# Wait for worker to be ready (longer for model loading)
echo "Waiting for worker to load model..."
sleep 10

# Send inference request
echo ""
echo "Sending inference request..."
RESPONSE=\$(curl -s -X POST http://localhost:9876/v1/execute \\
    -H "Content-Type: application/json" \\
    -d '{
        "job_id": "test-story",
        "prompt": "Once upon a time",
        "max_tokens": 50,
        "temperature": 0.7,
        "seed": 42
    }' || echo "")

# Kill worker
kill \$WORKER_PID 2>/dev/null || true
wait \$WORKER_PID 2>/dev/null || true

# Parse and display response
if [[ -n "\$RESPONSE" ]]; then
    echo ""
    echo "📖 Generated Story:"
    echo "─────────────────────────────────────────────────────────"
    echo "\$RESPONSE" | jq -r '.text // .error // "No text in response"' 2>/dev/null || echo "\$RESPONSE"
    echo "─────────────────────────────────────────────────────────"
    echo ""
    echo "Backend: $BACKEND"
    echo "Status: ✅ Inference test complete"
else
    echo "❌ No response from worker"
    exit 1
fi
EOF
    
    log_success "Inference test complete"
}

action_clean() {
    log_section "🧹 Cleaning Build Artifacts"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH

echo "Cleaning Cargo build artifacts..."
cargo clean

echo "Removing target directory..."
rm -rf target/

echo "Done."
EOF
    
    log_success "Clean complete"
}

action_info() {
    log_section "ℹ️  Backend & Hardware Info"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail

echo "System Information:"
echo "─────────────────────────────────────────────────────────"
uname -a
echo ""

case "$BACKEND" in
    cuda)
        echo "CUDA Information:"
        echo "─────────────────────────────────────────────────────────"
        nvcc --version 2>/dev/null || echo "nvcc not found"
        echo ""
        nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv 2>/dev/null || echo "nvidia-smi not available"
        ;;
    metal)
        echo "macOS & Metal Information:"
        echo "─────────────────────────────────────────────────────────"
        sw_vers 2>/dev/null || echo "Not macOS"
        echo ""
        sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        echo ""
        system_profiler SPDisplaysDataType 2>/dev/null | grep -A 2 "Metal" || echo "Metal info not available"
        ;;
    cpu)
        echo "CPU Information:"
        echo "─────────────────────────────────────────────────────────"
        lscpu 2>/dev/null || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        ;;
esac

echo ""
echo "Rust Toolchain:"
echo "─────────────────────────────────────────────────────────"
rustc --version
cargo --version
EOF
    
    log_success "Info retrieved"
}

action_all() {
    log_section "🚀 Running Full Workflow"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Steps: pull → build → test → download-model → inference"
    
    action_pull
    action_build
    action_test
    action_download_model
    action_inference
    
    log_section "✅ Full Workflow Complete"
}

# Execute action
case "$ACTION" in
    clone)
        action_clone
        ;;
    pull)
        action_pull
        ;;
    status)
        action_status
        ;;
    build)
        action_build
        ;;
    test)
        action_test
        ;;
    smoke)
        action_smoke
        ;;
    unit)
        action_unit
        ;;
    integration)
        action_integration
        ;;
    download-model)
        action_download_model
        ;;
    inference)
        action_inference
        ;;
    clean)
        action_clean
        ;;
    info)
        action_info
        ;;
    all)
        action_all
        ;;
    *)
        log_error "Unknown action: $ACTION"
        echo ""
        usage
        exit 1
        ;;
esac
