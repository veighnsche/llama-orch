#!/usr/bin/env bash
# llorch-remote: Remote testing CLI for llama-orch
# Created by: TEAM-018
# 
# A flexible CLI tool for managing remote builds, tests, and inference
# across different backends (CPU, CUDA, Metal) via SSH.

set -euo pipefail

VERSION="0.1.0"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_URL="${LLORCH_REPO_URL:-https://github.com/veighnsche/llama-orch.git}"
REMOTE_PATH="${LLORCH_REMOTE_PATH:-~/Projects/llama-orch}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${BLUE}â„¹${NC} $*"
}

log_success() {
    echo -e "${GREEN}âœ“${NC} $*"
}

log_warn() {
    echo -e "${YELLOW}âš ${NC} $*"
}

log_error() {
    echo -e "${RED}âœ—${NC} $*" >&2
}

log_section() {
    echo ""
    echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${CYAN}$*${NC}"
    echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
}

# SSH wrapper with fail-fast
ssh_exec() {
    local host="$1"
    shift
    ssh -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no "$host" "$@"
}

# Show usage
usage() {
    cat <<EOF
${CYAN}llorch-remote${NC} v${VERSION} - Remote testing CLI for llama-orch

${YELLOW}USAGE:${NC}
    llorch-remote <HOST> <BACKEND> <ACTION> [OPTIONS]

    ${GREEN}HOST${NC}        Remote host (e.g., mac.home.arpa, workstation.home.arpa)
    ${GREEN}BACKEND${NC}     Backend type: cpu, cuda, metal
    ${GREEN}ACTION${NC}      Action to perform (see below)

${YELLOW}ACTIONS:${NC}
    ${MAGENT### Actions

| Action | Description |
|--------|-------------|
| `clone` | Clone repository to remote host |
| `pull` | Pull latest changes from origin/main |
| `status` | Show git status and system info |
| `build` | Build backend binary (release mode) |
| `test` | Run all tests for backend |
| `smoke` | Run smoke tests only |
| `unit` | Run unit tests only |
| `integration` | Run integration tests only |
| `download-model` | Download test model (tinyllama by default) |
| `inference` | Generate a test story with actual model |
| `debug-inference` | Run inference with detailed logging and diagnostics |
| `logs` | Show worker logs from last run |
| `clean` | Clean build artifacts |
| `info` | Show backend and hardware info |
| `all` | Run: pull â†’ build â†’ test â†’ download-model â†’ inference |

${YELLOW}OPTIONS:${NC}
    --model PATH    Model path for inference (default: \$LLORCH_TEST_MODEL_PATH)
    --port PORT     Port for worker (default: 8080)
    --device ID     Device ID for GPU backends (default: 0)
    --help, -h      Show this help message
    --version, -v   Show version

${YELLOW}ENVIRONMENT VARIABLES:${NC}
    LLORCH_REPO_URL         Repository URL (default: github.com/veighnsche/llama-orch)
    LLORCH_REMOTE_PATH      Remote path (default: ~/Projects/llama-orch)
    LLORCH_TEST_MODEL_PATH  Model path for inference tests

${YELLOW}EXAMPLES:${NC}
    # Clone repo to Mac
    llorch-remote mac.home.arpa metal clone

    # Build CUDA backend on workstation
    llorch-remote workstation.home.arpa cuda build

    # Run tests on Mac Metal backend
    llorch-remote mac.home.arpa metal test

    # Generate test story (inference)
    llorch-remote mac.home.arpa metal inference --model /path/to/model

    # Full workflow: pull, build, test, inference
    llorch-remote mac.home.arpa metal all

    # Debug inference with detailed logs
    llorch-remote mac.home.arpa metal debug-inference

    # Check system info
    llorch-remote mac.home.arpa metal info

    # View logs from last run
    llorch-remote mac.home.arpa metal logs

${YELLOW}BACKEND-SPECIFIC BINARIES:${NC}
    cpu   â†’ llorch-cpu-candled
    cuda  â†’ llorch-cuda-candled
    metal â†’ llorch-metal-candled

EOF
}

# Parse arguments
if [[ $# -lt 3 ]]; then
    usage
    exit 1
fi

HOST="$1"
BACKEND="$2"
ACTION="$3"
shift 3

# Parse options
MODEL_PATH="${LLORCH_TEST_MODEL_PATH:-}"
PORT="8080"
DEVICE_ID="0"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_PATH="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --device)
            DEVICE_ID="$2"
            shift 2
            ;;
        --help|-h)
            usage
            exit 0
            ;;
        --version|-v)
            echo "llorch-remote v${VERSION}"
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            usage
            exit 1
            ;;
    esac
done

# Validate backend
case "$BACKEND" in
    cpu|cuda|metal)
        BINARY_NAME="llorch-${BACKEND}-candled"
        ;;
    *)
        log_error "Invalid backend: $BACKEND"
        log_error "Valid backends: cpu, cuda, metal"
        exit 1
        ;;
esac

# Action implementations
action_clone() {
    log_section "ğŸ“¥ Cloning Repository"
    log_info "Host: $HOST"
    log_info "Repo: $REPO_URL"
    log_info "Path: $REMOTE_PATH"
    
    ssh_exec "$HOST" "git clone $REPO_URL $REMOTE_PATH"
    
    log_success "Repository cloned successfully"
}

action_pull() {
    log_section "ğŸ”„ Pulling Latest Changes"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" "cd $REMOTE_PATH && git fetch --all && git reset --hard origin/main"
    
    log_success "Repository updated to latest main"
}

action_status() {
    log_section "ğŸ“Š Repository Status"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH

echo "Git Status:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
git log -1 --oneline
echo "Branch: \$(git branch --show-current)"
echo "Remote: \$(git remote get-url origin)"
echo ""

echo "System Info:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
uname -a
echo ""

echo "Rust Toolchain:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
rustc --version
cargo --version
EOF
    
    log_success "Status retrieved"
}

action_build() {
    log_section "ğŸ”¨ Building $BACKEND Backend"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Binary: $BINARY_NAME"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Building release binary..."
cargo build --release --features $BACKEND --bin $BINARY_NAME

echo ""
echo "Build Artifacts:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
ls -lh ../../target/release/$BINARY_NAME
EOF
    
    log_success "Build complete"
}

action_test() {
    log_section "ğŸ§ª Running Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running all tests..."
cargo test --features $BACKEND -- --nocapture || true
EOF
    
    log_success "Tests complete"
}

action_smoke() {
    log_section "ğŸ’¨ Running Smoke Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running smoke tests..."
cargo test --features $BACKEND --test team_009_smoke -- --nocapture || true
EOF
    
    log_success "Smoke tests complete"
}

action_unit() {
    log_section "ğŸ”¬ Running Unit Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running unit tests..."
cargo test --features $BACKEND --lib -- --nocapture || true
EOF
    
    log_success "Unit tests complete"
}

action_integration() {
    log_section "ğŸ”— Running Integration Tests ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH/bin/llorch-candled

echo "Running integration tests..."
cargo test --features $BACKEND --test team_011_integration -- --nocapture || true
EOF
    
    log_success "Integration tests complete"
}

action_download_model() {
    log_section "ğŸ“¥ Downloading Test Model"
    log_info "Host: $HOST"
    log_info "Model: TinyLlama 1.1B Chat (Q4_K_M)"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH

# Run the download script
bash .docs/testing/download_tinyllama.sh
EOF
    
    log_success "Model download complete"
}

action_debug_inference() {
    log_section "ğŸ› Debug Inference Test ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Mode: Detailed logging enabled"
    
    local remote_model_path="${MODEL_PATH:-.test-models/tinyllama}"
    log_info "Model: $remote_model_path"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running debug inference with detailed logging..."
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check if model exists
MODEL_FULL_PATH="\$HOME/Projects/llama-orch/$remote_model_path"
if [[ ! -d "\$MODEL_FULL_PATH" ]]; then
    echo "âŒ Model directory not found: \$MODEL_FULL_PATH"
    echo ""
    echo "Download model first:"
    echo "  llorch-remote $HOST $BACKEND download-model"
    exit 1
fi

# Start worker with debug logging
echo "Starting $BACKEND worker with debug logging..."
RUST_LOG=debug ../../target/release/llorch-${BACKEND}-candled \\
    --worker-id debug-test \\
    --model "\$MODEL_FULL_PATH" \\
    --port 9876 \\
    --callback-url http://localhost:9999 > /tmp/${BACKEND}_debug.log 2>&1 &

WORKER_PID=\$!
echo "Worker PID: \$WORKER_PID"

# Wait for worker
echo "Waiting for worker to be ready..."
for i in {1..30}; do
    sleep 1
    if curl -s http://localhost:9876/health > /dev/null 2>&1; then
        echo "Worker ready after \${i}s"
        break
    fi
    if ! kill -0 \$WORKER_PID 2>/dev/null; then
        echo "âŒ Worker died during startup"
        echo "Last 30 lines of log:"
        tail -30 /tmp/${BACKEND}_debug.log
        exit 1
    fi
done

# Send inference request
echo ""
echo "Sending inference request..."
curl -s -N -X POST http://localhost:9876/execute \\
    -H "Content-Type: application/json" \\
    -d '{"job_id":"debug-test","prompt":"Once upon a time","max_tokens":20,"temperature":0.7,"seed":42}' \\
    > /tmp/${BACKEND}_response.txt 2>&1 &

CURL_PID=\$!
sleep 10
kill \$CURL_PID 2>/dev/null || true
kill \$WORKER_PID 2>/dev/null || true
wait \$WORKER_PID 2>/dev/null || true

# Analyze results
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
if grep -q '"t":' /tmp/${BACKEND}_response.txt; then
    echo "âœ… Inference SUCCESS"
    echo ""
    echo "Generated tokens:"
    grep '"t":' /tmp/${BACKEND}_response.txt | head -5
    echo ""
    echo "Full response saved to: /tmp/${BACKEND}_response.txt"
    echo "Full worker log saved to: /tmp/${BACKEND}_debug.log"
else
    echo "âŒ Inference FAILED"
    echo ""
    echo "Response:"
    cat /tmp/${BACKEND}_response.txt
    echo ""
    echo "Worker log (last 30 lines):"
    tail -30 /tmp/${BACKEND}_debug.log
    echo ""
    echo "Full logs at: /tmp/${BACKEND}_debug.log"
    exit 1
fi
EOF
    
    log_success "Debug inference complete"
}

action_logs() {
    log_section "ğŸ“‹ Viewing Worker Logs ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail

echo "Recent worker logs:"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

if [[ -f /tmp/${BACKEND}_debug.log ]]; then
    echo "Debug log (last 50 lines):"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    tail -50 /tmp/${BACKEND}_debug.log
    echo ""
fi

if [[ -f /tmp/${BACKEND}_response.txt ]]; then
    echo "Last response:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    cat /tmp/${BACKEND}_response.txt
    echo ""
fi

if [[ -f /tmp/worker.log ]]; then
    echo "Worker log (last 50 lines):"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    tail -50 /tmp/worker.log
    echo ""
fi

if [[ ! -f /tmp/${BACKEND}_debug.log ]] && [[ ! -f /tmp/${BACKEND}_response.txt ]] && [[ ! -f /tmp/worker.log ]]; then
    echo "No logs found. Run inference first:"
    echo "  llorch-remote $HOST $BACKEND inference"
    echo "  llorch-remote $HOST $BACKEND debug-inference"
fi
EOF
    
    log_success "Logs retrieved"
}

action_inference() {
    log_section "ğŸ“– Running Inference Test ($BACKEND)"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    # Determine model path (expand ~ on remote) - use directory for SafeTensors
    local remote_model_path="${MODEL_PATH:-.test-models/tinyllama}"
    log_info "Model: $remote_model_path"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
cd $REMOTE_PATH/bin/llorch-candled

echo "Running inference with actual model..."
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

# Check if model exists (use absolute path from REMOTE_PATH)
MODEL_FULL_PATH="\$HOME/Projects/llama-orch/$remote_model_path"
if [[ ! -d "\$MODEL_FULL_PATH" ]]; then
    echo "âŒ Model directory not found: \$MODEL_FULL_PATH"
    echo ""
    echo "Download model first:"
    echo "  llorch-remote $HOST $BACKEND download-model"
    exit 1
fi

# Start the worker in background with output redirection
echo "Starting $BACKEND worker..."
../../target/release/llorch-${BACKEND}-candled \\
    --worker-id test-inference \\
    --model "\$MODEL_FULL_PATH" \\
    --port 9876 \\
    --callback-url http://localhost:9999 > /tmp/worker.log 2>&1 &

WORKER_PID=\$!
echo "Worker PID: \$WORKER_PID"

# Wait for worker to be ready (longer for model loading)
echo "Waiting for worker to load model..."
for i in {1..30}; do
    sleep 1
    if curl -s http://localhost:9876/health > /dev/null 2>&1; then
        echo "Worker is ready!"
        break
    fi
    if ! kill -0 \$WORKER_PID 2>/dev/null; then
        echo "âŒ Worker process died"
        echo "Last 20 lines of worker log:"
        tail -20 /tmp/worker.log
        exit 1
    fi
done

# Send inference request (SSE stream)
echo ""
echo "Sending inference request..."

# Capture SSE stream
curl -s -N -X POST http://localhost:9876/execute \\
    -H "Content-Type: application/json" \\
    -d '{"job_id":"test-story","prompt":"Once upon a time","max_tokens":50,"temperature":0.7,"seed":42}' \\
    > /tmp/sse_response.txt

# Kill worker
kill \$WORKER_PID 2>/dev/null || true
wait \$WORKER_PID 2>/dev/null || true

# Parse SSE response and extract tokens
if [[ -f /tmp/sse_response.txt ]] && [[ -s /tmp/sse_response.txt ]]; then
    echo ""
    echo "ğŸ“– Generated Story:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    
    # Extract token events and concatenate text
    STORY=\$(grep '^data:' /tmp/sse_response.txt | sed 's/^data: //' | jq -r 'select(.t != null) | .t' | tr -d '\\n')
    
    if [[ -n "\$STORY" ]]; then
        echo "Once upon a time\$STORY"
    else
        echo "âŒ No tokens generated"
        echo "Raw SSE response:"
        cat /tmp/sse_response.txt
    fi
    
    echo ""
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    echo ""
    echo "Backend: $BACKEND"
    echo "Status: âœ… Inference test complete"
    rm -f /tmp/sse_response.txt
else
    echo "âŒ No response from worker"
    echo "Worker log:"
    tail -30 /tmp/worker.log
    exit 1
fi
EOF
    
    log_success "Inference test complete"
}

action_clean() {
    log_section "ğŸ§¹ Cleaning Build Artifacts"
    log_info "Host: $HOST"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail
export PATH="\$HOME/.cargo/bin:\$PATH"
cd $REMOTE_PATH

echo "Cleaning Cargo build artifacts..."
cargo clean

echo "Removing target directory..."
rm -rf target/

echo "Done."
EOF
    
    log_success "Clean complete"
}

action_info() {
    log_section "â„¹ï¸  Backend & Hardware Info"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    
    ssh_exec "$HOST" bash <<EOF
set -euo pipefail

echo "System Information:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
uname -a
echo ""

case "$BACKEND" in
    cuda)
        echo "CUDA Information:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        nvcc --version 2>/dev/null || echo "nvcc not found"
        echo ""
        nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv 2>/dev/null || echo "nvidia-smi not available"
        ;;
    metal)
        echo "macOS & Metal Information:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        sw_vers 2>/dev/null || echo "Not macOS"
        echo ""
        sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        echo ""
        system_profiler SPDisplaysDataType 2>/dev/null | grep -A 2 "Metal" || echo "Metal info not available"
        ;;
    cpu)
        echo "CPU Information:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        lscpu 2>/dev/null || sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "CPU info not available"
        ;;
esac

echo ""
echo "Rust Toolchain:"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
rustc --version
cargo --version
EOF
    
    log_success "Info retrieved"
}

action_all() {
    log_section "ğŸš€ Running Full Workflow"
    log_info "Host: $HOST"
    log_info "Backend: $BACKEND"
    log_info "Steps: pull â†’ build â†’ test â†’ download-model â†’ inference"
    
    action_pull
    action_build
    action_test
    action_download_model
    action_inference
    
    log_section "âœ… Full Workflow Complete"
}

# Execute action
case "$ACTION" in
    clone)
        action_clone
        ;;
    pull)
        action_pull
        ;;
    status)
        action_status
        ;;
    build)
        action_build
        ;;
    test)
        action_test
        ;;
    smoke)
        action_smoke
        ;;
    unit)
        action_unit
        ;;
    integration)
        action_integration
        ;;
    download-model)
        action_download_model
        ;;
    inference)
        action_inference
        ;;
    debug-inference)
        action_debug_inference
        ;;
    logs)
        action_logs
        ;;
    clean)
        action_clean
        ;;
    info)
        action_info
        ;;
    all)
        action_all
        ;;
    *)
        log_error "Unknown action: $ACTION"
        echo ""
        usage
        exit 1
        ;;
esac
