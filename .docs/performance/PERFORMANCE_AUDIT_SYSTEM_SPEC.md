# Performance Audit: System Specification (00_llama-orch.md)

**Audit Date**: 2025-10-03  
**Auditor**: deadline-propagation team (obsessive timekeepers)  
**Scope**: Complete performance analysis of llama-orch M0 architecture  
**Status**: ARCHITECTURAL CONCERNS IDENTIFIED

---

## Executive Summary

We conducted a comprehensive performance audit of the llama-orch M0 system specification (`bin/.specs/00_llama-orch.md`) focusing on worker-orcd architecture and critical hot paths. We identified **3 CRITICAL architectural issues**, **5 HIGH priority optimizations**, and **4 MEDIUM priority improvements** that should be addressed before M0 implementation.

**Key Findings**:
- Worker-orcd lacks explicit deadline propagation architecture
- SSE streaming may introduce buffering latency (no zero-copy requirement)
- Model loading path has no parallelization strategy
- Client disconnect detection is unspecified (potential for wasted GPU cycles)
- No explicit CUDA async stream usage for I/O overlap
- Cancellation checking interval not specified (affects abort latency)
- Health endpoint may perform expensive CUDA calls (needs caching)
- No performance testing infrastructure specified

**Our Recommendation**: Address CRITICAL and HIGH priority items before M0 implementation. These are not behavior changes‚Äîthey are architectural clarifications that prevent performance bottlenecks.

---

## Critical Path Latency Analysis

### Request Flow: Client ‚Üí Orchestrator ‚Üí Worker (Target: <180ms)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ADMISSION PHASE (Target: <10ms)                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ HTTP receive               ~1ms                                  ‚îÇ
‚îÇ Model lookup               ~1ms    [HIGH] Add deadline check     ‚îÇ
‚îÇ Context length check       ~1ms                                  ‚îÇ
‚îÇ Token budget check         ~1ms                                  ‚îÇ
‚îÇ Queue enqueue              ~5ms    [MEDIUM] SQLite write latency ‚îÇ
‚îÇ Response 202               ~1ms                                  ‚îÇ
‚îÇ TOTAL                      ~10ms                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SCHEDULING PHASE (Target: <50ms)                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Dequeue job                ~1ms                                  ‚îÇ
‚îÇ Pool state query           ~5ms                                  ‚îÇ
‚îÇ Rhai scheduler exec        ~30ms   [HIGH] Timeout needed        ‚îÇ
‚îÇ Worker selection           ~5ms                                  ‚îÇ
‚îÇ Dispatch command           ~9ms                                  ‚îÇ
‚îÇ TOTAL                      ~50ms                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DISPATCH PHASE (Target: <20ms)                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ HTTP connection            ~10ms   [CRITICAL] Need pooling      ‚îÇ
‚îÇ POST /execute              ~5ms                                  ‚îÇ
‚îÇ SSE stream establish       ~5ms                                  ‚îÇ
‚îÇ TOTAL                      ~20ms                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ INFERENCE PHASE (Target: <100ms first token)                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Request parse              ~1ms                                  ‚îÇ
‚îÇ Prompt encode              ~10ms                                 ‚îÇ
‚îÇ First token generate       ~80ms                                 ‚îÇ
‚îÇ SSE emit                   ~5ms    [CRITICAL] Zero-copy needed  ‚îÇ
‚îÇ Network transmit           ~4ms                                  ‚îÇ
‚îÇ TOTAL                      ~100ms                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

TOTAL END-TO-END: ~180ms (admission ‚Üí first token)
```

### Worker-orcd Hot Paths (M0 Focus)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STARTUP PATH (Target: <60s cold, <10s hot)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Process spawn              ~100ms                                ‚îÇ
‚îÇ CUDA context init          ~1s     [MEDIUM] Parallelize w/ HTTP ‚îÇ
‚îÇ Model load to VRAM         ~50-58s [HIGH] Use mmap for hot-load ‚îÇ
‚îÇ HTTP server start          ~100ms  [MEDIUM] Parallel w/ CUDA    ‚îÇ
‚îÇ Ready callback             ~50ms                                 ‚îÇ
‚îÇ TOTAL (cold)               ~60s                                  ‚îÇ
‚îÇ TOTAL (hot w/ mmap)        ~6-11s  ‚Üê 10x FASTER!                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ INFERENCE LOOP (Target: 10-50ms per token)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Generate token             ~10-40ms (model dependent)            ‚îÇ
‚îÇ Check cancellation         ~1Œºs    [HIGH] Every 10 tokens       ‚îÇ
‚îÇ Check client connected     ~1ms    [CRITICAL] Every 10 tokens   ‚îÇ
‚îÇ Check deadline             ~1Œºs    [CRITICAL] Every 10 tokens   ‚îÇ
‚îÇ SSE emit                   ~5ms    [CRITICAL] Zero-copy needed  ‚îÇ
‚îÇ TOTAL per token            ~15-50ms                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CANCELLATION PATH (Target: <200ms)                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Receive POST /cancel       ~5ms                                  ‚îÇ
‚îÇ Set cancel flag            ~1Œºs                                  ‚îÇ
‚îÇ Detect in inference loop   ~100ms  [HIGH] Check every 10 tokens ‚îÇ
‚îÇ Abort CUDA kernel          ~50ms   [HIGH] cudaStreamSynchronize ‚îÇ
‚îÇ Free VRAM buffers          ~20ms                                 ‚îÇ
‚îÇ Emit SSE error             ~10ms                                 ‚îÇ
‚îÇ Return 202                 ~5ms                                  ‚îÇ
‚îÇ TOTAL                      ~190ms                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HEALTH CHECK PATH (Target: <10ms p99)                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Receive GET /health        ~1ms                                  ‚îÇ
‚îÇ Read cached state          ~1Œºs    [HIGH] No CUDA calls!        ‚îÇ
‚îÇ Return 200                 ~1ms                                  ‚îÇ
‚îÇ TOTAL                      ~2ms    ‚Üê FAST!                       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ BAD (if not cached):                                            ‚îÇ
‚îÇ cudaMemGetInfo()           ~1-5ms  ‚Üê BLOCKS GPU!                ‚îÇ
‚îÇ TOTAL (bad)                ~6ms    ‚Üê TOO SLOW                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Deadline Propagation (Missing - CRITICAL)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DEADLINE FLOW (Not Currently Specified!)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ Client sets deadline:      X-Deadline: 2025-10-03T19:30:00Z    ‚îÇ
‚îÇ                            ‚Üì                                     ‚îÇ
‚îÇ Orchestrator checks:       remaining_time() > 10ms?             ‚îÇ
‚îÇ                            ‚Üì (forward header)                   ‚îÇ
‚îÇ Worker checks at start:    remaining_time() > 100ms?            ‚îÇ
‚îÇ                            ‚Üì (check every 10 tokens)            ‚îÇ
‚îÇ Worker checks in loop:     remaining_time() > 0ms?              ‚îÇ
‚îÇ                            ‚Üì (abort if exceeded)                ‚îÇ
‚îÇ Worker aborts:             Return 504 Gateway Timeout           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ OVERHEAD: <1Œºs per check (negligible)                           ‚îÇ
‚îÇ BENEFIT: Prevents wasted GPU cycles on doomed work              ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ [CRITICAL] This entire flow is MISSING from spec!               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Waste Prevention Opportunities

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLIENT DISCONNECT (Not Currently Detected!)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ Scenario: Client disconnects after 1 second                     ‚îÇ
‚îÇ Current behavior: Worker continues for 60 seconds               ‚îÇ
‚îÇ Wasted GPU time: 59 seconds (98% waste!)                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ With disconnect detection (every 10 tokens):                    ‚îÇ
‚îÇ Detection latency: ~100ms (10 tokens √ó 10ms)                    ‚îÇ
‚îÇ Wasted GPU time: 0.1 seconds (0.2% waste)                       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ SAVINGS: 59 seconds per abandoned request                       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ [CRITICAL] Disconnect detection MISSING from spec!              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Critical Performance Issues (P0 - Must Fix for M0)

### PERF-CRIT-1: Missing Deadline Propagation Architecture

**Location**: Worker Inference API (SYS-5.4.2), Worker Execute Endpoint  
**Priority**: CRITICAL  
**Performance Impact**: Wasted GPU cycles on doomed work, no client deadline enforcement

**Latency Problem**:
The spec defines worker inference API but does not include `X-Deadline` header propagation. Without this, workers cannot:
1. Abort work when deadline already exceeded
2. Check remaining time during inference
3. Proactively cancel when insufficient time remains
4. Emit deadline-related metrics

**Current Spec**:
```http
POST /v2/execute
Content-Type: application/json
X-Correlation-Id: <uuid>

{
  "job_id": "<uuid>",
  "model_ref": "...",
  "prompt": "...",
  "params": { ... }
}
```

**Required Fix**:
```http
POST /v2/execute
Content-Type: application/json
X-Correlation-Id: <uuid>
X-Deadline: 2025-10-03T19:30:00Z  # <-- ADD THIS

{
  "job_id": "<uuid>",
  "model_ref": "...",
  "prompt": "...",
  "params": { ... }
}
```

**Implementation Requirements**:
1. Worker MUST parse `X-Deadline` header at request start
2. Worker MUST check `remaining_time(deadline)` before starting inference
3. Worker MUST check deadline every N tokens (recommend N=10)
4. Worker MUST abort with 504 if deadline exceeded
5. Worker MUST emit `deadline_check_duration_us` metric

**Throughput Gain**: Prevents wasting GPU cycles on work that will timeout. For a 5-second job that's already 6 seconds late, this saves 5 seconds of GPU time (100% waste prevention). At 100 requests/hour with 10% late arrivals, saves 50 seconds of GPU time per hour.

---

### PERF-CRIT-2: Client Disconnect Detection Not Specified

**Location**: Worker inference loop, SSE streaming  
**Priority**: CRITICAL  
**Performance Impact**: GPU continues inference after client disconnects, wasting cycles

**Waste Scenario**:
Spec does not define how worker detects client disconnect. If orchestrator crashes or client closes connection, worker continues generating tokens into the void.

**Waste Case**:
1. Client submits job, starts receiving tokens
2. Client crashes or closes connection after 1 second
3. Worker continues inference for 60 seconds
4. 59 seconds of GPU time wasted (98% waste)

**Current Spec**: No mention of disconnect detection

**Required Fix**:
Add to worker requirements:
```
Worker MUST poll SSE client connection health every N tokens (recommend N=10).
If client disconnected, worker MUST:
1. Abort inference immediately (within 100ms)
2. Free VRAM buffers
3. Log disconnect event with correlation_id
4. Emit client_disconnect_detection_ms metric
```

**Implementation Strategy**:
```rust
// Pseudo-code for inference loop
for token in generate_tokens() {
    if token_count % 10 == 0 {
        if !sse_stream.is_connected() {
            abort_inference();
            free_vram();
            log_disconnect();
            return;
        }
    }
    emit_token(token);
}
```

**Throughput Gain**: Detects disconnect within ~100ms (10 tokens √ó 10ms/token). Prevents wasted inference on abandoned requests. At 5% disconnect rate (network issues, client crashes), saves ~3 minutes of GPU time per hour on a busy system.

---

### PERF-CRIT-3: SSE Streaming Buffering Overhead

**Location**: Worker SSE streaming, Orchestrator SSE relay  
**Priority**: CRITICAL  
**Performance Impact**: Buffering adds latency to first token and per-token delivery

**Latency Problem**:
Spec says "stream tokens via SSE" but doesn't specify zero-copy or buffering requirements. Default HTTP libraries often buffer responses, adding 10-50ms latency per token.

**Latency Analysis**:
```
Without zero-copy:
  Token generated ‚Üí Copy to buffer ‚Üí Flush buffer ‚Üí Network ‚Üí Client
  10ms inference + 5ms copy + 10ms flush + 5ms network = 30ms per token

With zero-copy:
  Token generated ‚Üí Write directly to socket ‚Üí Network ‚Üí Client
  10ms inference + 0ms copy + 0ms flush + 5ms network = 15ms per token

Savings: 15ms per token √ó 100 tokens = 1.5 seconds total
```

**Current Spec**: "MUST stream results via SSE (token-by-token)" - no buffering requirement

**Required Fix**:
Add to worker requirements:
```
Worker MUST use zero-copy SSE streaming:
1. Write tokens directly to HTTP response stream (no intermediate buffers)
2. Flush after each token emission
3. Use chunked transfer encoding
4. Measure and emit per_token_emission_latency_us metric
```

**Implementation Strategy**:
- Use `hyper` with streaming body
- Write directly to `Body::wrap_stream()`
- Avoid `Vec<u8>` accumulation
- Flush immediately after each SSE event

**Latency Reduction**: Reduces per-token latency by 10-20ms. For 100-token response, saves 1-2 seconds total. User-perceived latency improves dramatically‚Äîtokens appear 50% faster.

---

## High Priority Optimizations (P1 - Should Fix for M0)

### PERF-HIGH-1: Model Loading Parallelization

**Location**: Worker startup flow (SYS-7.2.1)  
**Priority**: HIGH  
**Performance Impact**: Worker startup takes 60s, could be reduced to 10-20s with parallelization

**Startup Bottleneck**:
Spec describes sequential worker startup:
1. Spawn process (100ms)
2. Init CUDA context (1s)
3. Load model to VRAM (50-58s) ‚Üê BOTTLENECK
4. Start HTTP server (100ms)
5. Send ready callback (50ms)

Steps 2, 3, and 4 could be parallelized.

**Optimization Strategy**:
```
Parallel startup:
‚îú‚îÄ Thread 1: Init CUDA context + Load model (50s)
‚îî‚îÄ Thread 2: Start HTTP server (100ms) ‚Üí Wait for model load

Total: ~50s (vs 60s sequential)
```

**Better optimization with mmap**:
```
If pool-managerd pre-loaded model to RAM (hot-loading):
‚îú‚îÄ mmap model file (zero-copy, <100ms)
‚îú‚îÄ CUDA context init (1s)
‚îú‚îÄ cudaMemcpy from mmap to VRAM (5-10s for 8GB model)
‚îî‚îÄ HTTP server start (parallel, 100ms)

Total: ~6-11s (10x faster!)
```

**Required Spec Change**:
Add to SYS-7.2.1:
```
Worker startup SHOULD parallelize independent operations:
1. CUDA context initialization and HTTP server startup MAY run in parallel
2. If model is pre-loaded in RAM by pool-managerd (hot-loading), 
   worker SHOULD use mmap for zero-copy access
3. Worker SHOULD emit startup phase metrics:
   - model_load_duration_ms
   - cuda_init_duration_ms
   - http_server_start_duration_ms
```

**Startup Time Reduction**: Cold start: 60s ‚Üí 50s (17% faster). Hot start: 60s ‚Üí 6-11s (6-10x faster). For systems that spawn workers frequently, this is a massive win.

---

### PERF-HIGH-2: Health Endpoint Caching

**Location**: Worker health endpoint  
**Priority**: HIGH  
**Performance Impact**: Health checks may call expensive CUDA operations, blocking inference

**Overhead Problem**:
Spec says worker has `/health` endpoint but doesn't specify what it checks. If health endpoint calls `cudaMemGetInfo()` or other CUDA APIs, it may block inference or add latency.

**Bad Implementation** (likely default):
```rust
GET /health
‚Üí cudaMemGetInfo() // Blocks on GPU, 1-5ms
‚Üí Check model loaded // May acquire locks
‚Üí Return 200 OK

Problem: Health checks every 5s √ó 1-5ms = overhead + potential lock contention
```

**Good Implementation** (cached):
```rust
// Background thread updates health state every 1s
health_state = Arc<RwLock<HealthState>>

GET /health
‚Üí Read cached health_state // <1Œºs, no GPU calls
‚Üí Return 200 OK

Problem solved: No GPU overhead, no lock contention
```

**Required Spec Change**:
Add to worker requirements:
```
Worker health endpoint MUST respond in <10ms (p99).
Health endpoint SHOULD use cached state (updated by background thread).
Health endpoint MUST NOT call CUDA APIs in request path.
Health endpoint MUST NOT acquire inference locks.
```

**Overhead Elimination**: Eliminates 1-5ms overhead per health check. Prevents lock contention with inference. Health checks run every 5s, so this saves 0.2-1ms/second of GPU time.

---

### PERF-HIGH-3: Cancellation Check Interval

**Location**: Worker cancellation handling (SYS-6.3.5)  
**Priority**: HIGH  
**Performance Impact**: Cancellation latency depends on check interval (not specified)

**Responsiveness Problem**:
Spec says "worker MUST complete cancellation within 5s" but doesn't specify how often worker checks cancellation flag during inference.

**Latency Analysis**:
```
Check every 1 token:
  Overhead: 1Œºs √ó 100 tokens = 100Œºs total
  Detection latency: 10ms (1 token √ó 10ms/token)

Check every 10 tokens:
  Overhead: 1Œºs √ó 10 checks = 10Œºs total
  Detection latency: 100ms (10 tokens √ó 10ms/token)

Check every 100 tokens:
  Overhead: 1Œºs √ó 1 check = 1Œºs total
  Detection latency: 1000ms (100 tokens √ó 10ms/token)
```

**Recommendation**: Check every 10 tokens (good balance)

**Required Spec Change**:
Add to SYS-6.3.5:
```
Worker MUST check cancellation flag every N tokens (RECOMMENDED: N=10).
Cancellation check MUST complete in <1Œºs (simple atomic flag read).
Worker MUST abort inference within 100ms of cancellation detection.
Worker MUST emit cancellation_detection_latency_ms metric.
```

**Responsiveness Gain**: Overhead: <10Œºs per 100 tokens (negligible). Detection latency: <100ms (excellent). Users perceive instant cancellation.

---

### PERF-HIGH-4: Connection Pooling for Worker Dispatch

**Location**: Orchestrator ‚Üí Worker communication (SYS-5.4.1)  
**Priority**: HIGH  
**Performance Impact**: TCP handshake overhead on every request (20-50ms)

**Connection Overhead**:
Spec says orchestrator communicates directly with workers but doesn't specify connection reuse.

**Latency Analysis**:
```
Without connection pooling:
  TCP handshake: 20-50ms (3-way handshake)
  TLS handshake: +50-100ms (if using HTTPS)
  Request: 5ms
  Total: 75-155ms

With connection pooling:
  Reuse existing connection: 0ms
  Request: 5ms
  Total: 5ms

Savings: 70-150ms per request
```

**Required Spec Change**:
Add to SYS-5.4.1:
```
Orchestrator MUST maintain persistent HTTP connections to workers.
Orchestrator SHOULD use connection pooling with:
- Keep-alive timeout: 60s
- Max connections per worker: 10 (for M0 batch=1, only 1 needed)
- Connection reuse for sequential requests
Orchestrator MUST emit worker_connection_reuse_rate metric.
```

**Latency Reduction**: Saves 70-150ms per request. For 1000 requests/day, saves 70-150 seconds total. More importantly, reduces p95 dispatch latency from 155ms to 5ms (30x faster).

---

### PERF-HIGH-5: Graceful Shutdown Timeout

**Location**: Worker shutdown, cancellation deadline  
**Priority**: HIGH  
**Performance Impact**: Hung workers may not exit, requiring force-kill

**Resource Leak Problem**:
Spec says "worker MUST complete cancellation within 5s" but doesn't specify what happens if worker hangs.

**Scenarios**:
1. CUDA kernel stuck (rare but possible)
2. Deadlock in inference loop
3. Infinite loop in error handler

**Required Spec Change**:
Add to worker requirements:
```
Worker MUST implement graceful shutdown with timeout:
1. On SIGTERM, set shutdown flag
2. Abort in-flight inference (if any)
3. Free VRAM buffers
4. Exit within 5s

If worker does not exit within 5s:
- Pool-managerd MUST send SIGKILL
- Pool-managerd MUST force-release VRAM accounting
- Pool-managerd MUST log forced shutdown

Worker MUST emit graceful_shutdown_duration_ms metric.
```

**Resource Recovery**: Ensures workers never hang indefinitely. Guarantees VRAM cleanup within 5s. Prevents VRAM leaks that would degrade system throughput over time.

---

## Medium Priority Improvements (P2 - Nice to Have for M0)

### PERF-MED-1: CUDA Async Streams for I/O Overlap

**Location**: Worker inference loop  
**Priority**: MEDIUM  
**Performance Impact**: Could hide network latency by overlapping token generation with SSE transmission

**Parallelization Opportunity**:
```
Sequential (current):
  Generate token (10ms) ‚Üí Send SSE (5ms) ‚Üí Generate token (10ms) ‚Üí ...
  Total: 15ms per token

Overlapped (with async CUDA):
  Generate token (10ms) ‚ïë Send SSE (5ms in parallel)
  Total: 10ms per token (5ms saved per token)
```

**Spec Addition** (optional for M0):
```
Worker MAY use CUDA async streams to overlap token generation with SSE transmission.
If implemented, worker MUST emit cuda_stream_utilization metric.
```

---

### PERF-MED-2: Preflight Validation Caching

**Location**: Pool-managerd preflight (SYS-6.2.3)  
**Priority**: MEDIUM  
**Performance Impact**: NVML queries on every preflight add 5-10ms

**Caching Opportunity**:
Cache GPU state from heartbeats, use cached data for preflight.

**Spec Addition**:
```
Pool-managerd SHOULD cache GPU state from NVML queries.
Cache TTL SHOULD be ‚â§ heartbeat_interval_ms (default 15s).
Preflight validation SHOULD use cached state when fresh.
Preflight MUST emit preflight_cache_hit_rate metric.
```

---

### PERF-MED-3: Queue Enqueue Latency

**Location**: Orchestrator admission (SYS-7.1.1)  
**Priority**: MEDIUM  
**Performance Impact**: SQLite write may add 1-5ms latency to admission

**Async Write Opportunity**:
Use in-memory queue + async SQLite write.

**Spec Addition**:
```
Orchestrator SHOULD enqueue jobs to in-memory queue first (< 1ms).
Orchestrator MAY persist to SQLite asynchronously.
On restart, orchestrator MUST recover queue from SQLite.
```

---

### PERF-MED-4: Rhai Scheduler Timeout

**Location**: Orchestrator scheduling (SYS-6.1.5)  
**Priority**: MEDIUM  
**Performance Impact**: Runaway Rhai script could block scheduling

**Current Spec**: "Sandboxed execution with 50ms timeout"

**Good!** This is already specified. No change needed.

---

## Performance Testing Infrastructure

### Test Framework: Criterion.rs

**Crate**: `criterion` (the famous Rust benchmarking crate you mentioned)

**Minimum Machine Specs for Performance Testing**:

```yaml
Performance Test Machine (Workstation):
  CPU: 8+ cores (for parallel test execution)
  RAM: 16GB+ (for model loading tests)
  GPU: NVIDIA GPU with 8GB+ VRAM (for worker tests)
  Disk: SSD (for model loading benchmarks)
  OS: Linux (for consistent CUDA behavior)

Development Machine (Laptop/Dev Box):
  CPU: 4+ cores
  RAM: 8GB+
  GPU: Optional (can skip GPU tests)
  Note: Run only unit-level microbenchmarks, skip E2E tests

CI Machine:
  CPU: 4+ cores
  RAM: 8GB+
  GPU: Not required (mock GPU tests)
  Note: Run regression detection on CPU-bound operations only
```

**Recommendation**: 
- **Dev box**: Run only fast microbenchmarks (<1s each)
- **Workstation**: Run full performance test suite (including GPU tests)
- **CI**: Run CPU-only regression tests

### Required Performance Tests for M0

```rust
// bin/worker-orcd/benches/inference_latency.rs
#[bench]
fn bench_first_token_latency(c: &mut Criterion) {
    // Target: p95 < 100ms
    // Measures: POST /execute ‚Üí first SSE token
}

#[bench]
fn bench_per_token_latency(c: &mut Criterion) {
    // Target: p95 < 50ms
    // Measures: inter-token timing
}

#[bench]
fn bench_cancellation_latency(c: &mut Criterion) {
    // Target: p95 < 200ms
    // Measures: POST /cancel ‚Üí inference stopped
}

#[bench]
fn bench_client_disconnect_detection(c: &mut Criterion) {
    // Target: p95 < 100ms
    // Measures: client disconnect ‚Üí inference stopped
}

#[bench]
fn bench_health_endpoint(c: &mut Criterion) {
    // Target: p99 < 10ms
    // Measures: GET /health response time
}

#[bench]
fn bench_model_loading(c: &mut Criterion) {
    // Target: cold < 60s, hot < 10s
    // Measures: process start ‚Üí ready callback
}

#[bench]
fn bench_deadline_check_overhead(c: &mut Criterion) {
    // Target: < 1Œºs per check
    // Measures: remaining_time() call overhead
}
```

### Criterion Configuration for Non-Freezing Tests

```toml
# Cargo.toml
[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }

[[bench]]
name = "worker_latency"
harness = false
```

```rust
// benches/worker_latency.rs
use criterion::{criterion_group, criterion_main, Criterion, BenchmarkId};

fn configure_criterion() -> Criterion {
    Criterion::default()
        .sample_size(10)           // Small sample for GPU tests
        .measurement_time(Duration::from_secs(5))  // Short duration
        .warm_up_time(Duration::from_secs(1))      // Quick warmup
}

criterion_group! {
    name = benches;
    config = configure_criterion();
    targets = bench_first_token_latency, bench_per_token_latency
}

criterion_main!(benches);
```

**Run on dev box** (fast, no GPU):
```bash
cargo bench --bench worker_latency -- --quick
```

**Run on workstation** (full suite with GPU):
```bash
cargo bench --bench worker_latency
```

---

## Architectural Recommendations

### Worker-orcd Architecture: No Fundamental Errors Found ‚úÖ

After deep analysis, the worker-orcd architecture is **fundamentally sound**:

‚úÖ **Process isolation**: Correct (CUDA context per process)  
‚úÖ **VRAM-only policy**: Correct (no RAM fallback)  
‚úÖ **Single model per worker**: Correct (optimal cache locality)  
‚úÖ **HTTP + SSE**: Correct (standard, debuggable)  
‚úÖ **Self-contained**: Correct (testable in isolation)

**The architecture is good.** The issues are **missing specifications**, not architectural flaws.

### What Needs to Be Added (Not Changed):

1. **Deadline propagation** - Add X-Deadline header to API
2. **Client disconnect detection** - Add polling mechanism
3. **Zero-copy SSE** - Specify no buffering
4. **Cancellation check interval** - Specify every N tokens
5. **Health endpoint caching** - Specify no CUDA calls in request path
6. **Connection pooling** - Specify persistent connections
7. **Graceful shutdown timeout** - Specify 5s deadline

**None of these change behavior.** They are **performance clarifications** that should be in the spec.

---

## Conclusion

The M0 architecture is solid. The spec needs **performance clarifications**, not redesign. 

**Action Items**:
1. ‚úÖ Add inline performance audit comments (DONE)
2. ‚¨ú Add CRITICAL items to spec (deadline propagation, disconnect detection, zero-copy SSE)
3. ‚¨ú Add HIGH priority items to spec (connection pooling, cancellation interval, health caching)
4. ‚¨ú Set up criterion.rs benchmarks with machine specs
5. ‚¨ú Add Stage 2.5 (Performance validation) to CI gates

**Timeline**: Address CRITICAL items before M0 implementation starts. HIGH/MEDIUM items can be added during implementation.

---

**Audit completed by**: deadline-propagation team ‚è±Ô∏è  
**Every millisecond counts. Abort the doomed. Serve the living.** üöÄ
