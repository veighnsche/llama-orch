# Workflow

## 0) Guiding Principles

1. **Spec is law.** Requirements live in a single SPEC with stable IDs (`ORCH-XXXX`). All artifacts reference these IDs.
2. **Contract‑first.** OpenAPI + config schema + metrics are versioned contracts; code is generated/validated against them.
3. **TDD.** Write tests first: contracts → CDC (Pact) → stubs → provider verify → properties → E2E.
4. **Determinism by default.** Within a replica set, same `{prompt, params, seed}` MUST yield identical tokens.
5. **Fail fast.** No CPU spill; typed errors + backpressure.
6. **Short sessions.** No long‑running chats; no KV migration.
7. **Real‑model proof.** A live NVIDIA GPU Worker MUST pass the **Haiku** E2E test; prefer hitting a GPU worker over LAN from the dev box runner. In CI where a GPU is unreachable, allow a clearly marked CI‑only CPU carve‑out (still a real model). Mocks cannot satisfy release gates.

---

## 1) Repository Layout (workspace)

```bash
/ Cargo.toml                       # [workspace]
/.specs/
  orchestrator-spec.md             # RFC‑2119 with ORCH‑IDs
  metrics/otel-prom.md             # metric names, labels, units
/contracts/
  openapi/
    control.yaml                   # Control plane API (OpenAPI‑first)
    data.yaml                      # Data plane API (OpenAPI‑first)
  schemas/
    (emitted) config.schema.json   # From Rust types via schemars
  pacts/
    *.json                         # Consumer Pact files (in‑repo, no broker)
/contracts/api-types/              # Rust types generated by oapi‑codegen
/contracts/config-schema/          # Rust config types (emit JSON Schema)
/orchestrator-core/                # lib: queues, placement, fairness, SLO
/orchestratord/                    # bin: controller API (Axum/Tokio)
/pool-managerd/                    # bin: per‑host agent (spawns Workers by engine)
/worker-adapters/
  llamacpp-http/                   # lib: WorkerAdapter over llama.cpp HTTP
  vllm-http/                       # lib: WorkerAdapter over vLLM (OpenAI‑compat)
  tgi-http/                        # lib: WorkerAdapter over HF TGI
  triton/                          # lib: WorkerAdapter over Triton/TensorRT‑LLM
  mock/                            # lib: fault‑injecting adapter
/plugins/
  policy-host/                     # lib: WASI plugin loader/bridge
  policy-sdk/                      # lib: SDK for WASI placement policies
/cli/consumer-tests/               # Pact tests & CLI snapshots (insta)
/test-harness/
  e2e-haiku/                       # REAL MODEL haiku test (blocking)
  determinism-suite/               # 2 replicas, byte‑exact match
  chaos/                           # kill/restart/drain/reset scenarios (nightly)
/tools/
  spec-extract/                    # SPEC.md → requirements/index.yaml
  openapi-client/                  # generated client for black‑box tests
/requirements/
  index.yaml                       # AUTO‑GEN: req → tests → code
/ci/
  pipelines.yml                    # CI jobs
  dashboards/                      # Grafana dashboards & alerts
COMPLIANCE.md                      # AUTO‑GEN: req coverage
CHANGELOG_SPEC.md                  # Spec deltas
PROJECT_GUIDE.md                   # Newcomer on‑ramp
```

---

## 2) Locked‑In Choices (rationale)

* **OpenAPI:** **OpenAPI‑first** with `oapi-codegen` (true contract‑first; language‑agnostic; CI diffable).
* **Config schema:** **Rust types → JSON Schema** via `schemars` (DRY validation; compile‑time guarantees).
* **CDC:** **Pact (file‑based)** in repo (simple, reviewable; broker optional later).
* **Stubs:** **wiremock‑rs** (fast/ergonomic), backed by pact expectations.
* **Snapshots:** **insta** (diff‑friendly CLI transcripts/JSON/logs).
* **Properties:** **proptest** (invariants for queues/fairness/idempotency).
* **Determinism defaults:** inject `seed = hash(job_id)` if omitted; pin `sampler_profile_version = "v1"`.
* **Real model for CI:** **Qwen2.5‑0.5B‑Instruct (GGUF, Q4\_K\_M)** as **default**; **TinyLlama‑1.1B‑Chat (Q4\_K\_M)** as **fallback**; optional **SmolLM2‑1.7B‑Instruct** on nightly. Prefer targeting a GPU worker over LAN; if unavailable in CI, fall back to CPU locally.
* **Replica determinism flags (per engine):** for llama.cpp run with **`--parallel 1`** and **`--no-cont-batching`**; for other engines, use single‑slot/single‑request mode or their documented equivalents to ensure byte‑exact tests; pin engine commit/version.
* **Observability:** Prometheus + Grafana; JSON logs via `tracing`. Cardinality guards on labels. Metrics MUST include `engine` and engine‑specific version labels (e.g., `engine_version`, `trtllm_version`). See `.specs/metrics/otel-prom.md`.
* **Auth:** API key (HMAC) day‑1; OIDC later. Quotas: concurrent jobs, tokens/min, KV‑MB.
* **Policy plugins:** WASI (Rust first), pure deterministic function over snapshot, ≤ **1 ms** budget.
* **CI:** Prefer a dev‑hosted NVIDIA GPU worker reachable over LAN from the runner. If cloud CI cannot reach a GPU, add a clearly marked CI‑only CPU carve‑out (temporary); production remains NVIDIA‑only.

---

## 3) Stage Flow (TDD)

Note on heterogeneous GPUs: cross‑GPU splits are opt‑in and require explicit per‑GPU ratios; default is no cross‑GPU split. See the config example in `.specs/orchestrator-spec.md` §8.

### Stage 0 — Contract Freeze (TDD seed)

1. Author `contracts/openapi/{data.yaml,control.yaml}` with `x-req-id: ORCH-…` links back to SPEC. Define the public orchestrator API as **OrchQueue v1**: tasks, sessions, streams. Include an explicit `engine` enum on all relevant resources/types: `llamacpp | vllm | tgi | triton` (applies to Task, Pool, and typed error envelopes where relevant).
2. Generate Rust server/client with `oapi-codegen` into `/contracts/api-types` + server stubs in `/orchestratord`.
3. Define Rust config types in `/contracts/config-schema` (include `engine`, `devices`, optional `tensor_split` ratios) and **emit** `config.schema.json` via `schemars`.
   **Gate:** CI fails if OpenAPI or schema regeneration yields diffs.

### Stage 1 — Consumer‑Driven Tests (CLI) + Snapshots

1. Write **Pact** tests in `/cli/consumer-tests` against **OrchQueue v1** endpoints: `POST /v1/tasks`, `GET /v1/tasks/:id/stream`, `POST /v1/tasks/:id/cancel`, session endpoints. Commit pact JSON under `/contracts/pacts/`.
2. Run CLI against **wiremock‑rs** stubs that satisfy the pact. Capture CLI transcripts and JSON with **insta** snapshots.
   **Gate:** Pact tests + snapshots green before provider coding.

### Stage 2 — Provider Verification (orchestrator)

1. Add **Pact provider verification** (orchestratord) that loads pact files and verifies real handlers.
2. Implement minimal vertical slice to pass: admission → placement to one ready replica; determinism plumbing; fail‑fast errors + backpressure. Ensure all typed errors conform to contract and carry engine context where applicable.
   **Gate:** Provider verification green.

### Stage 3 — Properties & Invariants (core)

1. In `orchestrator-core`, add **proptest** suites for: FIFO invariants; priority fairness; `reject` vs `drop-lru` semantics; race‑free cancel.
   **Gate:** Property suites green.

### Stage 4 — Determinism Suite (replica set)

1. Launch two identical replicas per engine (same `engine_version` & sampler profile). For llama.cpp, start with **`--parallel 1 --no-cont-batching`**; for other engines, use single‑slot or documented equivalents to disable cross‑request batching.
2. Run **64 seeded prompts**; assert first‑32 tokens and full streams are **byte‑exact** for each engine.
    **Gate:** Any divergence → CI red with token diff artifact.

### Stage 5 — Observability & SLOs

1. Implement metrics exactly per `.specs/metrics/otel-prom.md` (names, units, labels). Labels MUST include `engine` and engine‑specific version labels (e.g., `engine_version`, `trtllm_version`). Add JSON logs with required fields (incl. `engine`, `seed`, `engine_version`, `sampler_profile_version`).
2. Commit Grafana dashboards + alert rules under `/ci/dashboards`.
    **Gate:** Metrics linter + dashboard render pass in CI (sample data).

### Stage 6 — Real‑Model E2E (Haiku) — **MANDATORY**

1. Prefer: point the test harness at a live NVIDIA GPU worker over LAN (your workstation), with metrics enabled — via the orchestrator’s **OrchQueue v1** APIs (`POST /v1/tasks`, then `GET /v1/tasks/:id/stream`). Fallback (CI‑only): boot **llama‑server** (CPU) with Qwen2.5‑0.5B‑Instruct GGUF and `--metrics` behind the orchestrator and still drive via OrchQueue v1.
2. Run **Haiku** test: nonce (8 chars) + **minute‑in‑words**; assert ≥ 3 non‑empty lines; both substrings present; `/metrics` token delta > 0; engine/model visible. Interact only with `/v1/tasks` + stream; do not call engine endpoints directly.
3. Enforce anti‑cheat: forbid `fixtures/haiku*`; repo scan for literals combining current minute words + nonce; `REQUIRE_REAL_LLAMA=1`.
    **Gate:** Pass within **≤ 30 s** (single retry if minute flips mid‑test).

### Stage 7 — Chaos & Load (nightly)

1. Kill workers, inject driver resets/oom, hot‑reload configs; verify idempotency and bounded backoff restarts.
2. 5–10 min load smoke; assert SLO budgets (TTFB p95, 64‑token p95) and no cardinality blow‑ups.
    **Gate:** Nightly only.

### Stage 8 — Compliance & Release

18. Run `spec-extract` → `requirements/index.yaml` (req → tests → code). Generate `COMPLIANCE.md`.
19. Update `CHANGELOG_SPEC.md`; tag release; publish artifacts (OpenAPI, Schema, dashboards).

---

## 4) Haiku Test (Normative Protocol)

**Inputs**

* `minute_words` = current minute in English (Europe/Amsterdam): "zero".."fifty‑nine".
* `nonce` = 8‑char random token.
* `seed` = fixed (or injected from `job_id`).

**Prompt template**

> Write a haiku (3 lines). Include this exact English minute phrase: "\<minute\_words>". Also include this nonce verbatim: "<nonce>". Output only the poem.

**Accept if**

* Output has ≥ 3 non‑empty lines.
* Contains `minute_words` and `nonce`.
* `/metrics` token counter increased (read once before job, once after).
* Worker exposes `engine_version` and `model_digest` (metrics or models API).

**Anti‑cheat**

* Fail if mock/stub engine detected or `/metrics` absent.
* Grep the repo for `fixtures/haiku*` and for lines containing both the current `minute_words` and the test `nonce`.
* Disallow hardcoded haiku content anywhere in source.

---

## 5) CI Jobs (reference; GitHub Actions)

**Blocking for PRs (order)**

1. `precommit` — fmt/lint; spec‑extract; regen OpenAPI & Schema; fail on diff.
2. `cdc_consumer` — Pact (CLI) → pact files.
3. `stub_flow` — wiremock stubs + insta snapshots.
4. `provider_verify` — Pact provider verification (orchestratord).
5. `unit_props` — unit + proptest.
6. `determinism` — per engine: two replicas, 64 prompts, byte‑exact (llama.cpp: `--parallel 1 --no-cont-batching`; others: single‑slot/single‑request equivalents).
7. `e2e_haiku` — prefer NVIDIA GPU worker over LAN via **OrchQueue v1** (`/v1/tasks` + stream); if unreachable in CI, run against CPU llama.cpp (Qwen 0.5B) as a CI‑only fallback, still via OrchQueue v1; minute+nonce, metrics delta, ≤ 30 s.
8. `docs_compliance` — spec‑extract + COMPLIANCE.md, linkcheck.

**Nightly**

* `chaos` — failure injections + recovery checks.
* `load_slo` — sustained load; SLO budget assertion.
* `e2e_gpu_optional` — same haiku/determinism on a GPU runner (preferred when available).

**Model cache step (example)**

```bash
HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download \
  Qwen/Qwen2.5-0.5B-Instruct-GGUF qwen2.5-0.5b-instruct-q4_k_m.gguf \
  --local-dir ~/.cache/models --local-dir-use-symlinks False
```

**Server start (CPU)**

```bash
llama-server \
  --model ~/.cache/models/qwen2.5-0.5b-instruct-q4_k_m.gguf \
  --host 127.0.0.1 --port 8080 \
  --metrics --no-webui
```

---

## 6) Developer Loop & Quality Gates (Refined)

Run these in order; second regen run must be diff‑clean.

1. Formatting and lints

```bash
cargo fmt --all -- --check
cargo clippy --all-targets --all-features -- -D warnings
```

2. Regenerators (deterministic)

```bash
cargo xtask regen-openapi
cargo xtask regen-schema
cargo run -p tools-spec-extract --quiet
git diff --exit-code
```

3. Tests (workspace + suites)

```bash
cargo test --workspace --all-features -- --nocapture
cargo test -p orchestratord --test provider_verify -- --nocapture
cargo test -p cli-consumer-tests -- --nocapture
cargo test -p tools-openapi-client -- --nocapture   # trybuild UI
cargo test -p test-harness-bdd -- --nocapture       # BDD placeholders (0 undefined/ambiguous)
```

4. Docs link checker

```bash
bash ci/scripts/check_links.sh
```

5. Convenience wrappers

```bash
cargo xtask ci:haiku:cpu
cargo xtask ci:determinism
```

See also: `README.md` (Quickstart) and `.specs/orchestrator-spec.md` (normative).

## SPEC→SHIP Workflow v2 (Contract‑First, Stub‑First, TDD)

**Status:** Adoptable now · **Scope:** NVIDIA‑only inference (Linux, headless) and multi‑engine (llama.cpp, vLLM, TGI, Triton/TensorRT‑LLM); Orchestrator + pools/scheduler + CLI consumer
**Conformance language:** RFC‑2119 (MUST/SHOULD/MAY)

This file is the authoritative buildbook to take the Orchestrator SPEC to a shippable release with **TDD**, **contract‑first**, **stub‑first**, and **real‑model gates**. Changes to this document require PR + review.

---

## 7) Backpressure, Policies, SLOs (defaults)

* **Queue full policy:** `interactive = reject`, `batch = drop-lru`.
* **Backpressure headers:** `429` + `Retry-After` (seconds) + `X-Backoff-Ms`.
* **Priority classes:** `interactive` (TTFB p95 ≤ 900 ms; 64‑tok p95 ≤ 2.5 s), `batch` (throughput‑oriented).

---

## 8) Security & Tenancy (day‑1)

* **Auth:** API keys (HMAC). Future: OIDC/JWT.
* **Quotas:** per‑tenant concurrent jobs, tokens/min, KV‑MB; reject before enqueue when exceeding.

---

## 9) Definition of Done (per requirement)

For each `ORCH-XXXX`:

1. Contract coverage (OpenAPI/Schema) present; 2) Consumer pact interaction exists; 3) Provider verification passes; 4) Unit/property tests cover edge cases; 5) Observability proves it (metric/log); 6) `requirements/index.yaml` links req → tests → code.

---

## 10) Rollout & Rollback

* Canary via label selectors or % splits. Rollback **MUST** be one action.
* Determinism note: new engine commit/version ⇒ **new replica set**; do not mix with old.

---

## 11) Env Conventions

* `TZ=Europe/Amsterdam` — enforced for E2E‑Haiku.
* `REQUIRE_REAL_LLAMA=1` — E2E must use a real Worker (GPU preferred; CPU is CI‑only fallback).
* `ORCH_REQUIRE_SAME_ENGINE=1` — enforce engine/version pin within replica sets.
* `ORCH_SEED_DEFAULT=hash(job_id)` — seed injection policy when omitted.

---

## 12) Notes

* Keep SPEC IDs stable; deprecate with `-DEPR`, never reuse.
* Treat metrics as contracts: changing names/labels is a breaking change.
* Haiku is a **human‑plausibility** test, not a content validator.
