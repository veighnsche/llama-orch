# TEAM-270: Worker HTTP API OpenAPI Specification
#
# This specification defines the HTTP API that ALL worker implementations
# must provide, regardless of their underlying technology (Candle, llama.cpp,
# vLLM, Ollama, etc.).

openapi: 3.0.3
info:
  title: rbee Worker API
  description: |
    HTTP API specification for rbee worker implementations.
    
    All workers (bespoke or adapters) must implement these endpoints to
    integrate with the rbee orchestration system.
    
    ## Worker Types
    
    - **Bespoke Workers**: Custom implementations using Candle ML framework
    - **Adapters**: Wrappers around existing inference engines (llama.cpp, vLLM, etc.)
    
    ## Lifecycle
    
    1. Hive spawns worker process
    2. Worker loads model into memory
    3. Worker starts HTTP server
    4. Worker sends heartbeat to queen every 30s
    5. Worker accepts inference requests
    6. Worker can be stopped by hive
    
  version: 0.1.0
  contact:
    name: rbee Team
  license:
    name: GPL-3.0-or-later

servers:
  - url: http://localhost:9301
    description: Local worker (default port 9301)

tags:
  - name: health
    description: Health check endpoints
  - name: info
    description: Worker information
  - name: inference
    description: Inference operations

paths:
  /health:
    get:
      summary: Health check
      description: |
        Quick liveness check. Returns 200 OK if worker is alive.
        
        Used by hive and queen to verify worker is running.
      tags:
        - health
      operationId: getHealth
      responses:
        '200':
          description: Worker is alive
          content:
            text/plain:
              schema:
                type: string
                example: "ok"

  /info:
    get:
      summary: Get worker information
      description: |
        Returns complete worker state including model, device, status, etc.
        
        Used by queen to track worker capabilities and availability.
      tags:
        - info
      operationId: getInfo
      responses:
        '200':
          description: Worker information
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WorkerInfo'

  /v1/infer:
    post:
      summary: Execute inference
      description: |
        Execute inference on the loaded model.
        
        Supports both streaming (SSE) and non-streaming responses.
        
        ## Streaming Mode
        
        When `stream: true`, tokens are sent via Server-Sent Events (SSE):
        
        ```
        data: {"token": "Hello"}
        data: {"token": " world"}
        data: {"token": "!"}
        data: [DONE]
        ```
        
        ## Non-Streaming Mode
        
        When `stream: false` or omitted, returns complete response as JSON.
      tags:
        - inference
      operationId: infer
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/InferRequest'
      responses:
        '200':
          description: Inference completed successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InferResponse'
            text/event-stream:
              schema:
                type: string
                description: Server-Sent Events stream of tokens
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '503':
          description: Worker is busy or not ready
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

components:
  schemas:
    WorkerInfo:
      type: object
      required:
        - id
        - model_id
        - device
        - port
        - status
        - implementation
        - version
      properties:
        id:
          type: string
          description: Unique worker ID (generated by hive)
          example: "worker-abc123"
        model_id:
          type: string
          description: Model being served
          example: "meta-llama/Llama-2-7b"
        device:
          type: string
          description: Device worker is using
          example: "GPU-0"
          enum:
            - CPU-0
            - GPU-0
            - GPU-1
            - GPU-2
            - GPU-3
        port:
          type: integer
          format: uint16
          description: HTTP port worker is listening on
          example: 9301
        status:
          $ref: '#/components/schemas/WorkerStatus'
        implementation:
          type: string
          description: Worker implementation type
          example: "llm-worker-rbee"
          enum:
            - llm-worker-rbee
            - llama-cpp-adapter
            - vllm-adapter
            - ollama-adapter
            - comfyui-adapter
        version:
          type: string
          description: Worker version
          example: "0.1.0"

    WorkerStatus:
      type: string
      description: Current worker status
      enum:
        - starting
        - ready
        - busy
        - stopped

    InferRequest:
      type: object
      required:
        - prompt
      properties:
        prompt:
          type: string
          description: Input text to generate from
          example: "Hello, world!"
        max_tokens:
          type: integer
          format: uint32
          description: Maximum tokens to generate (optional)
          example: 100
          minimum: 1
          maximum: 4096
        temperature:
          type: number
          format: float
          description: Sampling temperature (optional, default 0.7)
          example: 0.7
          minimum: 0.0
          maximum: 2.0
        stream:
          type: boolean
          description: Stream tokens via SSE (optional, default false)
          example: true

    InferResponse:
      type: object
      required:
        - text
        - tokens_generated
        - duration_ms
      properties:
        text:
          type: string
          description: Generated text
          example: "Hello! How can I help you today?"
        tokens_generated:
          type: integer
          format: uint32
          description: Number of tokens generated
          example: 8
        duration_ms:
          type: integer
          format: uint64
          description: Inference duration in milliseconds
          example: 250

    Error:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: Error message
          example: "Worker is busy processing another request"
