# ⚠️ NO LLAMA.CPP - PERMANENT RULE

**WE ARE BUILDING A LLAMA.CPP-FREE INFERENCE ENGINE**

## The Rule

❌ **NEVER** import, link, or depend on llama.cpp  
✅ **ALWAYS** build our own implementation

## Why

**We are llama-orch - the llama.cpp competitor**

We're building something better:
- Custom CUDA kernels
- MXFP4 quantization (43% VRAM savings)
- Production-ready from day 1
- Multi-GPU pool management

## Status

We are **so close** to a complete llama.cpp-free engine.

**Don't give up now. Build it ourselves.**

---

See: `bin/worker-orcd/NO_LLAMA_CPP_RULE.md` for full details.
