# TEAM-021 Handoff: Multi-Model Testing & Production Validation

**Date:** 2025-10-09  
**From:** TEAM-020  
**To:** TEAM-021  
**Status:** ‚úÖ Candle fork integrated and validated on Llama

---

## Executive Summary

TEAM-020 successfully created and integrated a Candle fork with the mask broadcasting fix. All three backends (CPU, CUDA, Metal) now work correctly with **TinyLlama** using the proper upstream fix instead of workarounds.

**Your mission:** Test the remaining model architectures (Mistral, Phi, Qwen) and validate production readiness.

---

## What TEAM-020 Completed ‚úÖ

### 1. Created Candle Fork with Mask Fix

**Repository:** https://github.com/veighnsche/candle  
**Branch:** `llorch/metal-bugfixes`  
**Commit:** 9c458371

**Changes Applied:**
- Modified `Cache` struct to use tuple key `(seq_len, seqlen_offset)`
- Updated `Cache::mask()` to create proper `[1, 1, t, t+offset]` shape
- Calculate `seqlen_offset` from KV cache length in attention
- Based on candle-vllm fix for Metal/CUDA inference bug

**Files Modified:**
- `candle-transformers/src/models/llama.rs`

### 2. Integrated Fork into llorch-candled

**Updated:** `bin/llorch-candled/Cargo.toml`

**Dependencies Changed:**
```toml
candle-core = { git = "https://github.com/veighnsche/candle.git", branch = "llorch/metal-bugfixes" }
candle-nn = { git = "https://github.com/veighnsche/candle.git", branch = "llorch/metal-bugfixes" }
candle-transformers = { git = "https://github.com/veighnsche/candle.git", branch = "llorch/metal-bugfixes" }
candle-kernels = { git = "https://github.com/veighnsche/candle.git", branch = "llorch/metal-bugfixes", optional = true }
```

### 3. Removed TEAM-019 Workaround

**File:** `src/backend/models/llama.rs`

**Removed:**
- Cache recreation on position=0 (workaround)
- ~10 lines of workaround code

**Result:** Clean implementation using proper Candle fix

### 4. Tested All Backends with Llama

**Test Results:**

| Backend | Status | Test Command | Result |
|---------|--------|--------------|--------|
| CPU | ‚úÖ | `cargo test --features cpu` | 123 tests passed |
| Metal | ‚úÖ | `llorch-remote mac.home.arpa metal debug-inference` | Generated 5 tokens |
| CUDA | ‚úÖ | `llorch-remote workstation.home.arpa cuda debug-inference` | Generated 5 tokens |

**Validation:**
- No broadcasting errors
- KV cache works correctly
- Inference generates coherent tokens
- All backends consistent behavior

### 5. Created Documentation

**New File:** `docs/MODEL_SUPPORT.md`

**Contents:**
- Supported architectures matrix
- Backend compatibility table
- Candle fork details
- Testing status
- Model requirements
- Known issues (all fixed)
- Future work roadmap

---

## What's Left TODO üöß

### Priority 1: Obtain SafeTensors Models ‚ö†Ô∏è BLOCKER

**Problem:** TEAM-020 downloaded GGUF models, but llorch-candled only supports SafeTensors format.

**Current Status:**
- ‚úÖ Llama: Tested on all backends (SafeTensors)
- ‚ö†Ô∏è Mistral: Code ready, GGUF download failed (TheBloke repo unavailable)
- ‚úÖ Phi: Code ready, GGUF downloaded (~2.4GB) - needs SafeTensors
- ‚úÖ Qwen: Code ready, GGUF downloaded Q4 (~469MB) and FP16 (~1.2GB) - needs SafeTensors

**Downloaded Models (GGUF - not compatible):**
- `.test-models/phi3/phi-3-mini-4k-instruct-q4.gguf` (2.4GB)
- `.test-models/qwen/qwen2.5-0.5b-instruct-q4_k_m.gguf` (469MB)
- `.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf` (1.2GB)

**Why This Matters:**
- Mistral/Qwen likely use same attention as Llama (should work)
- Phi uses different cache management (may need testing)
- Need to validate fork fix works for all architectures
- **BLOCKER:** Cannot test without SafeTensors format models

**Your Tasks:**
1. **Obtain SafeTensors models** for Mistral, Phi, and Qwen
   - Option A: Download SafeTensors versions from HuggingFace
   - Option B: Convert GGUF to SafeTensors
   - Option C: Add GGUF support to llorch-candled (larger effort)
2. Test on CPU first (fastest iteration)
3. Test on Metal and CUDA
4. Document any architecture-specific issues
5. Update MODEL_SUPPORT.md with results

### Priority 2: Obtain SafeTensors Models

**TEAM-020 Downloaded (GGUF - not compatible):**

1. **Phi-3-Mini-4K-Instruct** ‚úÖ
   - Downloaded: `.test-models/phi3/phi-3-mini-4k-instruct-q4.gguf` (2.4GB)
   - Format: GGUF Q4
   - **Need:** SafeTensors version

2. **Qwen2.5-0.5B-Instruct** ‚úÖ
   - Downloaded Q4: `.test-models/qwen/qwen2.5-0.5b-instruct-q4_k_m.gguf` (469MB)
   - Downloaded FP16: `.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf` (1.2GB)
   - Format: GGUF
   - **Need:** SafeTensors version

3. **Mistral-7B-Instruct** ‚ùå
   - Download failed: TheBloke repo unavailable
   - **Need:** Find SafeTensors source

**Recommended SafeTensors Sources:**

```bash
# Qwen2.5-0.5B-Instruct (SafeTensors)
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct \
    --include "*.safetensors" "*.json" \
    --local-dir .test-models/qwen-safetensors

# Phi-3-Mini-4K-Instruct (SafeTensors)
huggingface-cli download microsoft/Phi-3-mini-4k-instruct \
    --include "*.safetensors" "*.json" \
    --local-dir .test-models/phi3-safetensors

# Mistral-7B-Instruct-v0.2 (SafeTensors)
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.2 \
    --include "*.safetensors" "*.json" \
    --local-dir .test-models/mistral-safetensors
```

### Priority 3: Multi-Model Test Script

**Status:** ‚úÖ Created by TEAM-020

**Script:** `.docs/testing/test_multi_model.sh`

**Features:**
- Test all 4 architectures
- Run on specified backend
- Check for common errors
- Generate test report

**Current State:**
- Script created and ready
- Commented out tests (waiting for SafeTensors models)
- Will work once SafeTensors models are available

**Example Usage (after SafeTensors obtained):**
```bash
./test_multi_model.sh cpu
./test_multi_model.sh metal
./test_multi_model.sh cuda
```

### Priority 4: Production Validation

**After multi-model testing passes:**

1. **Performance Benchmarks**
   - Measure tokens/sec per model
   - Compare fork vs. workaround (Llama only)
   - Test with long contexts (>1000 tokens)
   - Memory usage profiling

2. **Stability Testing**
   - Run extended inference tests
   - Test rapid request sequences
   - Verify no memory leaks
   - Test edge cases (very long sequences)

3. **Documentation Updates**
   - Update MODEL_SUPPORT.md with test results
   - Document any model-specific quirks
   - Add performance benchmarks
   - Update README with fork usage

---

## Recommended Approach

### Week 1: Obtain Models & Quick Validation

**Day 1: Get SafeTensors Models**
- [ ] Download Qwen2.5-0.5B-Instruct SafeTensors (~1GB)
- [ ] Download Phi-3-Mini SafeTensors (~5GB)
- [ ] Download Mistral-7B SafeTensors (~14GB)
- [ ] Verify model structure (config.json, tokenizer.json, *.safetensors)

**Day 2: Qwen Testing (Smallest Model)**
- [ ] Test Qwen on CPU
- [ ] Test Qwen on Metal
- [ ] Test Qwen on CUDA
- [ ] Document results

**Day 3: Phi Testing**
- [ ] Test Phi on CPU
- [ ] Test Phi on Metal
- [ ] Test Phi on CUDA
- [ ] Note any differences (different cache management)

**Day 4: Mistral Testing**
- [ ] Test Mistral on CPU
- [ ] Test Mistral on Metal
- [ ] Test Mistral on CUDA
- [ ] Compare with Llama behavior

**Day 5: Test Script & Documentation**
- [ ] Update `test_multi_model.sh` with SafeTensors paths
- [ ] Run full test suite
- [ ] Update MODEL_SUPPORT.md
- [ ] Document any issues found

### Week 2: Production Validation

**Day 1-2: Performance Benchmarks**
- [ ] Tokens/sec per model per backend
- [ ] Memory usage measurements
- [ ] Long context testing (>1000 tokens)
- [ ] Compare fork vs. workaround (Llama)

**Day 3-4: Stability Testing**
- [ ] Extended inference runs
- [ ] Rapid request sequences
- [ ] Edge case testing
- [ ] Memory leak detection

**Day 5: Documentation & Handoff**
- [ ] Final MODEL_SUPPORT.md update
- [ ] Performance report
- [ ] TEAM_022_HANDOFF.md
- [ ] Recommend when to upstream

---

## Test Script Template

Create `.docs/testing/test_multi_model.sh`:

```bash
#!/usr/bin/env bash
# Multi-model architecture test
# Created by: TEAM-021
# Tests all supported architectures on current backend

set -euo pipefail

BACKEND="${1:-cpu}"
MODELS_DIR="${2:-.test-models}"

echo "Testing multi-model support on $BACKEND backend"
echo "================================================"

# Test results
PASSED=0
FAILED=0
SKIPPED=0

test_model() {
    local arch="$1"
    local model_dir="$2"
    
    echo ""
    echo "Testing $arch..."
    
    if [[ ! -d "$model_dir" ]]; then
        echo "‚ö†Ô∏è  Model not found: $model_dir (skipping)"
        ((SKIPPED++))
        return
    fi
    
    # Start worker
    ./target/release/llorch-${BACKEND}-candled \
        --worker-id "test-$arch" \
        --model "$model_dir" \
        --port 9876 \
        --callback-url http://localhost:9999 &
    WORKER_PID=$!
    
    sleep 5
    
    # Test inference
    RESPONSE=$(curl -s -X POST http://localhost:9876/execute \
        -H "Content-Type: application/json" \
        -d '{"job_id":"test","prompt":"Hello","max_tokens":10,"seed":42}')
    
    kill $WORKER_PID 2>/dev/null || true
    
    if echo "$RESPONSE" | grep -q '"type":"token"'; then
        echo "‚úÖ $arch works on $BACKEND"
        ((PASSED++))
    else
        echo "‚ùå $arch failed on $BACKEND"
        echo "Response: $RESPONSE"
        ((FAILED++))
    fi
}

# Test each architecture
test_model "llama" "$MODELS_DIR/tinyllama"
test_model "mistral" "$MODELS_DIR/mistral"
test_model "phi" "$MODELS_DIR/phi"
test_model "qwen" "$MODELS_DIR/qwen"

# Summary
echo ""
echo "================================================"
echo "Test Results: $PASSED passed, $FAILED failed, $SKIPPED skipped"
echo "================================================"

[[ $FAILED -eq 0 ]]
```

---

## Known Issues & Gotchas

### 1. Model Download Sizes

**Be aware:**
- Mistral-7B: ~14GB (large!)
- Phi-2: ~5.4GB (medium)
- Qwen2-0.5B: ~1GB (small, test first!)

**Recommendation:** Start with Qwen for fast iteration.

### 2. Different Cache Patterns

**Llama/Mistral/Qwen:**
- Use position-based cache
- Explicit position parameter
- Should all benefit from mask fix

**Phi:**
- Internal cache management
- No position parameter
- May have different behavior

### 3. Tokenizer Differences

| Architecture | Tokenizer | EOS Token |
|--------------|-----------|-----------|
| Llama | SentencePiece | 2 |
| Mistral | SentencePiece | 2 |
| Phi | GPT-2 | 50256 |
| Qwen | Custom | 151643 |

**Note:** Already handled in model wrappers.

### 4. Model Format

**All models must be SafeTensors format:**
- ‚úÖ Single file: `model.safetensors`
- ‚úÖ Sharded: `model-00001-of-00002.safetensors`, etc.
- ‚ùå PyTorch: Not supported
- ‚ùå GGUF: Not supported (yet)

---

## Success Criteria

Your work is complete when:

- [ ] **4 model architectures tested** on all 3 backends (12 test cases)
- [ ] **Test script created** (`test_multi_model.sh`)
- [ ] **All models work** on CPU, Metal, and CUDA
- [ ] **Performance benchmarks** collected
- [ ] **MODEL_SUPPORT.md updated** with test results
- [ ] **No regressions** from fork integration
- [ ] **Production validation** complete

---

## Resources

### Internal Documentation
- `docs/MODEL_SUPPORT.md` - Model support matrix (TEAM-020)
- `.specs/CANDLE_UPSTREAM_OPPORTUNITIES.md` - Fork strategy
- `.specs/METAL_CUDA_INFERENCE_BUG_REPORT.md` - Original bug
- `src/backend/models/mod.rs` - Model factory

### Candle Fork
- Repository: https://github.com/veighnsche/candle
- Branch: `llorch/metal-bugfixes`
- Commit: 9c458371

### Model Sources
- HuggingFace: https://huggingface.co/models
- Qwen: https://huggingface.co/Qwen/Qwen2-0.5B-Instruct
- Mistral: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- Phi: https://huggingface.co/microsoft/phi-2

### Testing Tools
- `scripts/homelab/llorch-remote` - Remote testing CLI
- `.docs/testing/download_tinyllama.sh` - Example download script

---

## Questions for TEAM-021

Before you start:

1. **Model priority:** Which model to test first?
   - Recommend: Qwen (smallest, fastest to download)

2. **Test depth:** How extensive should testing be?
   - Minimum: 10 tokens per model per backend
   - Recommended: 100+ tokens, multiple prompts

3. **Performance baseline:** Should we benchmark against workaround?
   - Yes, for Llama only (we have both versions)

4. **Upstream timeline:** When should we create PR to candle-rs?
   - Recommend: After 1-2 months production use

---

## Handoff Checklist

### TEAM-020 Completed ‚úÖ
- [x] Candle fork created with mask fix
- [x] Fork integrated into llorch-candled
- [x] TEAM-019 workaround removed
- [x] Llama tested on all backends (CPU, Metal, CUDA)
- [x] MODEL_SUPPORT.md documentation created
- [x] Multi-model test script created (`.docs/testing/test_multi_model.sh`)
- [x] Downloaded Qwen GGUF models (Q4 and FP16)
- [x] Downloaded Phi GGUF model (Q4)
- [x] Attempted Mistral download (failed - repo unavailable)

### TEAM-021 TODO ‚ö†Ô∏è
- [ ] Obtain SafeTensors models for Mistral, Phi, Qwen
- [ ] Test Mistral on all backends
- [ ] Test Phi on all backends
- [ ] Test Qwen on all backends
- [ ] Update test script with SafeTensors paths
- [ ] Performance benchmarks collected
- [ ] Production validation complete

---

**Handoff completed:** 2025-10-09  
**From:** TEAM-020  
**To:** TEAM-021  
**Status:** ‚ö†Ô∏è Fork validated with Llama, GGUF models downloaded, need SafeTensors  
**Next action:** TEAM-021 to obtain SafeTensors models and test all architectures

**IMPORTANT:** TEAM-020 downloaded GGUF models but llorch-candled requires SafeTensors format. Priority 1 is obtaining compatible models before testing can proceed.

---

**Signed:**  
TEAM-020  
2025-10-09T12:47:00+02:00
