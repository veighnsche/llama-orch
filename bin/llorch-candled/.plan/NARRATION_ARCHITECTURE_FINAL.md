# ğŸ¯ Narration Architecture: Stdout vs SSE

**Status**: âœ… **CLARIFIED**  
**Date**: 2025-10-09  
**Author**: Narration Core Team ğŸ€

---

## ğŸ“‹ Executive Summary

Narration events serve **two different purposes** at **two different times** in the worker's lifecycle:

1. **Stdout Narration** - Worker lifecycle events (startup, model loading, shutdown)
   - Captured by pool-manager
   - Used for operational monitoring
   - ~8 events per worker lifetime

2. **SSE Narration** - Per-request events (inference pipeline, progress, completion)
   - Streamed to user via orchestrator
   - Used for real-time user feedback
   - ~7 events per inference request

**Both are needed!** They are not redundant - they serve different audiences at different times.

---

## ğŸ” Worker Lifecycle Timeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: Worker Startup (NO HTTP connection)                    â”‚
â”‚ Audience: Pool-Manager (via stdout)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Pool-manager spawns worker binary                            â”‚
â”‚ 2. Worker process starts                                        â”‚
â”‚ 3. Device initialization                                        â”‚
â”‚ 4. Model loading                                                â”‚
â”‚ 5. HTTP server starts                                           â”‚
â”‚ 6. Worker calls back to pool-manager "I'm ready!"               â”‚
â”‚                                                                  â”‚
â”‚ Narration Output: stdout â†’ Pool-manager captures                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: Worker Ready (HTTP server running)                     â”‚
â”‚ Audience: None (waiting for requests)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Worker is idle, waiting for orchestrator to send /execute       â”‚
â”‚                                                                  â”‚
â”‚ Narration Output: None (no events during idle)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: Inference Request (HTTP connection active)             â”‚
â”‚ Audience: User (via orchestrator, through SSE stream)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Orchestrator sends POST /execute                             â”‚
â”‚ 2. Worker validates request                                     â”‚
â”‚ 3. Worker starts inference                                      â”‚
â”‚ 4. Worker tokenizes prompt                                      â”‚
â”‚ 5. Worker resets cache                                          â”‚
â”‚ 6. Worker generates tokens (streaming)                          â”‚
â”‚ 7. Worker completes inference                                   â”‚
â”‚ 8. Worker sends final SSE event                                 â”‚
â”‚                                                                  â”‚
â”‚ Narration Output: SSE stream â†’ Orchestrator â†’ User's screen     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: Worker Shutdown (HTTP connection may be closed)        â”‚
â”‚ Audience: Pool-Manager (via stdout)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Pool-manager sends SIGTERM or POST /shutdown                 â”‚
â”‚ 2. Worker gracefully shuts down HTTP server                     â”‚
â”‚ 3. Worker frees VRAM                                            â”‚
â”‚ 4. Worker exits                                                 â”‚
â”‚                                                                  â”‚
â”‚ Narration Output: stdout â†’ Pool-manager captures                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Narration Event Classification

### Category 1: Stdout-Only Events (Worker Lifecycle)

**These happen when there is NO active HTTP request:**

| File | Line | Actor | Action | Event | Why Stdout? |
|------|------|-------|--------|-------|-------------|
| `main.rs` | 76-84 | `llorch-candled` | `startup` | "Starting Candle worker on port 8080" | Binary just started, no HTTP yet |
| `device.rs` | 18-25 | `device-manager` | `device_init` | "Initialized CPU device" | During startup, before HTTP |
| `device.rs` | 37-45 | `device-manager` | `device_init` | "Initialized CUDA device 0" | During startup, before HTTP |
| `device.rs` | 58-66 | `device-manager` | `device_init` | "Initialized Apple Metal device 0" | During startup, before HTTP |
| `main.rs` | 95-103 | `model-loader` | `model_load` | "Loading Llama model from /models/..." | During startup, before HTTP |
| `inference.rs` | 58-66 | `model-loader` | `model_load` | "Loaded Llama model (7000 MB, vocab: 32000)" | During startup, before HTTP |
| `main.rs` | 119-128 | `llorch-candled` | `callback_ready` | "Reporting ready to pool-managerd" | During startup, callback to pool-manager |
| `startup.rs` | 33-42 | `llorch-candled` | `callback_ready` | "Calling pool-managerd at http://..." | During startup, callback to pool-manager |
| `startup.rs` | 48-57 | `llorch-candled` | `error` | "Pool manager callback failed: 500" | During startup, callback error |
| `server.rs` | 83-90 | `http-server` | `server_start` | "HTTP server initialized on 0.0.0.0:8080" | Server lifecycle, not request-specific |
| `server.rs` | 126-133 | `http-server` | `server_bind` | "HTTP server listening on 0.0.0.0:8080" | Server lifecycle, not request-specific |
| `server.rs` | 108-116 | `http-server` | `error` | "Failed to bind to 0.0.0.0:8080" | Server lifecycle error |
| `server.rs` | 160-167 | `http-server` | `server_shutdown` | "HTTP server shutting down gracefully" | Server lifecycle, shutdown |

**Total: 13 stdout-only events**

**Audience**: Pool-manager (operational monitoring)  
**Output**: stdout â†’ captured by pool-manager â†’ pool-manager logs  
**Purpose**: Track worker lifecycle, diagnose startup/shutdown issues

---

### Category 2: SSE Events (Per-Request)

**These happen DURING an active `/execute` HTTP request:**

| File | Line | Actor | Action | Event | Why SSE? |
|------|------|-------|--------|-------|----------|
| `execute.rs` | 36-45 | `http-server` | `error` | "Validation failed for job job-123" | During request, user needs to see |
| `execute.rs` | 52-60 | `http-server` | `execute_request` | "Inference request validated for job job-123" | During request, user wants feedback |
| `execute.rs` | 81-90 | `candle-backend` | `error` | "Inference failed for job job-123: ..." | During request, user needs error |
| `inference.rs` | 158-165 | `candle-backend` | `inference_start` | "Starting inference (prompt: 15 chars, max_tokens: 50)" | During inference, user wants progress |
| `inference.rs` | 176-184 | `tokenizer` | `tokenize` | "Tokenized prompt (15 tokens)" | During inference, user wants details |
| `inference.rs` | 192-199 | `candle-backend` | `cache_reset` | "Reset KV cache before inference" | During inference, technical detail |
| `inference.rs` | 295-303 | `candle-backend` | `token_generate` | "Generated 10 tokens" | During inference, progress update |
| `inference.rs` | 325-334 | `candle-backend` | `inference_complete` | "Inference completed (50 tokens in 250 ms, 200 tok/s)" | During inference, completion status |

**Total: 8 SSE events per request**

**Audience**: End user (via orchestrator)  
**Output**: SSE stream â†’ orchestrator â†’ user's screen  
**Purpose**: Real-time feedback on inference progress, show what's happening

---

### Category 3: Hybrid Events (Could Be Either)

**These could go to stdout OR SSE depending on context:**

| File | Line | Actor | Action | Event | Context |
|------|------|-------|--------|-------|---------|
| `health.rs` | 43-50 | `http-server` | `health_check` | "Health check: healthy (VRAM: 8000 MB)" | During `/health` request |
| `inference.rs` | 87-94 | `candle-backend` | `warmup` | "Starting GPU warmup" | During startup (stdout) OR on-demand (SSE) |
| `inference.rs` | 124-132 | `candle-backend` | `warmup` | "GPU warmup complete (50 ms)" | During startup (stdout) OR on-demand (SSE) |

**Decision**: Currently stdout-only, but could be SSE if warmup is triggered during a request.

---

## ğŸ¯ Implementation Strategy

### Current State (Partially Correct)

âœ… **Stdout narration is implemented correctly:**
- All lifecycle events go to stdout
- Pool-manager can capture them
- Works for operational monitoring

âŒ **SSE narration is NOT implemented:**
- Per-request events only go to stdout
- User cannot see them in real-time
- Missing `narration` event type in SSE

---

### Required Changes

#### 1. Add Narration Event Type to SSE

**File**: `src/http/sse.rs`

```rust
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum InferenceEvent {
    Started { ... },
    Token { ... },
    Metrics { ... },
    
    /// NEW: Narration event for user-facing progress updates
    Narration {
        actor: String,
        action: String,
        target: String,
        human: String,
        #[serde(skip_serializing_if = "Option::is_none")]
        cute: Option<String>,
        #[serde(skip_serializing_if = "Option::is_none")]
        story: Option<String>,
        #[serde(skip_serializing_if = "Option::is_none")]
        correlation_id: Option<String>,
        #[serde(skip_serializing_if = "Option::is_none")]
        job_id: Option<String>,
    },
    
    End { ... },
    Error { ... },
}
```

#### 2. Create SSE Channel for Narration

**File**: `src/http/execute.rs`

```rust
pub async fn handle_execute<B: InferenceBackend>(
    State(backend): State<Arc<Mutex<B>>>,
    Json(req): Json<ExecuteRequest>,
) -> Result<Sse<EventStream>, ValidationErrorResponse> {
    // Create channel for narration events
    let (narration_tx, mut narration_rx) = tokio::sync::mpsc::unbounded_channel();
    
    // Store in request-local context (thread-local or async-local)
    NARRATION_SENDER.with(|sender| {
        *sender.borrow_mut() = Some(narration_tx.clone());
    });
    
    // ... rest of handler
}
```

#### 3. Modify Narration Function

**File**: `narration-core/src/lib.rs` (or create worker-specific wrapper)

```rust
pub fn narrate(fields: NarrationFields) {
    // 1. Always log to stdout (for pool-manager)
    tracing::event!(Level::INFO, ...);
    
    // 2. If we're in an HTTP request context, also emit SSE
    if let Some(tx) = get_current_narration_sender() {
        let _ = tx.send(InferenceEvent::Narration {
            actor: fields.actor.to_string(),
            action: fields.action.to_string(),
            target: fields.target,
            human: fields.human,
            cute: fields.cute,
            story: fields.story,
            correlation_id: fields.correlation_id,
            job_id: fields.job_id,
        });
    }
}
```

#### 4. Merge Narration Events into SSE Stream

**File**: `src/http/execute.rs`

```rust
// Merge narration events with token events
let narration_stream = tokio_stream::wrappers::UnboundedReceiverStream::new(narration_rx);
let token_stream = /* ... existing token stream ... */;

// Interleave them
let merged_stream = stream::select(narration_stream, token_stream);
```

---

## ğŸ“Š Event Flow Diagrams

### Stdout Flow (Worker Lifecycle)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pool-Manager    â”‚
â”‚                 â”‚
â”‚ 1. Spawns       â”‚
â”‚    worker       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Binary   â”‚
â”‚                 â”‚
â”‚ narrate()       â”‚
â”‚   â†“             â”‚
â”‚ tracing::event()â”‚
â”‚   â†“             â”‚
â”‚ stdout          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pool-Manager    â”‚
â”‚ Captures stdout â”‚
â”‚                 â”‚
â”‚ Logs:           â”‚
â”‚ "Worker started"â”‚
â”‚ "Model loaded"  â”‚
â”‚ "Server ready"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SSE Flow (Per-Request)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User's Screen   â”‚
â”‚                 â”‚
â”‚ "Starting..."   â”‚
â”‚ "Tokenizing..." â”‚
â”‚ "Generated 10"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ SSE stream
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestrator    â”‚
â”‚ Relays events   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ SSE stream
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker          â”‚
â”‚ /execute        â”‚
â”‚                 â”‚
â”‚ narrate()       â”‚
â”‚   â†“             â”‚
â”‚ SSE channel     â”‚
â”‚   â†“             â”‚
â”‚ HTTP response   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Benefits of Dual Output

### Stdout (Pool-Manager)
- âœ… Operational monitoring
- âœ… Worker lifecycle tracking
- âœ… Startup/shutdown diagnostics
- âœ… Model loading verification
- âœ… Server health monitoring

### SSE (User)
- âœ… Real-time inference progress
- âœ… Transparency (user sees what's happening)
- âœ… Better UX (not just waiting for tokens)
- âœ… Debugging (user can see where it's slow)
- âœ… Trust (user sees the system working)

---

## ğŸš¨ Critical Distinction

**NOT redundant!** They serve different purposes:

| Aspect | Stdout | SSE |
|--------|--------|-----|
| **When** | Worker lifecycle (startup/shutdown) | During inference request |
| **Who** | Pool-manager (operator) | End user (via orchestrator) |
| **Why** | Operational monitoring | User experience |
| **What** | "Worker started", "Model loaded" | "Tokenizing...", "Generated 10 tokens" |
| **Frequency** | ~13 events per worker lifetime | ~8 events per request |

---

## ğŸ“ Implementation Checklist

### Phase 1: SSE Narration Events
- [ ] Add `Narration` variant to `InferenceEvent` enum
- [ ] Create narration channel in execute handler
- [ ] Store channel in request-local context
- [ ] Modify `narrate()` to check for SSE channel
- [ ] Merge narration events into SSE stream
- [ ] Test: User sees narration events in real-time

### Phase 2: Stdout Narration (Already Done)
- [x] Worker startup events
- [x] Device initialization events
- [x] Model loading events
- [x] Pool-manager callback events
- [x] Server lifecycle events

### Phase 3: Documentation
- [ ] Update OpenAPI spec with narration events
- [ ] Document event ordering
- [ ] Add examples to API docs
- [ ] Update orchestrator relay logic

---

## ğŸ¯ Expected User Experience

### User's Screen (Orchestrator PC)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference Request: "Write a haiku about GPUs"           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Narration Panel]                                       â”‚
â”‚ âœ… Inference request validated for job job-123          â”‚
â”‚ ğŸš€ Starting inference (prompt: 28 chars, max_tokens: 50)â”‚
â”‚ ğŸ° Tokenized prompt (7 tokens)                          â”‚
â”‚ ğŸ§¹ Reset KV cache before inference                      â”‚
â”‚ ğŸ¯ Generated 10 tokens                                  â”‚
â”‚ ğŸ¯ Generated 20 tokens                                  â”‚
â”‚ ğŸ‰ Inference completed (42 tokens in 250 ms, 168 tok/s) â”‚
â”‚                                                          â”‚
â”‚ [Token Stream] (goes to AI agent)                       â”‚
â”‚ Silicon dreams ignite                                   â”‚
â”‚ Parallel cores dance as one                             â”‚
â”‚ CUDA's warm embrace                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pool-Manager Logs

```
[2025-10-09T13:27:00Z] INFO worker-gpu0-r1: Starting Candle worker on port 8080
[2025-10-09T13:27:00Z] INFO worker-gpu0-r1: Initialized CUDA device 0
[2025-10-09T13:27:01Z] INFO worker-gpu0-r1: Loaded Llama model (7000 MB, vocab: 32000)
[2025-10-09T13:27:01Z] INFO worker-gpu0-r1: HTTP server listening on 0.0.0.0:8080
[2025-10-09T13:27:01Z] INFO worker-gpu0-r1: Calling pool-managerd at http://localhost:9000/ready
```

**Both are valuable! Different audiences, different purposes.**

---

*Documented by the Narration Core Team ğŸ€*  
*May your stdout flow to pool-manager and your SSE flow to users! ğŸ’*
