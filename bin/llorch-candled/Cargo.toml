[package]
name = "llorch-candled"
version = "0.1.0"
edition = "2021"
authors = ["TEAM-000 Foundation"]
description = "Candle-based Llama-2 inference worker daemon with CUDA acceleration"
license = "GPL-3.0-or-later"

[dependencies]
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# HTTP SERVER DEPENDENCIES (integrated from worker-http)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# TEAM-015: Integrated worker-common and worker-http directly into binary

axum = "0.7"
tower = "0.4"
tower-http = { version = "0.5", features = ["trace", "cors"] }
futures = "0.3"
reqwest = { version = "0.12", features = ["json"] }

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CANDLE INTEGRATION — Hybrid Approach (Math Functions + Kernels)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Following TEAM_001_CANDLE_CATALOG_PLAN.md: Use Candle for math, our architecture
# - candle-core: Tensor operations, Device abstraction
# - candle-nn: Neural network functions (rms_norm, silu, softmax, etc.)
# - candle-kernels: Optimized CUDA kernels (automatic when CUDA enabled)
# - cudarc: CUDA runtime for kernel invocation
# Modified by: TEAM-001
# Note: Using published versions from crates.io (workspace inheritance incompatible)

# Candle core for tensor operations
candle-core = "0.9"

# Candle neural network functions (rms_norm, silu, softmax, linear_no_bias)
candle-nn = "0.9"

# TEAM-008: Use candle-transformers for complete model implementations
candle-transformers = "0.9"

# Candle kernels for CUDA acceleration (optional, feature-gated)
# TEAM-007: Added CUDA version feature for cudarc
candle-kernels = { version = "0.9", optional = true }
cudarc = { version = "0.11", optional = true, features = ["cuda-12050"] }

# HuggingFace tokenizers (used by Candle examples)
tokenizers = "0.15"

# CPU tensor operations (keep for conversion helpers and legacy code)
ndarray = "0.15"

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# ASYNC RUNTIME
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# CRITICAL: Single-threaded runtime for optimal CPU performance
# TEAM-015: Added signal feature for graceful shutdown

tokio = { version = "1", features = ["rt", "macros", "sync", "time", "signal"] }
async-trait = "0.1"

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# UTILITIES
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json", "env-filter"] }
clap = { version = "4", features = ["derive"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
# TEAM-014: Removed rand = "0.8" (no longer needed, using Candle's LogitsProcessor)

[dev-dependencies]
# Testing utilities
approx = "0.5"
chrono = "0.4"
criterion = "0.5"

[features]
default = []

# Backend features (mutually exclusive at build time)
# Modified by: TEAM-007
cpu = []
cuda = ["candle-kernels", "cudarc", "candle-core/cuda", "candle-nn/cuda"]
accelerate = ["candle-core/accelerate", "candle-nn/accelerate"]

# Performance benchmarking
benchmark = []

# TEAM-007: Multi-backend binaries
# Each binary is feature-gated to a specific backend
[[bin]]
name = "llorch-cpu-candled"
path = "src/bin/cpu.rs"
required-features = ["cpu"]

[[bin]]
name = "llorch-cuda-candled"
path = "src/bin/cuda.rs"
required-features = ["cuda"]

[[bin]]
name = "llorch-accelerate-candled"
path = "src/bin/accelerate.rs"
required-features = ["accelerate"]

# Legacy binary (for backward compatibility during migration)
# TEAM-013: Requires CPU feature
[[bin]]
name = "llorch-candled"
path = "src/main.rs"
required-features = ["cpu"]

[lib]
name = "llorch_candled"
path = "src/lib.rs"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
strip = true

[profile.dev]
opt-level = 0
debug = true
