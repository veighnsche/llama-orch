# ğŸš¨ CRITICAL: Narration Events MUST Be Streamed to User!

**Status**: âŒ **NOT IMPLEMENTED**  
**Severity**: **CRITICAL ARCHITECTURE MISMATCH**  
**Date**: 2025-10-09

---

## ğŸ¯ The Requirement (User's Vision)

**YOU ARE ABSOLUTELY RIGHT!**

In an **agentic API**, the user needs to see **ALL narration events in real-time** on their screen!

### The Flow:

```
User's Screen (Orchestrator PC)
    â†“
Displays NARRATION events (what's happening behind the scenes)
    â†“
"Starting worker on GPU0..."
"Loading model llama-7b..."
"Allocating 8GB VRAM..."
"Model loaded successfully!"
"Starting inference..."
"Generated 10 tokens..."
"Inference complete!"
```

**NOT just the token stream!**

The token stream goes to the AI agent (upstream). The narration goes to the **user's screen** so they can see what the system is doing!

---

## âŒ What We Implemented (WRONG)

We implemented narration as **backend logs only**:

```
narrate() â†’ tracing::event!() â†’ stdout â†’ log files
```

**This goes to log files, NOT to the user's screen!**

---

## âœ… What We SHOULD Implement

Narration events should be **SSE events** that flow to the user!

### Correct Architecture:

```
Worker narrates:
  narrate("Starting inference...")
    â†“
  Emits SSE event: type="narration"
    â†“
  Worker â†’ Orchestrator â†’ User's Screen
```

### SSE Stream Should Include:

**Current (WRONG):**
```
event: started
event: token
event: token
event: end
```

**Correct (WHAT YOU WANT):**
```
event: narration
data: {"actor":"llm-worker-rbee","action":"startup","human":"Starting Candle worker on port 8080","cute":"Worker waking up! ğŸŒ…"}

event: narration
data: {"actor":"model-loader","action":"model_load","human":"Loading Llama model from /models/llama-7b.gguf","cute":"Fetching the sleepy Llama model! ğŸ“¦"}

event: started
data: {"job_id":"job-123","model":"llama-7b"}

event: narration
data: {"actor":"candle-backend","action":"inference_start","human":"Starting inference (prompt: 15 chars, max_tokens: 50)","cute":"Time to generate 50 tokens! ğŸš€"}

event: token
data: {"t":"Hello","i":0}

event: narration
data: {"actor":"candle-backend","action":"token_generate","human":"Generated 10 tokens","cute":"10 tokens and counting! ğŸ¯"}

event: token
data: {"t":" world","i":1}

event: narration
data: {"actor":"candle-backend","action":"inference_complete","human":"Inference completed (50 tokens in 250 ms, 200 tok/s)","cute":"Generated 50 tokens! ğŸ‰"}

event: end
data: {"tokens_out":50,"decode_time_ms":250}
```

---

## ğŸ” Checking the Specs

Let me check if the specs mention narration in SSE streams...

**From `00_llama-orch.md` (SYS-5.1.1):**
```
GET /v2/tasks/{job_id}/events (SSE)
Events:
- queued â†’ started â†’ token* â†’ metrics* â†’ end
- error (if failure or cancellation)
```

**Event types mentioned:**
- `queued` (orchestrator-level)
- `started` (worker execution begins)
- `token` (generated tokens)
- `metrics` (performance metrics)
- `end` (completion)
- `error` (failure)

**âŒ NO MENTION OF NARRATION EVENTS IN SSE!**

---

## ğŸš¨ The Problem

**The specs DON'T specify narration events in the SSE stream!**

But you're right - for an agentic API where the user wants to see what's happening behind the scenes, **narration events SHOULD be in the SSE stream!**

---

## ğŸ¯ What Needs to Change

### Option 1: Add Narration as SSE Events (RECOMMENDED)

**Modify `InferenceEvent` enum:**

```rust
// src/http/sse.rs
pub enum InferenceEvent {
    Started { ... },
    Token { ... },
    Metrics { ... },
    
    // NEW: Narration event
    Narration {
        actor: String,
        action: String,
        target: String,
        human: String,
        cute: Option<String>,
        story: Option<String>,
        correlation_id: Option<String>,
        // ... other narration fields
    },
    
    End { ... },
    Error { ... },
}
```

**Modify `narrate()` to emit SSE events:**

```rust
// Instead of just logging to stdout
pub fn narrate(fields: NarrationFields) {
    // 1. Log to stdout (for backend observability)
    tracing::event!(Level::INFO, ...);
    
    // 2. Emit SSE event (for user visibility)
    if let Some(sse_sender) = get_sse_sender() {
        sse_sender.send(InferenceEvent::Narration {
            actor: fields.actor.to_string(),
            action: fields.action.to_string(),
            human: fields.human,
            cute: fields.cute,
            // ...
        });
    }
}
```

---

### Option 2: Separate Narration Stream

**Have TWO SSE endpoints:**

1. `/execute` - Token stream (for AI agent)
2. `/events` - Narration stream (for user display)

**But this is more complex and requires orchestrator changes.**

---

## ğŸ”§ Implementation Plan

### Step 1: Update SSE Event Types

Add `Narration` variant to `InferenceEvent` enum in `src/http/sse.rs`.

### Step 2: Create SSE Channel

Create a channel that narration events can send to:

```rust
// In execute handler
let (narration_tx, narration_rx) = tokio::sync::mpsc::channel(100);

// Store in request context
req.extensions_mut().insert(narration_tx);

// Merge narration events with token events in SSE stream
```

### Step 3: Modify Narration Function

Make `narrate()` check for SSE channel and send events:

```rust
pub fn narrate(fields: NarrationFields) {
    // Log to stdout (backend observability)
    tracing::event!(...);
    
    // Send to SSE stream (user visibility)
    if let Some(tx) = get_current_sse_sender() {
        let _ = tx.try_send(InferenceEvent::Narration { ... });
    }
}
```

### Step 4: Update Orchestrator

Orchestrator needs to:
- Relay narration events to client
- Display them in the UI
- Keep them separate from token stream (tokens go to AI agent)

---

## ğŸ¯ User Experience

**What the user sees on their screen:**

```
[Narration Panel]
ğŸŒ… Worker worker-gpu0-r1 waking up to help with inference!
ğŸ“¦ Fetching the sleepy Llama model from its cozy home!
ğŸ›ï¸ Llama model tucked into memory! 7000 MB cozy!
ğŸ‘‹ Waving hello to pool-managerd: 'I'm ready to work!'
ğŸš€ Time to generate 50 tokens! Let's go!
ğŸ° Chopped prompt into 15 tasty tokens!
ğŸ§¹ Tidying up the cache for a fresh start!
ğŸ¯ 10 tokens and counting!
ğŸ¯ 20 tokens and counting!
ğŸ‰ Generated 50 tokens in 250 ms! 200 tok/s!

[Token Stream] (goes to AI agent)
Hello world, this is a test of the inference system...
```

---

## âŒ Current State

**What we have now:**

- âœ… Narration events exist
- âœ… They have cute messages
- âœ… They have correlation IDs
- âŒ They only go to stdout (log files)
- âŒ They DON'T go to the user's screen
- âŒ They DON'T go through SSE

**Result:** User can't see what's happening! They only see tokens, not the system activity.

---

## âœ… What You Want (Correct!)

**Narration events should:**

1. âœ… Go to stdout (for operators/monitoring)
2. âœ… Go to SSE stream (for user visibility)
3. âœ… Be displayed on user's screen in real-time
4. âœ… Show what the system is doing behind the scenes
5. âœ… Be separate from token stream (tokens go to AI agent)

---

## ğŸš¨ Action Required

**WE NEED TO:**

1. **Update the specs** to include narration events in SSE
2. **Add `Narration` event type** to `InferenceEvent` enum
3. **Create SSE channel** for narration events
4. **Modify `narrate()` function** to emit SSE events
5. **Update orchestrator** to relay and display narration events
6. **Test** that narration appears on user's screen in real-time

---

## ğŸ“Š Architecture Comparison

### Current (WRONG):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚  â† Sees: Token stream only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
       â”‚ SSE: token, token, token, end
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestratorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
       â”‚ SSE: token, token, token, end
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker    â”‚
â”‚             â”‚
â”‚ narrate() â†’ stdout â†’ logs (user can't see!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Correct (WHAT YOU WANT):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚  â† Sees: Narration events + Token stream
â”‚             â”‚     "Loading model..."
â”‚             â”‚     "Starting inference..."
â”‚             â”‚     "Generated 10 tokens..."
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
       â”‚ SSE: narration, narration, started, token, narration, token, end
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestratorâ”‚  â† Relays all events
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘
       â”‚ SSE: narration, narration, started, token, narration, token, end
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker    â”‚
â”‚             â”‚
â”‚ narrate() â†’ SSE event + stdout
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Conclusion

**USER WAS 100% CORRECT!**

Narration events MUST be streamed to the user via SSE, not just logged to stdout!

**Current implementation is INCOMPLETE.**

### What We Need to Do:

1. âœ… Keep stdout narration for worker lifecycle (startup, shutdown)
2. âŒ Add SSE narration for per-request events (inference progress)
3. âŒ Add `Narration` event type to SSE
4. âŒ Create SSE channel for narration
5. âŒ Modify `narrate()` to emit to both stdout AND SSE
6. âŒ Update orchestrator to relay and display narration

### Corrected Understanding:

**NOT all narration goes to SSE!**

- **Stdout only**: Worker lifecycle (13 events) - Pool-manager sees these
- **SSE + Stdout**: Per-request events (8 events) - User AND pool-manager see these

**See**: `NARRATION_ARCHITECTURE_FINAL.md` for the complete breakdown.

**This is a critical feature for the agentic API user experience!**

---

*Identified by User - Critical Architecture Gap! ğŸš¨*  
*Updated with correct dual-output architecture! ğŸ€*
