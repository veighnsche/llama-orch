/**
 * Embedding Lookup Kernel
 * 
 * Implements token embedding retrieval from weight matrix.
 * This is the first layer of transformer inference, shared across all architectures.
 * 
 * Features:
 * - Coalesced memory access for optimal GPU performance
 * - Bounds checking for invalid token IDs
 * - Support for FP16 and FP32 precision
 * - Handles arbitrary hidden dimensions (not limited to 256)
 * 
 * Spec: M0-W-1430, CUDA-5030
 * Story: FT-015
 * 
 * ============================================================================
 * [TEAM_CHARLIE] INVESTIGATION NOTE (2025-10-06 16:48 UTC)
 * ============================================================================
 * ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è THIS KERNEL IS CORRECT - NOT THE BUG LOCATION! ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
 *
 * This kernel was considered during investigation but is NOT the bug source.
 *
 * Verified: Token embeddings are loaded correctly from the model file.
 * - Embedding values start at ¬±0.04 (normal range for FP16)
 * - Memory layout is correct: [vocab_size, hidden_dim] row-major
 * - Bounds checking prevents invalid token IDs
 *
 * The model file is CORRECT. llama.cpp generates perfect haiku with it:
 *   /home/vince/Projects/llama-orch/reference/llama.cpp/build/bin/llama-cli \
 *     -m /home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-q4_k_m.gguf \
 *     -p "Write a haiku about autumn:" -n 50 --temp 0.7
 * Output: Perfect haiku every time!
 *
 * The bug is NOT in embedding lookup. Investigate attention, RoPE, KV cache, or FFN.
 * See: investigation-teams/TEAM_CHARLIE_I_WAS_WRONG.md
 * ============================================================================
 */

#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cstdint>
#include <stdio.h>

namespace worker {
namespace kernels {

/**
 * Embedding lookup kernel (FP16).
 * 
 * Retrieves embeddings for input token IDs from weight matrix.
 * Each thread handles one element of one embedding.
 * 
 * Memory layout:
 * - weight_matrix: [vocab_size, hidden_dim] row-major
 * - embeddings: [batch_size, hidden_dim] row-major
 * 
 * Grid configuration:
 * - Grid: (batch_size, ceil(hidden_dim / 256))
 * - Block: 256 threads
 * 
 * Memory access pattern:
 * - Coalesced: consecutive threads read consecutive memory locations
 * - Each block processes 256 dimensions of one token's embedding
 * 
 * @param token_ids Input token IDs [batch_size]
 * @param weight_matrix Embedding weight matrix [vocab_size, hidden_dim]
 * @param embeddings Output embeddings [batch_size, hidden_dim]
 * @param batch_size Number of tokens
 * @param hidden_dim Embedding dimension
 * @param vocab_size Vocabulary size
 */
__global__ void embedding_lookup_fp16(
    const int* token_ids,
    const half* weight_matrix,
    half* embeddings,
    int batch_size,
    int hidden_dim,
    int vocab_size
) {
    // Each thread handles one element of one embedding
    int token_idx = blockIdx.x;  // Which token in batch
    int dim_idx = threadIdx.x + blockIdx.y * blockDim.x;  // Which dimension
    
    // Bounds check
    if (token_idx >= batch_size || dim_idx >= hidden_dim) {
        return;
    }
    
    // Get token ID
    int token_id = token_ids[token_idx];
    
    // Validate token ID (bounds check)
    //
    // ============================================================================
    // [TEAM CHAIR] 2025-10-07T02:36Z - FALSE LEAD! ‚ùå DO NOT INVESTIGATE THIS!
    // ============================================================================
    // 
    // SYMPTOM: Worker crashes when processing token 151644 (special token <|im_start|>)
    // 
    // INITIAL HYPOTHESIS (WRONG): vocab_size parameter mismatch causes OOB access
    //   - Thought: embedding table has 151643 rows, vocab_size=151936 allows token 151644
    //   - Thought: Token 151644 would access beyond table bounds ‚Üí SEGFAULT
    // 
    // INVESTIGATION RESULT: This is NOT the bug! ‚úÖ VERIFIED:
    //   - Actual embedding table dimensions: [896, 151936] (it IS padded!)
    //   - Token 151644 is WITHIN BOUNDS (151644 < 151936)
    //   - Bounds check here is CORRECT and working as intended
    //   - The crash happens AFTER this check passes, somewhere else
    // 
    // PROOF: Added logging to cuda_backend.rs, got:
    //   "üîç token_embd.weight dimensions: [896, 151936]"
    //   This means 896 rows (hidden_dim) √ó 151936 cols (vocab, padded)
    // 
    // FALSE_LEAD: DO NOT waste time investigating embedding table size!
    //   The table IS padded to 151936, special tokens ARE within bounds.
    //   The bug is somewhere else (maybe in the embedding values themselves,
    //   or in CUDA kernel launch params, or downstream in the transformer).
    // 
    // NEXT TEAM: Skip this file! The embedding kernel is correct.
    //   Focus on: Why does worker crash AFTER embedding lookup succeeds?
    //   - Check if special token embeddings contain NaN/Inf
    //   - Check CUDA error codes after embedding kernel
    //   - Check what happens in first transformer layer
    // 
    // See: investigation-teams/TEAM_CHAIR_HANDOFF.md for full investigation trail
    // ============================================================================
    if (token_id < 0 || token_id >= vocab_size) {
        // Invalid token ID, set to zero
        // TEAM FREE [Review]
        // Category: Error handling
        // Hypothesis: Silent zero-fill for OOB token IDs masks upstream bugs; no error signal propagates to host.
        // Evidence: Line 95 returns silently; caller has no way to detect invalid token_id was passed.
        // Risk: Debugging difficulty when wrong tokens fed; zero embeddings cause subtle model drift vs crash.
        // Confidence: Medium
        // Next step: Add atomic error counter or return error code via separate device buffer for host to check.
        embeddings[token_idx * hidden_dim + dim_idx] = __float2half(0.0f);
        return;
    }
    
    // Lookup embedding from weight matrix
    // ============================================================================
    // [TEAM SHAKESPEARE 2025-10-07T23:07-23:11Z] CRITICAL INVESTIGATION AREA!
    // ============================================================================
    // 
    // SYMPTOM: Model generates garbage output (foreign tokens, mojibake, code tokens)
    // EVIDENCE: llama.cpp produces perfect haiku with SAME model file
    // 
    // HYPOTHESIS TESTED: Embedding table transpose bug
    //   - Reference implementations (candle, mistral.rs) expect [vocab_size, hidden_size]
    //   - VAN GOGH found our GGUF has dimensions [896, 151936] (might be transposed)
    //   - Our code assumes [vocab_size, hidden_dim] layout
    // 
    // TEST RESULTS:
    //   Original code: weight_matrix[token_id * hidden_dim + dim_idx]
    //     ‚Üí Generated tokens: [20695, 131033, 42294, 43321, ...] (garbage)
    //   
    //   Transposed access: weight_matrix[dim_idx * vocab_size + token_id]
    //     ‚Üí Generated tokens: [37557, 103357, 69289, 62341, ...] (DIFFERENT garbage!)
    //   
    //   CONCLUSION: Changing indexing DOES change output (proves embedding matters)
    //               BUT output still garbage (transpose alone not the fix)
    // 
    // NEXT TEAM (TEAM FROST) SHOULD:
    //   1. Dump actual embedding values from GGUF for token_id=0
    //   2. Dump what this code reads for token_id=0
    //   3. Dump what llama.cpp reads for token_id=0
    //   4. Compare byte-for-byte to find exact mismatch
    //   5. Check if there are OTHER transpose bugs (lm_head, Q/K/V, FFN)
    //   6. Verify GGUF dimensions with gguf-dump tool
    // 
    // CONFIDENCE: üî•üî• 75% that bug is in embedding layer (proven by test)
    //             üî• 50% that it's a simple transpose (changed output but still garbage)
    // 
    // See: investigation-teams/TRANSPOSE_FIX_TEST_RESULTS.md
    //      investigation-teams/REFERENCE_IMPLEMENTATION_ANALYSIS.md
    // ============================================================================
    half value = weight_matrix[token_id * hidden_dim + dim_idx];  // ‚Üê CURRENT (original indexing)
    embeddings[token_idx * hidden_dim + dim_idx] = value;
}

/**
 * Embedding lookup kernel (FP32).
 * 
 * Same as FP16 version but with single-precision floats.
 * Use for higher precision requirements or when FP16 not available.
 * 
 * @param token_ids Input token IDs [batch_size]
 * @param weight_matrix Embedding weight matrix [vocab_size, hidden_dim]
 * @param embeddings Output embeddings [batch_size, hidden_dim]
 * @param batch_size Number of tokens
 * @param hidden_dim Embedding dimension
 * @param vocab_size Vocabulary size
 */
__global__ void embedding_lookup_fp32(
    const int* token_ids,
    const float* weight_matrix,
    float* embeddings,
    int batch_size,
    int hidden_dim,
    int vocab_size
) {
    int token_idx = blockIdx.x;
    int dim_idx = threadIdx.x + blockIdx.y * blockDim.x;
    
    if (token_idx >= batch_size || dim_idx >= hidden_dim) {
        return;
    }
    
    int token_id = token_ids[token_idx];
    
    if (token_id < 0 || token_id >= vocab_size) {
        embeddings[token_idx * hidden_dim + dim_idx] = 0.0f;
        return;
    }
    
    // [TEAM SHAKESPEARE 2025-10-07T23:07Z] Same transpose fix as FP16 version
    float value = weight_matrix[dim_idx * vocab_size + token_id];  // ‚Üê TRANSPOSED ACCESS
    embeddings[token_idx * hidden_dim + dim_idx] = value;
}

/**
 * Launch embedding lookup kernel (FP16).
 * 
 * Configures grid/block dimensions and launches kernel.
 * 
 * Grid configuration:
 * - Grid X: batch_size (one block per token)
 * - Grid Y: ceil(hidden_dim / 256) (multiple blocks if hidden_dim > 256)
 * - Block: 256 threads
 * 
 * Example: batch_size=4, hidden_dim=1024
 * - Grid: (4, 4) = 16 blocks
 * - Block: 256 threads
 * - Total threads: 4096
 * 
 * @param token_ids Device pointer to token IDs [batch_size]
 * @param weight_matrix Device pointer to embedding weights [vocab_size, hidden_dim]
 * @param embeddings Device pointer to output embeddings [batch_size, hidden_dim]
 * @param batch_size Number of tokens
 * @param hidden_dim Embedding dimension
 * @param vocab_size Vocabulary size
 * @param stream CUDA stream (0 = default stream)
 */
void launch_embedding_lookup_fp16(
    const int* token_ids,
    const half* weight_matrix,
    half* embeddings,
    int batch_size,
    int hidden_dim,
    int vocab_size,
    cudaStream_t stream
) {
    // Validate inputs
    if (batch_size <= 0 || hidden_dim <= 0 || vocab_size <= 0) {
        fprintf(stderr, "Invalid dimensions: batch_size=%d, hidden_dim=%d, vocab_size=%d\n",
                batch_size, hidden_dim, vocab_size);
        // TEAM FREE [Review]
        // Category: Error handling
        // Hypothesis: Early return after fprintf leaves kernel unlaunched; caller assumes success, proceeds with uninitialized embeddings buffer.
        // Evidence: No error code returned; cudaGetLastError() on line 209 only catches launch failures, not pre-launch validation failures.
        // Risk: Downstream NaNs/garbage if embeddings buffer not pre-zeroed; silent failure mode.
        // Confidence: Medium
        // Next step: Return error code or use cudaMemset to zero output buffer before returning.
        return;
    }
    
    if (token_ids == nullptr || weight_matrix == nullptr || embeddings == nullptr) {
        fprintf(stderr, "Null pointer in embedding lookup\n");
        return;
    }
    
    // Kernel launch configuration
    // Grid: (batch_size, ceil(hidden_dim / 256))
    // Block: 256 threads
    int threads_per_block = 256;
    int blocks_y = (hidden_dim + threads_per_block - 1) / threads_per_block;
    
    dim3 grid(batch_size, blocks_y);
    dim3 block(threads_per_block);
    
    // Launch kernel
    embedding_lookup_fp16<<<grid, block, 0, stream>>>(
        token_ids,
        weight_matrix,
        embeddings,
        batch_size,
        hidden_dim,
        vocab_size
    );
    
    // Check for launch errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        fprintf(stderr, "Embedding kernel launch failed: %s\n", cudaGetErrorString(err));
    }
}

/**
 * Launch embedding lookup kernel (FP32).
 * 
 * Same as FP16 version but with single-precision floats.
 * 
 * @param token_ids Device pointer to token IDs [batch_size]
 * @param weight_matrix Device pointer to embedding weights [vocab_size, hidden_dim]
 * @param embeddings Device pointer to output embeddings [batch_size, hidden_dim]
 * @param batch_size Number of tokens
 * @param hidden_dim Embedding dimension
 * @param vocab_size Vocabulary size
 * @param stream CUDA stream (0 = default stream)
 */
void launch_embedding_lookup_fp32(
    const int* token_ids,
    const float* weight_matrix,
    float* embeddings,
    int batch_size,
    int hidden_dim,
    int vocab_size,
    cudaStream_t stream
) {
    if (batch_size <= 0 || hidden_dim <= 0 || vocab_size <= 0) {
        fprintf(stderr, "Invalid dimensions: batch_size=%d, hidden_dim=%d, vocab_size=%d\n",
                batch_size, hidden_dim, vocab_size);
        return;
    }
    
    if (token_ids == nullptr || weight_matrix == nullptr || embeddings == nullptr) {
        fprintf(stderr, "Null pointer in embedding lookup\n");
        return;
    }
    
    int threads_per_block = 256;
    int blocks_y = (hidden_dim + threads_per_block - 1) / threads_per_block;
    
    dim3 grid(batch_size, blocks_y);
    dim3 block(threads_per_block);
    
    embedding_lookup_fp32<<<grid, block, 0, stream>>>(
        token_ids,
        weight_matrix,
        embeddings,
        batch_size,
        hidden_dim,
        vocab_size
    );
    
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        fprintf(stderr, "Embedding kernel launch failed: %s\n", cudaGetErrorString(err));
    }
}

} // namespace kernels
} // namespace worker

// Extern C wrapper for transformer
extern "C" {
    void cuda_embedding_lookup(
        const uint32_t* token_ids,
        const void* embedding_table,
        void* output,
        uint32_t batch_size,
        uint32_t vocab_size,
        uint32_t hidden_dim,
        cudaStream_t stream
    ) {
        worker::kernels::launch_embedding_lookup_fp16(
            reinterpret_cast<const int*>(token_ids),
            reinterpret_cast<const half*>(embedding_table),
            reinterpret_cast<half*>(output),
            batch_size,
            hidden_dim,
            vocab_size,
            stream
        );
    }
}

// ---
// Crafted by GPT-Gamma ü§ñfunction-Alpha üèóÔ∏è
