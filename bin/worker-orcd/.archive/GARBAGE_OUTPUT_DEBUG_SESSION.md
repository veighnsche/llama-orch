# Garbage Output Debugging Session - 2025-10-06

## Initial Problem Statement

**User Report**: Haiku generation test (`haiku_generation_anti_cheat.rs`) produces "garbage output" despite the model (Qwen2.5-0.5B FP16) working correctly in Ollama and LM Studio.

**Test File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/tests/haiku_generation_anti_cheat.rs`

**Explicit Note in Test** (lines 56-64):
```rust
#[tokio::test(flavor = "multi_thread")]
#[cfg(feature = "cuda")]
#[ignore] // Real inference but garbage output. Run with --ignored
async fn test_haiku_generation_STUB_PIPELINE_ONLY() {
    // ‚ö†Ô∏è  WARNING: Real inference but producing garbage output
    eprintln!("‚ö†Ô∏è  WARNING: REAL INFERENCE - BUT GARBAGE OUTPUT");
    eprintln!("‚ö†Ô∏è  This test uses real GPU inference, but output quality is poor");
    eprintln!("‚ö†Ô∏è  Debugging needed for coherent text generation");
```

**Key Constraint**: The model itself is NOT the problem - it works in other environments.

## Investigation Phase

### Step 1: Verify Real Inference Path

**Question**: Is the test using a stub or real inference?

**Files Examined**:
- `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs`
- `/home/vince/Projects/llama-orch/bin/worker-orcd/src/cuda/real_inference.rs`
- `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/src/inference_impl.cpp` (stub)
- `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/src/ffi_inference.cpp`

**Finding**: The inference path is REAL, not stubbed:
```
CudaInferenceBackend::execute()
  ‚Üí RealInference::init() / generate_token()
    ‚Üí FFI: cuda_inference_init() / cuda_inference_generate_token()
      ‚Üí QwenTransformer::forward() in cuda/src/transformer/qwen_transformer.cpp
```

The stub in `inference_impl.cpp` is NOT used by `CudaInferenceBackend`.

### Step 2: Examine Model Configuration

**Files Examined**:
- `/home/vince/Projects/llama-orch/bin/worker-orcd/src/cuda/model.rs` (lines 64-100)
- `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs` (lines 90-115)

**Finding**: Hardcoded model configuration!

```rust
// BEFORE FIX - model.rs line 70-78
// TEMPORARY: Hardcode Qwen2.5-0.5B config
// TODO: Parse from GGUF metadata when parser is complete
let vocab_size = 151936u32;
let hidden_dim = 896u32;
let num_layers = 24u32;
let num_heads = 14u32;
let num_kv_heads = 2u32;
let context_length = 32768u32;
```

```rust
// BEFORE FIX - cuda_backend.rs line 92-93
// TODO: Fix vocab_size() in tokenizer - currently returns 0
// For now, hardcode Qwen2.5-0.5B vocab size
let vocab_size = 151936u32; // Qwen2.5-0.5B
```

```rust
// BEFORE FIX - cuda_backend.rs line 105
let ffn_dim = hidden_dim * 4; // Standard transformer FFN ratio
```

**Problem**: 
- If the actual model differs from these hardcoded values, dimension mismatches cause garbage
- Qwen models don't always use 4√ó FFN ratio
- vocab_size mismatch corrupts sampling buffer

### Step 3: Examine GQA Attention KV Cache

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/gqa_attention.cu`

**Finding**: Critical bug in KV cache writes (lines 155-165 in decode kernel):

```cpp
// BEFORE FIX - line 156
if (kv_cache_k != nullptr && q_head < num_kv_heads) {
    int k_idx = batch * num_kv_heads * head_dim + kv_head * head_dim + d;
    int cache_write_idx = batch * num_kv_heads * max_seq_len * head_dim +
                          kv_head * max_seq_len * head_dim +
                          cache_len * head_dim + d;
    kv_cache_k[cache_write_idx] = k_current[k_idx];
    kv_cache_v[cache_write_idx] = v_current[v_idx];
}
```

**Problem**: For Qwen2.5-0.5B with 14 query heads and 2 KV heads:
- Condition `q_head < num_kv_heads` means only q_head=0 and q_head=1 write to cache
- Both write to `kv_head = q_head / (num_q_heads / num_kv_heads) = 0`
- Result: `kv_head=1` is NEVER updated, corrupting attention for half the heads

**Same bug in prefill kernel** (lines 212-220).

### Step 4: Examine Prefill Logic

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs` (lines 144-159)

**Finding**: Incorrect prefill implementation:

```rust
// BEFORE FIX - lines 144-159
// Process prompt tokens (prefill phase)
let mut current_token = token_ids[0];
for &token_id in &token_ids[1..] {
    current_token = inference.generate_token(
        current_token,
        0.0, // Greedy for prefill
        0,
        1.0,
        config.seed,
    )?;
    // Verify we're following the prompt
    if current_token != token_id {
        // This is expected during prefill - we're feeding the prompt
        current_token = token_id;
    }
}
```

**Problem**: 
- Calls `generate_token()` for token[0], gets sampled output, discards it
- Sets `current_token = token_ids[1]`
- Calls `generate_token()` for token[1], gets sampled output, discards it
- Sets `current_token = token_ids[2]`
- Result: Only the LAST prompt token ends up in KV cache, all context is lost!

### Step 5: Examine Prefill Kernel Cache Indexing

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/gqa_attention.cu` (lines 212-220)

**Finding**: Prefill kernel uses wrong stride:

```cpp
// BEFORE FIX - lines 214-219
if (kv_cache_k != nullptr && q_head < num_kv_heads) {
    int cache_idx = batch * num_kv_heads * 1 * head_dim +  // ‚ùå Should be max_seq_len, not 1!
                    kv_head * 1 * head_dim +
                    0 * head_dim + d;  // pos = 0
    kv_cache_k[cache_idx] = k[k_idx];
    kv_cache_v[cache_idx] = v[v_idx];
}
```

**Problem**: 
- Cache layout is `[batch, kv_head, max_seq_len, head_dim]`
- Prefill kernel uses stride of `1` instead of `max_seq_len`
- Writes to wrong memory location, corrupting cache

### Step 6: Examine GGUF Metadata Parsing

**File**: `/home/vince/Projects/llama-orch/bin/worker-crates/worker-gguf/src/lib.rs` (lines 136-144)

**Finding**: vocab_size() only looks for `tokenizer.ggml.tokens`:

```rust
pub fn vocab_size(&self) -> Result<usize, GGUFError> {
    match self.metadata.get("tokenizer.ggml.tokens") {
        Some(MetadataValue::Array { count, .. }) => Ok(*count as usize),
        _ => Err(GGUFError::MissingKey("tokenizer.ggml.tokens".to_string())),
    }
}
```

**Problem**: FP16 model doesn't have `tokenizer.ggml.tokens` key, causing model load to fail.

## Fixes Applied

### Fix 1: GQA KV Cache Write Condition

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/gqa_attention.cu`

**Lines Changed**: 157, 214

```cpp
// AFTER FIX - Decode kernel (line 157)
if (kv_cache_k != nullptr && (q_head % (num_q_heads / num_kv_heads) == 0)) {
    int k_idx = batch * num_kv_heads * head_dim + kv_head * head_dim + d;
    int cache_write_idx = batch * num_kv_heads * max_seq_len * head_dim +
                          kv_head * max_seq_len * head_dim +
                          cache_len * head_dim + d;
    kv_cache_k[cache_write_idx] = k_current[k_idx];
    kv_cache_v[cache_write_idx] = v_current[v_idx];
}

// AFTER FIX - Prefill kernel (line 214)
if (kv_cache_k != nullptr && (q_head % (num_q_heads / num_kv_heads) == 0)) {
    // ... (but this kernel is now disabled, see Fix 5)
}
```

**Explanation**: 
- For 14 query heads and 2 KV heads, groups are [0-6] ‚Üí kv_head=0, [7-13] ‚Üí kv_head=1
- Write once per group: q_head=0 writes kv_head=0, q_head=7 writes kv_head=1
- Both KV heads now get updated correctly

### Fix 2: Read Model Config from GGUF

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/src/cuda/model.rs`

**Lines Changed**: 69-100

```rust
// AFTER FIX
eprintln!("ü¶Ä [Rust] Loading model with Rust weight loading + Q4_K dequantization");

// Parse model configuration from GGUF metadata
let meta = GGUFMetadata::from_file(model_path)
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to parse GGUF metadata: {}", e)))?;

// Try to get vocab_size from metadata, fallback to tensor dimensions
let vocab_size = match meta.vocab_size() {
    Ok(size) => size as u32,
    Err(_) => {
        // Fallback: derive from token_embd.weight tensor
        eprintln!("‚ö†Ô∏è  [Rust] tokenizer.ggml.tokens not found, deriving vocab_size from token_embd.weight");
        let tensors = GGUFMetadata::parse_tensors(model_path)
            .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to parse tensors: {}", e)))?;
        
        tensors.iter()
            .find(|t| t.name == "token_embd.weight")
            .and_then(|t| t.dimensions.last())
            .map(|&d| d as u32)
            .ok_or_else(|| CudaError::ModelLoadFailed("Cannot determine vocab_size".to_string()))?
    }
};
let hidden_dim = meta.hidden_dim()
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to read hidden_dim: {}", e)))? as u32;
let num_layers = meta.num_layers()
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to read num_layers: {}", e)))? as u32;
let num_heads = meta.num_heads()
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to read num_heads: {}", e)))? as u32;
let num_kv_heads = meta.num_kv_heads()
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to read num_kv_heads: {}", e)))? as u32;
let context_length = meta.context_length()
    .map_err(|e| CudaError::ModelLoadFailed(format!("Failed to read context_length: {}", e)))? as u32;

eprintln!(
    "üìã [Rust] Model config (from GGUF): vocab={}, hidden={}, layers={}, heads={}/{} ctx={}",
    vocab_size, hidden_dim, num_layers, num_heads, num_kv_heads, context_length
);
```

**Same fix applied to**: `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs` (lines 91-105)

### Fix 3: Derive FFN Dimension from GGUF

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs`

**Lines Changed**: 115-129

```rust
// AFTER FIX
// Calculate head_dim and derive ffn_dim from GGUF tensors (do not assume 4x)
let head_dim = hidden_dim / num_heads;
let ffn_dim = match worker_gguf::GGUFMetadata::parse_tensors(&self.model_path) {
    Ok(tensors) => {
        // Prefer ffn_up.weight; fall back to ffn_gate.weight
        let mut derived: Option<u32> = None;
        for t in &tensors {
            if t.name == "blk.0.ffn_up.weight" || t.name == "blk.0.ffn_gate.weight" {
                if let Some(&d0) = t.dimensions.first() { derived = Some(d0 as u32); break; }
            }
        }
        derived.unwrap_or(hidden_dim * 4)
    }
    Err(_) => hidden_dim * 4,
};
```

**Explanation**: 
- `blk.0.ffn_up.weight` shape is `[ffn_dim, hidden_dim]`
- Read first dimension to get actual FFN size
- Fallback to 4√ó only if parsing fails

### Fix 4: Correct Prefill Phase

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs`

**Lines Changed**: 144-163

```rust
// AFTER FIX
// Process prompt tokens (prefill phase)
// Feed all prompt tokens through the transformer to build KV cache
// We call generate_token() to run the forward pass, but we ignore the sampled output
// and feed the next prompt token instead (teacher forcing)
for (i, &token_id) in token_ids.iter().enumerate() {
    if i < token_ids.len() - 1 {
        // Prefill: run forward pass with this token, ignore sampled output
        let _ = inference.generate_token(
            token_id,
            0.0, // Greedy (doesn't matter, we ignore output)
            0,
            1.0,
            config.seed,
        )?;
        // Continue with next prompt token (teacher forcing)
    }
}

// Start generation from the last prompt token
let mut current_token = *token_ids.last().unwrap();
```

**Explanation**:
- For prompt `[The, quick, brown, fox]`:
  - Call `generate_token(The)` ‚Üí builds KV cache for "The", ignore sampled output
  - Call `generate_token(quick)` ‚Üí builds KV cache for "quick", ignore sampled output
  - Call `generate_token(brown)` ‚Üí builds KV cache for "brown", ignore sampled output
  - Set `current_token = fox`
  - Now KV cache has [The, quick, brown] and we start generating from "fox"

### Fix 5: Always Use Decode Kernel

**File**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/gqa_attention.cu`

**Lines Changed**: 396-412

```cpp
// AFTER FIX
// Always use decode kernel (it handles cache_len=0 correctly)
// The prefill kernel has a bug where it doesn't use max_seq_len for cache indexing
cuda_gqa_attention_decode(
    output_half,
    q_half,
    k_half,
    v_half,
    k_cache_half,
    v_cache_half,
    batch_size,
    cache_len,
    max_seq_len,
    num_q_heads,
    num_kv_heads,
    head_dim,
    scale
);
```

**Explanation**: 
- Prefill kernel doesn't receive `max_seq_len` parameter, can't compute correct cache indices
- Decode kernel handles `cache_len=0` correctly (first token attends only to itself)
- Simpler to use one kernel for all cases

## Complete Happy Path for Inference

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HTTP Request (POST /execute)                                    ‚îÇ
‚îÇ Body: { job_id, prompt, max_tokens, temperature, seed, ... }   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ worker-http (Axum HTTP Server)                                  ‚îÇ
‚îÇ File: bin/worker-crates/worker-http/src/routes/execute.rs      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 1. Validate request (ExecuteRequest)                           ‚îÇ
‚îÇ 2. Call backend.execute(prompt, config)                        ‚îÇ
‚îÇ 3. Stream SSE events back to client                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CudaInferenceBackend                                            ‚îÇ
‚îÇ File: bin/worker-orcd/src/inference/cuda_backend.rs            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 1. Tokenize prompt ‚Üí token_ids                                 ‚îÇ
‚îÇ 2. Read model config from GGUF metadata                        ‚îÇ
‚îÇ 3. Initialize RealInference context                            ‚îÇ
‚îÇ 4. Prefill phase: feed prompt tokens                           ‚îÇ
‚îÇ 5. Decode phase: generate new tokens                           ‚îÇ
‚îÇ 6. Detokenize and stream via SSE                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RealInference (Rust wrapper)                                    ‚îÇ
‚îÇ File: bin/worker-orcd/src/cuda/real_inference.rs               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ - init(): Create InferenceContext with QwenTransformer         ‚îÇ
‚îÇ - generate_token(): Run forward pass + sampling                ‚îÇ
‚îÇ - reset(): Clear KV cache                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ FFI (unsafe extern "C")
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ C++ FFI Layer                                                   ‚îÇ
‚îÇ File: bin/worker-orcd/cuda/src/ffi_inference.cpp               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ - cuda_inference_init(): Create InferenceContext               ‚îÇ
‚îÇ - cuda_inference_generate_token(): Forward + sample            ‚îÇ
‚îÇ - cuda_inference_reset(): Reset KV cache                       ‚îÇ
‚îÇ - cuda_inference_context_free(): Cleanup                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ QwenTransformer (C++ CUDA)                                      ‚îÇ
‚îÇ File: bin/worker-orcd/cuda/src/transformer/qwen_transformer.cpp‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ forward(token_id, batch_size, output_logits):                  ‚îÇ
‚îÇ   1. Get current position from KV cache                        ‚îÇ
‚îÇ   2. Embed token ‚Üí hidden_states                               ‚îÇ
‚îÇ   3. For each layer:                                           ‚îÇ
‚îÇ      a. Attention RMSNorm                                      ‚îÇ
‚îÇ      b. Q/K/V projections (with biases)                        ‚îÇ
‚îÇ      c. Apply RoPE to Q and K                                  ‚îÇ
‚îÇ      d. GQA attention with KV cache                            ‚îÇ
‚îÇ      e. Attention output projection                            ‚îÇ
‚îÇ      f. Residual connection                                    ‚îÇ
‚îÇ      g. FFN RMSNorm                                            ‚îÇ
‚îÇ      h. SwiGLU FFN (gate/up/down)                              ‚îÇ
‚îÇ      i. Final residual                                         ‚îÇ
‚îÇ   4. Final RMSNorm                                             ‚îÇ
‚îÇ   5. Project to vocabulary (lm_head)                           ‚îÇ
‚îÇ   6. Increment position in KV cache                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CUDA Kernels                                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ ‚Ä¢ embedding.cu: Token embedding lookup                         ‚îÇ
‚îÇ ‚Ä¢ rmsnorm.cu: RMSNorm forward pass                             ‚îÇ
‚îÇ ‚Ä¢ rope.cu: Rotary positional embeddings                        ‚îÇ
‚îÇ ‚Ä¢ gqa_attention.cu: Grouped query attention                    ‚îÇ
‚îÇ ‚Ä¢ swiglu_ffn.cu: SwiGLU feed-forward network                   ‚îÇ
‚îÇ ‚Ä¢ sampling_wrapper.cu: Temperature/top-k/top-p sampling        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Detailed Step-by-Step Flow

#### Phase 1: Model Loading (Startup)

```
1. main.rs: Parse CLI args (model_path, gpu_device, port)
   ‚Üì
2. cuda::Context::new(gpu_device)
   ‚Üí cuda_init() FFI call
   ‚Üí Initialize CUDA context for GPU
   ‚Üì
3. cuda::Model::load(ctx, model_path)
   ‚Üì
   3a. Parse GGUF metadata
       - Read vocab_size (fallback to token_embd.weight dimensions)
       - Read hidden_dim, num_layers, num_heads, num_kv_heads, context_length
   ‚Üì
   3b. Load weights in Rust (weight_loader.rs)
       - Parse all tensor metadata from GGUF
       - Pre-allocate GPU memory for all tensors
       - Load each tensor:
         * FP16: Read and copy to GPU
         * FP32: Convert to FP16, copy to GPU
         * Q4_K/Q5_0/Q6_K/Q8_0: Dequantize on GPU, store as FP16
       - Store GPU pointers in global registry (never freed)
   ‚Üì
   3c. Pass GPU pointers to C++ (ffi_weight_loading.cpp)
       - Create GpuPointerMap
       - Insert all tensor name ‚Üí GPU pointer mappings
       - Call cuda_load_model_from_pointers()
   ‚Üì
   3d. Wire pointers in C++ (qwen_weight_loader.cpp)
       - Create QwenModel struct
       - Wire token_embd.weight
       - Wire each layer's weights:
         * attn_norm.weight
         * attn_q.weight, attn_q.bias
         * attn_k.weight, attn_k.bias
         * attn_v.weight, attn_v.bias
         * attn_output.weight
         * ffn_norm.weight
         * ffn_gate.weight, ffn_up.weight, ffn_down.weight
       - Wire output_norm.weight
       - Wire output.weight (lm_head)
   ‚Üì
4. CudaInferenceBackend::new(model, model_path)
   - Parse GGUF metadata again (for tokenizer)
   - Load tokenizer from GGUF
   ‚Üì
5. HTTP server starts, ready to accept requests
```

#### Phase 2: Inference Request

```
1. HTTP POST /execute
   Body: {
     "job_id": "test-123",
     "prompt": "The quick brown fox",
     "max_tokens": 100,
     "temperature": 0.8,
     "seed": 42
   }
   ‚Üì
2. worker-http validates request
   ‚Üì
3. CudaInferenceBackend::execute(prompt, config)
   ‚Üì
   3a. Tokenize prompt
       - tokenizer.encode("The quick brown fox", add_special=true)
       - Result: token_ids = [The_id, quick_id, brown_id, fox_id]
   ‚Üì
   3b. Read model config from GGUF
       - vocab_size (with fallback)
       - hidden_dim, num_layers, num_heads, num_kv_heads, context_length
       - Derive ffn_dim from blk.0.ffn_up.weight tensor shape
       - Calculate head_dim = hidden_dim / num_heads
   ‚Üì
   3c. Initialize RealInference
       - RealInference::init(model, vocab_size, hidden_dim, ...)
       - FFI: cuda_inference_init()
       - C++: Create InferenceContext
         * Create QwenTransformer with config
         * Allocate KV cache: [num_layers, batch=1, num_kv_heads, context_length, head_dim]
         * Allocate logits buffer: [vocab_size] (FP32)
         * Initialize KV cache position to 0
   ‚Üì
   3d. PREFILL PHASE: Feed prompt tokens
       for i in 0..token_ids.len()-1:
         token_id = token_ids[i]
         _ = inference.generate_token(token_id, temp=0.0, top_k=0, top_p=1.0, seed)
         // Runs forward pass, builds KV cache, ignores sampled output
       
       current_token = token_ids.last()
       // Now KV cache contains [The, quick, brown], ready to generate from "fox"
   ‚Üì
   3e. DECODE PHASE: Generate new tokens
       for token_idx in 0..max_tokens:
         ‚Üì
         next_token_id = inference.generate_token(
           current_token,
           temperature=0.8,
           top_k=40,
           top_p=0.9,
           seed=seed+token_idx
         )
         ‚Üì
         // Inside generate_token():
         
         3e-i. Run transformer forward pass
               - FFI: cuda_inference_generate_token()
               - C++: QwenTransformer::forward(token_id, batch_size=1, logits_buffer)
               
               Step 1: Get current position from KV cache
                 cudaMemcpy(&pos, kv_cache.seq_lens, ..., DeviceToHost)
               
               Step 2: Embed token
                 cuda_embedding_lookup(token_id, token_embd.weight, hidden_states)
                 // hidden_states = token_embd.weight[token_id, :]
               
               Step 3: Process through all layers
                 layer_input = hidden_states
                 for layer_idx in 0..num_layers:
                   
                   3a. Attention RMSNorm
                       cuda_rmsnorm_forward(layer_input, layer.attn_norm, normed)
                   
                   3b. Q/K/V projections (cuBLAS GEMM + bias)
                       Q = normed @ attn_q.weight^T + attn_q.bias  // [batch, num_heads * head_dim]
                       K = normed @ attn_k.weight^T + attn_k.bias  // [batch, num_kv_heads * head_dim]
                       V = normed @ attn_v.weight^T + attn_v.bias  // [batch, num_kv_heads * head_dim]
                   
                   3c. Apply RoPE to Q and K
                       cuda_rope_forward(Q, K, batch, num_heads, head_dim, pos, freq_base=1e6)
                       // Rotary positional embeddings with base frequency 1,000,000
                   
                   3d. GQA Attention with KV cache
                       layer_k_cache = kv_cache.k[layer_idx, :, :, :, :]
                       layer_v_cache = kv_cache.v[layer_idx, :, :, :, :]
                       
                       cuda_gqa_attention_forward(
                         Q, K, V,
                         layer_k_cache, layer_v_cache,
                         attn_output,
                         batch_size=1,
                         num_q_heads, num_kv_heads, head_dim,
                         seq_len=1,
                         cache_len=pos,
                         max_seq_len=context_length
                       )
                       
                       // Inside GQA kernel (ALWAYS uses decode kernel):
                       - For each query head:
                         * Compute attention scores with all cached K vectors (0..pos)
                         * Compute attention score with current K
                         * Apply softmax
                         * Compute weighted sum of V vectors
                         * Write current K,V to cache at position 'pos'
                           (only once per KV group using q_head % (num_q_heads/num_kv_heads) == 0)
                   
                   3e. Attention output projection
                       attn_output = attn_output @ attn_output.weight^T
                   
                   3f. Residual connection
                       residual = layer_input + attn_output
                   
                   3g. FFN RMSNorm
                       cuda_rmsnorm_forward(residual, layer.ffn_norm, normed)
                   
                   3h. SwiGLU FFN
                       gate_out = normed @ ffn_gate.weight^T
                       up_out = normed @ ffn_up.weight^T
                       swiglu_out = silu(gate_out) * up_out
                       ffn_output = swiglu_out @ ffn_down.weight^T
                   
                   3i. Final residual
                       layer_output = residual + ffn_output
                   
                   layer_input = layer_output  // Swap for next layer
               
               Step 4: Final RMSNorm
                 cuda_rmsnorm_forward(layer_input, output_norm, normed)
               
               Step 5: Project to vocabulary (lm_head)
                 logits = normed @ lm_head^T  // [batch, vocab_size] in FP32
               
               Step 6: Increment position
                 pos++
                 cudaMemcpy(kv_cache.seq_lens, &pos, ..., HostToDevice)
         
         3e-ii. Sample next token
                cuda_sample_token(logits, vocab_size, temperature, top_k, top_p, seed)
                
                // Inside sampling:
                - Apply temperature scaling: logits /= temperature
                - Apply top-k filtering: set logits[i] = -inf for tokens outside top-k
                - Apply top-p filtering: set logits[i] = -inf for tokens outside nucleus
                - Compute softmax: probs = softmax(logits)
                - Sample from distribution: next_token_id = categorical(probs)
         
         ‚Üì
         3e-iii. Detokenize token
                 token_text = tokenizer.decode([next_token_id], skip_special=false)
         
         ‚Üì
         3e-iv. Stream SSE event
                Send: data: {"t": token_text, "i": token_idx}\n\n
         
         ‚Üì
         current_token = next_token_id
         
         if next_token_id == eos_token_id:
           break
   ‚Üì
4. Send final SSE event
   data: {"stop_reason": "MaxTokens", "tokens_out": N, ...}\n\n
   ‚Üì
5. Close SSE stream
```

### Key Data Structures

#### KV Cache Layout
```
Shape: [num_layers, batch=1, num_kv_heads, max_seq_len, head_dim]
Type: FP16 (half)

Example for Qwen2.5-0.5B:
- num_layers = 24
- num_kv_heads = 2
- max_seq_len = 8192 (context_length)
- head_dim = 64 (hidden_dim / num_heads = 896 / 14)

Total size per cache (K or V):
24 * 1 * 2 * 8192 * 64 * 2 bytes = 50.3 MB

Indexing for layer L, kv_head H, position P, dimension D:
index = L * (1 * num_kv_heads * max_seq_len * head_dim)
      + 0 * (num_kv_heads * max_seq_len * head_dim)
      + H * (max_seq_len * head_dim)
      + P * head_dim
      + D
```

#### QwenModel Weights Structure
```cpp
struct QwenWeights {
    void* token_embd;      // [vocab_size, hidden_dim]
    
    struct Layer {
        void* attn_norm;       // [hidden_dim]
        void* attn_q_weight;   // [num_heads * head_dim, hidden_dim]
        void* attn_q_bias;     // [num_heads * head_dim]
        void* attn_k_weight;   // [num_kv_heads * head_dim, hidden_dim]
        void* attn_k_bias;     // [num_kv_heads * head_dim]
        void* attn_v_weight;   // [num_kv_heads * head_dim, hidden_dim]
        void* attn_v_bias;     // [num_kv_heads * head_dim]
        void* attn_output;     // [hidden_dim, hidden_dim]
        
        void* ffn_norm;        // [hidden_dim]
        void* ffn_gate;        // [ffn_dim, hidden_dim]
        void* ffn_up;          // [ffn_dim, hidden_dim]
        void* ffn_down;        // [hidden_dim, ffn_dim]
    };
    
    std::vector<Layer> layers;  // [num_layers]
    
    void* output_norm;     // [hidden_dim]
    void* lm_head;         // [vocab_size, hidden_dim]
};
```

#### Tensor Name Mappings (GGUF ‚Üí C++)
```
GGUF Name                    ‚Üí C++ Field
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
token_embd.weight            ‚Üí weights.token_embd

blk.{i}.attn_norm.weight     ‚Üí weights.layers[i].attn_norm
blk.{i}.attn_q.weight        ‚Üí weights.layers[i].attn_q_weight
blk.{i}.attn_q.bias          ‚Üí weights.layers[i].attn_q_bias
blk.{i}.attn_k.weight        ‚Üí weights.layers[i].attn_k_weight
blk.{i}.attn_k.bias          ‚Üí weights.layers[i].attn_k_bias
blk.{i}.attn_v.weight        ‚Üí weights.layers[i].attn_v_weight
blk.{i}.attn_v.bias          ‚Üí weights.layers[i].attn_v_bias
blk.{i}.attn_output.weight   ‚Üí weights.layers[i].attn_output

blk.{i}.ffn_norm.weight      ‚Üí weights.layers[i].ffn_norm
blk.{i}.ffn_gate.weight      ‚Üí weights.layers[i].ffn_gate
blk.{i}.ffn_up.weight        ‚Üí weights.layers[i].ffn_up
blk.{i}.ffn_down.weight      ‚Üí weights.layers[i].ffn_down

output_norm.weight           ‚Üí weights.output_norm
output.weight                ‚Üí weights.lm_head
```

## Test Results

### Before Fixes
```
Running: cargo test --test haiku_generation_anti_cheat --features cuda --release -- --ignored

Output: Garbage tokens, repetitive patterns
Example: "ƒ†firstƒ†firstƒ†firstƒ≤ƒ§ƒ§ƒ§√§¬πƒ∞√•¬Æƒø√ß≈Å¬≥√•¬Æƒø√ß≈Å¬≥..."
Result: FAILED - haiku doesn't contain required word
```

### After Fixes
```
Running: cargo test --test simple_generation_test --features cuda --release -- --ignored

Output: Still garbage/repetitive, but different patterns
Example: "--√ôƒß√ò¬ß√ôƒ®ƒ†pulƒ†originally√ëƒ¢√ê¬æ√ôƒß√ò¬ß√ôƒ®¬¶ƒ≤√§¬∏ƒ¢√•ƒØ¬∞ƒ†versƒ†versƒ†versƒ≤√ßƒºƒ©ƒ†vers¬¶raries√ßƒºƒ©√ßƒºƒ©"
Result: PASSED (ASCII ratio > 30%, generates tokens)

Model generates tokens successfully, but output quality is poor.
```

## Remaining Issues

### Current Status
- ‚úÖ Model loads successfully
- ‚úÖ Weights are in GPU memory
- ‚úÖ Tokenizer works
- ‚úÖ Inference pipeline executes without crashes
- ‚úÖ Embeddings have reasonable values (-0.03 to 0.04)
- ‚úÖ Logits have reasonable values (not NaN/Inf)
- ‚úÖ Sampling produces token IDs
- ‚úÖ Detokenization works
- ‚ö†Ô∏è **Output is repetitive/garbage**

### Symptoms
1. **Repetitive tokens**: Same token generated multiple times in a row (e.g., "ƒ†vers" 3 times)
2. **Non-English tokens**: Many Unicode/non-ASCII characters
3. **No coherent text**: Output doesn't follow prompt context
4. **Low ASCII ratio**: ~45% ASCII characters (should be >80% for English)

### Possible Root Causes

#### 1. Attention Mask Issue
**Hypothesis**: Model might not be applying causal masking correctly, allowing future tokens to attend to past.

**Evidence**: None yet - need to verify attention kernel implementation.

**Next Step**: Check if `gqa_attention_decode_kernel` applies causal mask when computing attention scores.

#### 2. RoPE Frequency Mismatch
**Hypothesis**: RoPE base frequency (1e6) might be wrong for this specific model.

**Evidence**: 
- Code uses `rope_freq_base = 1000000.0f` (line 298 in qwen_transformer.cpp)
- Online references confirm 1e6 for Qwen2.x models
- But specific model variant might differ

**Next Step**: Extract RoPE frequency from GGUF metadata (`qwen2.rope.freq_base` key).

#### 3. Tensor Name Mismatch
**Hypothesis**: GGUF tensor names don't match C++ expectations, causing wrong weights to be used.

**Evidence**:
- Weight loader logs show: `token_embd.weight -> 0x70aea2000000`
- C++ retrieves: `get_ptr("token_embd.weight")`
- Names appear to match

**Next Step**: Verify all tensor names match by comparing GGUF tensor list with C++ expectations.

#### 4. Numerical Precision Issues
**Hypothesis**: FP16 precision loss or accumulation errors in long computation chains.

**Evidence**: 
- All weights stored as FP16
- Logits computed in FP32
- Intermediate activations in FP16

**Next Step**: Add logging to check for NaN/Inf in intermediate activations (after each layer).

#### 5. Incorrect Attention Implementation
**Hypothesis**: GQA attention kernel has bugs beyond the KV cache write issue.

**Evidence**: 
- Decode kernel computes attention scores for all cached positions
- Softmax is applied
- But implementation might have indexing bugs

**Next Step**: Add debug logging to print attention scores for first few tokens.

#### 6. Model Architecture Mismatch
**Hypothesis**: Qwen2.5-0.5B uses a slightly different architecture than implemented.

**Evidence**:
- Code assumes standard Qwen2 architecture
- But 0.5B variant might have differences

**Next Step**: Compare with Ollama/LM Studio implementation to find differences.

### Recommended Debugging Steps

1. **Compare with Reference Implementation**
   ```bash
   # Run same prompt in Ollama
   ollama run qwen2.5:0.5b "The quick brown fox"
   
   # Extract first-token logits from both implementations
   # Compare to find divergence point
   ```

2. **Verify Tensor Mappings**
   ```bash
   # List all tensors in GGUF
   python3 -c "
   from gguf import GGUFReader
   reader = GGUFReader('qwen2.5-0.5b-instruct-fp16.gguf')
   for tensor in reader.tensors:
       print(f'{tensor.name}: {tensor.shape}')
   "
   
   # Compare with C++ expectations in qwen_weight_loader.cpp
   ```

3. **Add Intermediate Activation Logging**
   ```cpp
   // In qwen_transformer.cpp, after each layer:
   half host_hidden[10];
   cudaMemcpy(host_hidden, layer_output, 10 * sizeof(half), cudaMemcpyDeviceToHost);
   fprintf(stderr, "Layer %d output: ", layer_idx);
   for (int i = 0; i < 10; i++) {
       fprintf(stderr, "%.2f ", __half2float(host_hidden[i]));
   }
   fprintf(stderr, "\n");
   ```

4. **Check RoPE Frequency**
   ```rust
   // In cuda_backend.rs, read from GGUF:
   let rope_freq_base = metadata.get_f32("qwen2.rope.freq_base")
       .unwrap_or(1000000.0);
   eprintln!("RoPE freq base: {}", rope_freq_base);
   ```

5. **Verify Attention Scores**
   ```cpp
   // In gqa_attention.cu decode kernel, after softmax:
   if (batch == 0 && q_head == 0 && tid == 0) {
       printf("Attention scores for head 0: ");
       for (int i = 0; i <= cache_len && i < 10; i++) {
           printf("%.3f ", scores[i]);
       }
       printf("\n");
   }
   ```

## Files Modified

### Rust Files
1. `/home/vince/Projects/llama-orch/bin/worker-orcd/src/cuda/model.rs`
   - Lines 69-100: Read model config from GGUF with vocab_size fallback

2. `/home/vince/Projects/llama-orch/bin/worker-orcd/src/inference/cuda_backend.rs`
   - Lines 91-105: Read model config from GGUF with vocab_size fallback
   - Lines 115-129: Derive ffn_dim from GGUF tensor shapes
   - Lines 144-163: Fix prefill phase to properly build KV cache

### CUDA Files
3. `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/gqa_attention.cu`
   - Line 157: Fix KV cache write condition in decode kernel
   - Line 214: Fix KV cache write condition in prefill kernel (now disabled)
   - Lines 396-412: Always use decode kernel instead of prefill kernel

## Summary

### What We Fixed
1. ‚úÖ **GQA KV cache write bug** - Both KV heads now get updated correctly
2. ‚úÖ **Hardcoded model config** - Now reads from GGUF metadata
3. ‚úÖ **FFN dimension assumption** - Now derives from actual tensor shapes
4. ‚úÖ **Prefill phase bug** - Now builds complete KV cache with all prompt tokens
5. ‚úÖ **Prefill kernel cache indexing** - Disabled buggy prefill kernel, use decode kernel

### What Still Needs Work
- ‚ö†Ô∏è **Output quality** - Model generates tokens but they're repetitive/garbage
- üîç **Root cause unknown** - Requires deeper debugging or reference comparison

### Impact
- Model now executes inference without crashes
- All prompt tokens are properly processed and cached
- KV cache is correctly maintained across generation steps
- But output quality suggests a deeper issue in attention, RoPE, or weight mapping

---

**Session Date**: 2025-10-06  
**Model Tested**: Qwen2.5-0.5B-Instruct FP16 GGUF  
**Test Status**: Inference works, output quality poor  
**Next Steps**: Compare with reference implementation to identify divergence
