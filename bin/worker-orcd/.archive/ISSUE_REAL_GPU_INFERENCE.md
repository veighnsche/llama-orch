# Issue: Implement Real GPU Inference for Haiku Test

**Created**: 2025-10-05T16:22:45Z  
**Priority**: P0 (M0 blocker)  
**Status**: TODO  
**Related Fine**: test-harness/FINES.md #001

---

## Problem

The haiku test currently uses a **stub implementation** that hardcodes the haiku instead of performing real GPU inference.

**Current behavior**:
- ‚ùå Extracts minute word from prompt
- ‚ùå Inserts it into hardcoded template
- ‚ùå Returns fake haiku
- ‚ùå **No real inference happens**

**Expected behavior**:
- ‚úÖ Load GGUF weights to GPU VRAM
- ‚úÖ Tokenize prompt
- ‚úÖ Run transformer forward pass on GPU
- ‚úÖ Sample tokens from logits
- ‚úÖ Detokenize and return real haiku

---

## Impact

**CRITICAL**: The test passes when the product is broken.

If any of these are broken, the test still passes:
- GGUF weight loading
- Tokenizer
- Transformer layers
- CUDA kernels
- Token sampling

**This is a false positive** and violates M0 success criteria.

---

## Implementation Plan

Per `BUG_HAIKU_TEST_MODEL_LOADING.md` and `ACTUAL_IMPLEMENTATION_STATUS.md`:

### Phase 1: GGUF Weight Loading (9-13 hours)

**Files to modify**:
- `cuda/src/model_impl.cpp` - Actually load weights to GPU
- `cuda/src/model/gpt_weights.cpp` - Use existing weight loader
- `cuda/src/model/gpt_model.cpp` - Wire to ModelImpl

**Tasks**:
1. Wire `ModelImpl` to existing `GPTWeightLoader`
2. Load tensors from GGUF file to GPU memory
3. Allocate VRAM for weights
4. Verify weights are resident in VRAM

**Existing code to use**:
- ‚úÖ `cuda/src/gguf/header_parser.cpp` - GGUF parsing (done)
- ‚úÖ `cuda/src/io/mmap_file.cpp` - Memory mapping (done)
- ‚úÖ `cuda/src/model/gpt_weights.cpp` - Weight structures (done)
- ‚úÖ `cuda/src/device_memory.cpp` - VRAM allocation (done)

### Phase 2: Tokenizer Integration (5-7 hours)

**Files to modify**:
- `cuda/src/tokenizer/` - Create tokenizer module
- `cuda/src/model_impl.cpp` - Add tokenizer to model

**Tasks**:
1. Extract tokenizer from GGUF metadata
2. Implement BPE encode/decode
3. Handle special tokens
4. Wire to inference pipeline

**Existing code to use**:
- ‚úÖ `cuda/src/gguf/llama_metadata.cpp` - Metadata extraction (done)

### Phase 3: Transformer Forward Pass (8-11 hours)

**Files to modify**:
- `cuda/src/inference_impl.cpp` - Replace stub with real inference
- `cuda/src/model/gpt_model.cpp` - Wire forward pass

**Tasks**:
1. Implement prefill (process prompt tokens)
2. Implement decode (generate one token at a time)
3. Wire to existing CUDA kernels
4. Implement token sampling

**Existing code to use**:
- ‚úÖ `cuda/kernels/attention.cu` - Attention kernels (done)
- ‚úÖ `cuda/kernels/gemm.cu` - Matrix multiplication (done)
- ‚úÖ `cuda/kernels/sampling.cu` - Token sampling (done)
- ‚úÖ `cuda/src/gpt_transformer_layer.cpp` - Layer execution (done)
- ‚úÖ `cuda/src/kv_cache.cpp` - KV cache (done)

---

## Timeline

**Optimistic**: 5 days (1 developer, focused)
- Day 1-2: Phase 1 (GGUF weight loading)
- Day 3: Phase 2 (Tokenizer)
- Day 4-5: Phase 3 (Inference)

**Realistic**: 7-10 days (with testing and debugging)

**Deadline**: 2025-10-15 (10 days from fine issuance)

---

## Success Criteria

### Test Output

**Before (stub)**:
```
üé® M0 Haiku Anti-Cheat Test PASSED
Haiku:
seventeen threads spin 
CUDA cores burning bright 
GPU's warm glow
```

**After (real inference)**:
```
üé® M0 Haiku Anti-Cheat Test PASSED (REAL GPU INFERENCE)
Haiku:
[actual haiku generated by model, containing minute word]
```

### Verification

1. ‚úÖ GGUF weights loaded to GPU (verify with `nvidia-smi`)
2. ‚úÖ Tokenizer encodes/decodes correctly
3. ‚úÖ Transformer forward pass executes on GPU
4. ‚úÖ Tokens are sampled from model logits
5. ‚úÖ Haiku contains minute word (anti-cheat works)
6. ‚úÖ **Different haiku each run** (not hardcoded)

### Test Rename

- From: `test_haiku_generation_STUB_PIPELINE_ONLY`
- To: `test_haiku_generation_anti_cheat` (original name)

Remove all stub warnings and fine references.

---

## Dependencies

### Already Implemented

- ‚úÖ GGUF header parsing
- ‚úÖ Memory-mapped file I/O
- ‚úÖ CUDA context management
- ‚úÖ VRAM allocation
- ‚úÖ KV cache
- ‚úÖ CUDA kernels (attention, GEMM, sampling, etc.)
- ‚úÖ GPT transformer layer structure
- ‚úÖ HTTP/SSE streaming pipeline

### Need to Implement

- ‚ùå GGUF weight loading to GPU
- ‚ùå Tokenizer (BPE encode/decode)
- ‚ùå Transformer forward pass wiring
- ‚ùå Token sampling integration

---

## Resources

### Documentation

- `BUG_HAIKU_TEST_MODEL_LOADING.md` - Original bug analysis
- `ACTUAL_IMPLEMENTATION_STATUS.md` - What's already done
- `BUGS_FIXED_HAIKU_IMPLEMENTATION.md` - Lessons learned
- `cuda/README.md` - CUDA implementation overview

### Existing Code

- `cuda/src/gguf/` - GGUF parsing (complete)
- `cuda/src/model/` - Model structures (partial)
- `cuda/kernels/` - CUDA kernels (complete)
- `cuda/src/io/` - File I/O (complete)

### Reference Implementations

- GGUF spec: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- Qwen model: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF

**Note**: Do NOT use llama.cpp code (see `NO_LLAMA_CPP.md`)

---

## Acceptance Criteria

### Code

1. ‚úÖ Real GGUF weights loaded to GPU VRAM
2. ‚úÖ Tokenizer encodes prompt correctly
3. ‚úÖ Transformer executes on GPU
4. ‚úÖ Tokens sampled from real logits
5. ‚úÖ No hardcoded templates
6. ‚úÖ No stub warnings

### Tests

1. ‚úÖ Haiku test passes with real inference
2. ‚úÖ Different haiku each run
3. ‚úÖ Minute word appears in haiku
4. ‚úÖ Test renamed to original name
5. ‚úÖ All stub warnings removed

### Documentation

1. ‚úÖ Implementation documented
2. ‚úÖ Fine remediation proof submitted
3. ‚úÖ Testing Team sign-off received

---

## Remediation Status

**Fine**: FINE-001-20251005  
**Deadline**: 2025-10-06T16:22:45Z (24 hours for warnings)  
**Implementation Deadline**: 2025-10-15 (10 days for real inference)

### Immediate (24 hours) - COMPLETED

- ‚úÖ Added WARNING to test output
- ‚úÖ Renamed test to `test_haiku_generation_STUB_PIPELINE_ONLY`
- ‚úÖ Updated test documentation
- ‚úÖ Created this tracking issue

### Long-term (10 days) - TODO

- ‚¨ú Phase 1: GGUF weight loading
- ‚¨ú Phase 2: Tokenizer integration
- ‚¨ú Phase 3: Transformer forward pass
- ‚¨ú Test renamed back to original
- ‚¨ú Stub warnings removed
- ‚¨ú Fine remediation proof submitted

---

## Notes

### Why This Matters

The haiku test is an **M0 success criteria**. It proves:
- Real GPU inference works
- GGUF models can be loaded
- Transformers execute correctly
- The entire pipeline is functional

**A stub test defeats the purpose.**

### What We Learned

From `BUGS_FIXED_HAIKU_IMPLEMENTATION.md`:
- We fixed 31 bugs to get the stub working
- We have 80-90% of the infrastructure done
- The hard parts (GGUF parsing, CUDA kernels) are complete
- We just need to wire it together

**We're close. Let's finish it properly.**

---

**Created by**: Testing Team (in response to FINE-001)  
**Assigned to**: Foundation-Alpha  
**Priority**: P0  
**Estimate**: 22-31 hours  
**Deadline**: 2025-10-15

---

Tracked by Testing Team üîç
