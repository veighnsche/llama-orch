warning: /home/vince/Projects/llama-orch/bin/shared-crates/narration-core/Cargo.toml: unused manifest key: package.autodocs
warning: /home/vince/Projects/llama-orch/xtask/Cargo.toml: unused manifest key: package.autodocs
warning: /home/vince/Projects/llama-orch/bin/shared-crates/auth-min/Cargo.toml: unused manifest key: package.autodocs
warning: unused variable: `alignment_offset`
  --> bin/worker-crates/worker-gguf/src/parser.rs:86:13
   |
86 |         let alignment_offset = 0u64;
   |             ^^^^^^^^^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_alignment_offset`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: `worker-gguf` (lib) generated 1 warning
warning: unused variable: `model`
   --> bin/worker-crates/worker-models/src/gpt.rs:274:9
    |
274 |         model: &GPTModel,
    |         ^^^^^ help: if this is intentional, prefix it with an underscore: `_model`
    |
    = note: `#[warn(unused_variables)]` on by default

warning: unused variable: `config`
   --> bin/worker-crates/worker-models/src/gpt.rs:277:9
    |
277 |         config: &GPTForwardConfig,
    |         ^^^^^^ help: if this is intentional, prefix it with an underscore: `_config`

warning: associated function `detect_architecture_from_filename` is never used
   --> bin/worker-crates/worker-models/src/factory.rs:139:8
    |
 77 | impl AdapterFactory {
    | ------------------- associated function in this implementation
...
139 |     fn detect_architecture_from_filename(path...
    |        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |
    = note: `#[warn(dead_code)]` on by default

warning: associated function `calculate_layer_params` is never used
   --> bin/worker-crates/worker-models/src/gpt.rs:203:8
    |
140 | impl GPTWeightLoader {
    | -------------------- associated function in this implementation
...
203 |     fn calculate_layer_params(config: &GPTCon...
    |        ^^^^^^^^^^^^^^^^^^^^^^

warning: worker-orcd@0.0.0: Building WITH CUDA support
warning: worker-orcd@0.0.0: CUDA detected via nvcc at /usr
warning: worker-orcd@0.0.0: Using CUDA toolkit at: /usr
warning: `worker-models` (lib) generated 4 warnings
warning: variant `Q2_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:59:5
   |
59 |     Q2_K = 10,
   |     ^^^^ help: convert the identifier to upper camel case: `Q2K`
   |
   = note: `#[warn(non_camel_case_types)]` on by default

warning: variant `Q3_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:60:5
   |
60 |     Q3_K = 11,
   |     ^^^^ help: convert the identifier to upper camel case: `Q3K`

warning: variant `Q4_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:61:5
   |
61 |     Q4_K = 12, // Q4_K_M in GGUF file type naming
   |     ^^^^ help: convert the identifier to upper camel case: `Q4K`

warning: variant `Q5_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:62:5
   |
62 |     Q5_K = 13,
   |     ^^^^ help: convert the identifier to upper camel case: `Q5K`

warning: variant `Q6_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:63:5
   |
63 |     Q6_K = 14,
   |     ^^^^ help: convert the identifier to upper camel case: `Q6K`

warning: variant `Q8_K` should have an upper camel case name
  --> bin/worker-orcd/src/cuda/weight_loader.rs:64:5
   |
64 |     Q8_K = 15,
   |     ^^^^ help: convert the identifier to upper camel case: `Q8K`

warning: variant `Q4_K_M` should have an upper camel case name
  --> bin/worker-orcd/src/inference/gpt_adapter.rs:73:5
   |
73 |     Q4_K_M,
   |     ^^^^^^ help: convert the identifier to upper camel case: `Q4KM`

warning: use of deprecated function `cuda::weight_loader::load_tensor`: Use load_tensor_gpu() instead for GPU dequantization
  --> bin/worker-orcd/src/cuda/mod.rs:48:27
   |
48 |     load_model_from_rust, load_tensor, load_tensor_gpu, load_weights_to_gpu, GGMLType, TensorInfo,
   |                           ^^^^^^^^^^^
   |
   = note: `#[warn(deprecated)]` on by default

warning: unused variable: `ctx`
  --> bin/worker-orcd/src/cuda/model.rs:63:17
   |
63 |     pub fn load(ctx: &Context, model_path: &str) -> Result<Self, CudaError> {
   |                 ^^^ help: if this is intentional, prefix it with an underscore: `_ctx`
   |
   = note: `#[warn(unused_variables)]` on by default

warning: method `as_ptr` is never used
   --> bin/worker-orcd/src/cuda/context.rs:170:19
    |
 38 | impl Context {
    | ------------ method in this implementation
...
170 |     pub(crate) fn as_ptr(&self) -> *mut ffi::CudaContext {
    |                   ^^^^^^
    |
    = note: `#[warn(dead_code)]` on by default

warning: field `0` is never read
  --> bin/worker-orcd/src/cuda/weight_loader.rs:32:19
   |
32 | struct GpuPointer(*mut c_void);
   |        ---------- ^^^^^^^^^^^
   |        |
   |        field in this struct
   |
   = help: consider removing this field
   = note: `GpuPointer` has a derived impl for the trait `Clone`, but this is intentionally ignored during dead code analysis

warning: field `ptr` is never read
  --> bin/worker-orcd/src/cuda_ffi/mod.rs:75:5
   |
74 | pub struct SafeCudaPtr {
   |            ----------- field in this struct
75 |     ptr: *mut c_void,
   |     ^^^

warning: `worker-orcd` (lib) generated 12 warnings
    Finished `release` profile [optimized] target(s) in 0.19s
     Running tests/haiku_generation_anti_cheat.rs (/home/vince/Projects/llama-orch/target/release/deps/haiku_generation_anti_cheat-05931b90abac8510)

running 1 test
test test_haiku_generation_stub_pipeline_only ... ⚠️  DEBUGGING: Matrix layout fixed, investigating attention mechanism
⚠️  Q values now correct, but output still garbage
⚠️  See TEST_RESULTS_AFTER_FIX.md for analysis

{"timestamp":"2025-10-06T16:14:00.076154Z","level":"INFO","fields":{"message":"Worker starting","worker_id":"test-worker-37887","model":"/home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf","gpu_device":0,"port":37887}}
{"timestamp":"2025-10-06T16:14:00.190187Z","level":"INFO","fields":{"message":"CUDA context initialized","gpu_device":0}}
{"timestamp":"2025-10-06T16:14:00.190203Z","level":"INFO","fields":{"message":"Loading model to VRAM...","model":"/home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf"}}
🦀 [Rust] Loading model with Rust weight loading + Q4_K dequantization
🔍 [Rust] output.weight dimensions: [896, 151936]
🔍 [Rust] output.weight ggml_type: 1
🔍 [Rust] output.weight offset: 5947744
🔍 [Rust] output.weight expected size: 272269312 bytes (259 MB)
✅ [Rust] Actual vocab size from output.weight: 151936
📋 [Rust] Model config (from GGUF): vocab=151936, hidden=896, layers=24, heads=14/2 ctx=8192
🔧 [Rust] Parsing GGUF tensors from: /home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf
📦 [Rust] Found 291 tensors in GGUF file
⚡ [Rust] Using optimized batch loading...
🔄 [Rust] Pre-allocating GPU memory for 291 tensors...
✅ [Rust] Pre-allocated 1201.95 MB GPU memory
  [1/291] 0.7s elapsed, 1 tensors/sec
🔍 [Rust] Copying token_embd.weight to GPU
   GPU pointer: 0x74e028000000
   Size: 272269312 bytes
   First 20 bytes from host: [80, 161, 56, 41, 248, 32, 144, 4, 240, 166, 24, 154, 192, 148, 0, 165, 72, 39, 232, 158]
✅ [Rust] token_embd.weight copied to GPU successfully
  [51/291] 1.1s elapsed, 46 tensors/sec
  [101/291] 1.2s elapsed, 83 tensors/sec
  [151/291] 1.3s elapsed, 114 tensors/sec
  [201/291] 1.4s elapsed, 141 tensors/sec
  [251/291] 1.5s elapsed, 165 tensors/sec
  [291/291] 1.6s elapsed, 182 tensors/sec
✅ [Rust] Loaded 291 tensors to GPU (1201.95 MB total VRAM) in 1.6s (754 MB/s)
🔒 [Rust] Stored 291 GPU pointers in global registry (will never be freed)
🔍 [Rust] Passing 291 tensors to C++:
   - blk.18.attn_output.weight -> 0x74e016000000
   - blk.0.ffn_up.weight -> 0x74e04a400000
   - token_embd.weight -> 0x74e028000000
🔍 [C++] Stored token_embd.weight pointer: 0x74e028000000
   - blk.3.attn_output.weight -> 0x74e009e00000
   - blk.14.attn_output.weight -> 0x74e021e00000
   - blk.17.attn_output.weight -> 0x74e01be00000
   - blk.21.attn_output.weight -> 0x74e00fe00000
   - blk.22.attn_output.weight -> 0x74e00de00000
   - blk.0.ffn_norm.weight -> 0x74e038400800
   - blk.12.attn_output.weight -> 0x74e025e00000
   - blk.0.attn_q.weight -> 0x74e039a00000
   - blk.20.attn_output.weight -> 0x74e011e00000
   - blk.0.attn_v.bias -> 0x74e0385c1a00
   - blk.5.attn_output.weight -> 0x74e005e00000
   - blk.13.attn_output.weight -> 0x74e023e00000
   - blk.0.attn_norm.weight -> 0x74e038400000
   - blk.10.attn_output.weight -> 0x74e04b800000
   - blk.0.attn_output.weight -> 0x74e038439200
   - blk.9.attn_output.weight -> 0x74dffde00000
   - blk.2.attn_output.weight -> 0x74e013400000
   - blk.0.ffn_down.weight -> 0x74e038600000
   - blk.0.attn_k.bias -> 0x74e038401000
   - blk.0.attn_k.weight -> 0x74e038401200
   - blk.0.attn_q.bias -> 0x74e0385c1200
   - output_norm.weight -> 0x74e04bbfa000
   - blk.4.attn_output.weight -> 0x74e007e00000
   - blk.8.attn_output.weight -> 0x74dfffe00000
   - blk.16.attn_output.weight -> 0x74e01de00000
   - blk.6.attn_output.weight -> 0x74e003e00000
   - blk.11.attn_output.weight -> 0x74e027e00000
   - blk.19.attn_output.weight -> 0x74e014a00000
   - blk.23.attn_output.weight -> 0x74e00be00000
   - blk.0.attn_v.weight -> 0x74e0385c1c00
   - blk.7.attn_output.weight -> 0x74e001e00000
   - output.weight -> 0x74e03a000000
   - blk.1.attn_output.weight -> 0x74e039c00000
   - blk.0.ffn_gate.weight -> 0x74e039000000
   - blk.15.attn_output.weight -> 0x74e01fe00000
🔗 [C++] Wiring 291 pre-loaded GPU pointers...
🔍 [C++] Retrieved token_embd.weight pointer: 0x74e028000000
✅ [C++] Wired all 24 layers (VRAM: 0.00 MB)
🎉 [Rust] Model loaded successfully via Rust weight loading!
{"timestamp":"2025-10-06T16:14:02.610697Z","level":"INFO","fields":{"message":"Model loaded to VRAM","vram_bytes":0}}
{"timestamp":"2025-10-06T16:14:02.610713Z","level":"INFO","fields":{"message":"Test mode: skipping pool manager callback"}}
{"timestamp":"2025-10-06T16:14:02.610715Z","level":"INFO","fields":{"message":"Worker ready, starting HTTP server"}}
{"timestamp":"2025-10-06T16:14:02.610718Z","level":"INFO","fields":{"message":"🔧 Creating CudaInferenceBackend with REAL inference"}}
{"timestamp":"2025-10-06T16:14:02.610720Z","level":"INFO","fields":{"message":"   Model path: /home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf"}}
{"timestamp":"2025-10-06T16:14:03.017290Z","level":"INFO","fields":{"message":"✅ GGUF metadata parsed"}}
{"timestamp":"2025-10-06T16:14:03.598772Z","level":"INFO","fields":{"message":"✅ Tokenizer loaded"}}
{"timestamp":"2025-10-06T16:14:03.598789Z","level":"INFO","fields":{"message":"🎉 CudaInferenceBackend created successfully - REAL INFERENCE ENABLED"}}
{"timestamp":"2025-10-06T16:14:03.598831Z","level":"INFO","fields":{"message":"HTTP server initialized","addr":"0.0.0.0:37887"}}
{"timestamp":"2025-10-06T16:14:03.598879Z","level":"INFO","fields":{"message":"HTTP server listening","addr":"0.0.0.0:37887"}}
🔍 Worker base URL: http://localhost:37887
🔍 Testing health endpoint again...
✅ Health check passed
✅ Worker process is still running
🔍 Sending POST to http://localhost:37887/execute
{"timestamp":"2025-10-06T16:14:04.301048Z","level":"INFO","fields":{"message":"Inference request validated","job_id":"m0-haiku-anti-cheat-d2d5e8d0-d82c-4168-9bd2-627b0da220e1"}}
{"timestamp":"2025-10-06T16:14:04.301076Z","level":"INFO","fields":{"message":"🚀 REAL INFERENCE STARTING"}}
{"timestamp":"2025-10-06T16:14:04.301085Z","level":"INFO","fields":{"message":"   Prompt: Write a haiku about GPU computing that includes the word \"fourteen\" (nonce: XGU0qDs0)"}}
{"timestamp":"2025-10-06T16:14:04.303523Z","level":"INFO","fields":{"message":"✅ Tokenized to 25 tokens"}}
{"timestamp":"2025-10-06T16:14:04.726936Z","level":"INFO","fields":{"message":"✅ Actual vocab size from output.weight: 151936"}}
{"timestamp":"2025-10-06T16:14:04.726964Z","level":"INFO","fields":{"message":"Model config: vocab=151936, hidden=896, layers=24, heads=14, kv_heads=2"}}
{"timestamp":"2025-10-06T16:14:04.726967Z","level":"INFO","fields":{"message":"RoPE frequency base: 1000000"}}
🎉 [C++] Using pre-loaded model from Rust (VRAM: 0.00 MB)
✅ QwenTransformer initialized
   Vocab: 151936, Layers: 24, Hidden: 896, Heads: 14, KV Heads: 2
✅ Inference context initialized
   Vocab: 151936, Hidden: 896, Layers: 24
{"timestamp":"2025-10-06T16:14:05.145394Z","level":"INFO","fields":{"message":"🔄 Prefill phase: processing 24 prompt tokens"}}

[DEEP_INVESTIGATION] ========================================
[DEEP_INVESTIGATION] TRACKING HIDDEN STATE EVOLUTION
[DEEP_INVESTIGATION] Date: 2025-10-06 16:13 UTC
[DEEP_INVESTIGATION] ========================================

[DEEP_INVESTIGATION] After embedding:
  Range: [-0.0417, 0.0461], Mean: 0.0007, Std: 0.0145
  ✅ Values within acceptable range

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.0348, -0.0337, 0.0590, -0.0947, 0.1508
  Q magnitude: 1.9936 (norm of 64-dim vector)
  K_current[0:5]: -0.1893, -0.1949, -0.1465, -0.1093, -0.1279
  K magnitude: 1.3144
  Unscaled Q·K: 0.1484 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.0186 
  Max scaled score: 0.0186
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0059, out_val[0]: 0.0059
  V_current[1]: -0.0154, out_val[1]: -0.0154
  V_current[2]: -0.0180, out_val[2]: -0.0180
  V_current[3]: -0.0225, out_val[3]: -0.0225
  V_current[4]: -0.0224, out_val[4]: -0.0224
[DEEP_INVESTIGATION] After layer 0:
  Range: [-0.0762, 0.0785], Mean: 0.0008, Std: 0.0212

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.4646, -0.5645, -0.1163, -0.5220, -0.0656
  Q magnitude: 4.9472 (norm of 64-dim vector)
  K_current[0:5]: 1.7656, 1.8623, -0.0469, 1.2793, 0.0296
  K magnitude: 7.7457
  Unscaled Q·K: -2.8483 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.3560 
  Max scaled score: -0.3560
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0809, out_val[0]: -0.0809
  V_current[1]: 0.0976, out_val[1]: 0.0976
  V_current[2]: 0.4285, out_val[2]: 0.4285
  V_current[3]: -0.0840, out_val[3]: -0.0840
  V_current[4]: -0.0508, out_val[4]: -0.0508
[DEEP_INVESTIGATION] After layer 1:
  Range: [-0.6304, 0.6074], Mean: 0.0022, Std: 0.1960

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 2.7812, -0.0563, -0.0012, -0.0625, 0.1244
  Q magnitude: 6.6634 (norm of 64-dim vector)
  K_current[0:5]: -0.2064, 0.8662, 0.1313, 1.6152, -0.7275
  K magnitude: 9.4815
  Unscaled Q·K: -2.6280 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.3285 
  Max scaled score: -0.3285
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5142, out_val[0]: 0.5142
  V_current[1]: -0.2559, out_val[1]: -0.2559
  V_current[2]: -0.3843, out_val[2]: -0.3843
  V_current[3]: 0.3101, out_val[3]: 0.3101
  V_current[4]: 0.5679, out_val[4]: 0.5679
[DEEP_INVESTIGATION] After layer 2:
  Range: [-1.2568, 1.2031], Mean: 0.0039, Std: 0.3926

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.6675, -0.0678, 0.0992, 0.3093, 0.6875
  Q magnitude: 6.5970 (norm of 64-dim vector)
  K_current[0:5]: 0.8354, -0.6729, 1.4346, 0.0150, -1.0029
  K magnitude: 7.8510
  Unscaled Q·K: -0.4479 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.0560 
  Max scaled score: -0.0560
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.2209, out_val[0]: 0.2209
  V_current[1]: -0.9648, out_val[1]: -0.9648
  V_current[2]: 0.0503, out_val[2]: 0.0503
  V_current[3]: 0.3479, out_val[3]: 0.3479
  V_current[4]: 1.1162, out_val[4]: 1.1162
[DEEP_INVESTIGATION] After layer 3:
  Range: [-2.2363, 1.9414], Mean: -0.0050, Std: 0.5875

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.1543, 0.5396, 0.0123, 0.0197, 0.3259
  Q magnitude: 6.4415 (norm of 64-dim vector)
  K_current[0:5]: 0.4309, 0.6089, 0.0495, 0.0610, -0.0563
  K magnitude: 9.2333
  Unscaled Q·K: 1.2912 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.1614 
  Max scaled score: 0.1614
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.4683, out_val[0]: -0.4683
  V_current[1]: 0.0030, out_val[1]: 0.0030
  V_current[2]: -0.6538, out_val[2]: -0.6538
  V_current[3]: -0.5332, out_val[3]: -0.5332
  V_current[4]: -0.8628, out_val[4]: -0.8628
[DEEP_INVESTIGATION] After layer 4:
  Range: [-2.8691, 3.0801], Mean: -0.0093, Std: 0.8970

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.7656, -0.4504, -0.4185, -1.8955, -0.5366
  Q magnitude: 6.2049 (norm of 64-dim vector)
  K_current[0:5]: 0.5210, -0.3848, -0.4402, 0.3750, 0.9736
  K magnitude: 6.7896
  Unscaled Q·K: -6.0727 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.7591 
  Max scaled score: -0.7591
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5425, out_val[0]: 0.5425
  V_current[1]: 0.3235, out_val[1]: 0.3235
  V_current[2]: 0.5054, out_val[2]: 0.5054
  V_current[3]: -0.2615, out_val[3]: -0.2615
  V_current[4]: -0.4790, out_val[4]: -0.4790
[DEEP_INVESTIGATION] After layer 5:
  Range: [-3.2734, 3.5195], Mean: 0.0111, Std: 1.1454

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.1229, -0.0390, -0.5356, -0.4810, -0.1438
  Q magnitude: 5.4465 (norm of 64-dim vector)
  K_current[0:5]: 0.7383, -0.4727, -0.3015, -0.1526, -0.8452
  K magnitude: 8.5971
  Unscaled Q·K: -4.9195 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.6149 
  Max scaled score: -0.6149
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.1389, out_val[0]: 0.1389
  V_current[1]: -0.0933, out_val[1]: -0.0933
  V_current[2]: -0.4016, out_val[2]: -0.4016
  V_current[3]: 0.0889, out_val[3]: 0.0889
  V_current[4]: -0.0439, out_val[4]: -0.0439
[DEEP_INVESTIGATION] After layer 6:
  Range: [-3.9258, 3.8516], Mean: -0.0122, Std: 1.2576

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.3848, 1.8301, 0.5967, -1.0996, 0.7686
  Q magnitude: 6.2875 (norm of 64-dim vector)
  K_current[0:5]: -0.1334, -0.0450, -0.3582, -0.7227, -0.0045
  K magnitude: 9.2678
  Unscaled Q·K: -8.2594 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-1.0324 
  Max scaled score: -1.0324
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7539, out_val[0]: -0.7539
  V_current[1]: -0.4836, out_val[1]: -0.4836
  V_current[2]: -0.2612, out_val[2]: -0.2612
  V_current[3]: -0.1065, out_val[3]: -0.1065
  V_current[4]: -0.4172, out_val[4]: -0.4172
[DEEP_INVESTIGATION] After layer 7:
  Range: [-4.1289, 4.3047], Mean: -0.0257, Std: 1.4526

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.0525, -0.2440, -0.8384, 0.3298, 0.3794
  Q magnitude: 11.6144 (norm of 64-dim vector)
  K_current[0:5]: -0.4780, -1.7393, -0.5903, 1.1025, 0.2949
  K magnitude: 9.5380
  Unscaled Q·K: 4.7147 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.5893 
  Max scaled score: 0.5893
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.4070, out_val[0]: -0.4070
  V_current[1]: -0.4216, out_val[1]: -0.4216
  V_current[2]: 0.0356, out_val[2]: 0.0356
  V_current[3]: -0.0799, out_val[3]: -0.0799
  V_current[4]: 0.8086, out_val[4]: 0.8086
[DEEP_INVESTIGATION] After layer 8:
  Range: [-5.0625, 5.1328], Mean: -0.0180, Std: 1.6355

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.0678, -0.5400, 0.3992, 1.2754, 0.2118
  Q magnitude: 7.4245 (norm of 64-dim vector)
  K_current[0:5]: -0.0867, 0.6685, 0.1935, 0.7861, 2.0195
  K magnitude: 11.5544
  Unscaled Q·K: -14.9252 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-1.8656 
  Max scaled score: -1.8656
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.5928, out_val[0]: -0.5928
  V_current[1]: 1.5127, out_val[1]: 1.5127
  V_current[2]: -0.6963, out_val[2]: -0.6963
  V_current[3]: -0.4805, out_val[3]: -0.4805
  V_current[4]: -0.3606, out_val[4]: -0.3606
[DEEP_INVESTIGATION] After layer 9:
  Range: [-5.6094, 5.6406], Mean: -0.0281, Std: 1.8233

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.9214, -1.1152, -1.2480, -0.4595, -0.7769
  Q magnitude: 8.0511 (norm of 64-dim vector)
  K_current[0:5]: -0.2500, 0.8975, 0.1405, 0.1519, 1.0107
  K magnitude: 11.9423
  Unscaled Q·K: 5.7928 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.7241 
  Max scaled score: 0.7241
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.0029, out_val[0]: 1.0029
  V_current[1]: 0.0616, out_val[1]: 0.0616
  V_current[2]: -0.6938, out_val[2]: -0.6938
  V_current[3]: -0.2771, out_val[3]: -0.2771
  V_current[4]: 0.1526, out_val[4]: 0.1526
[DEEP_INVESTIGATION] After layer 10:
  Range: [-6.0234, 6.7734], Mean: -0.0084, Std: 2.0398

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.6401, 1.2178, -0.0674, -0.4106, 1.6777
  Q magnitude: 10.8004 (norm of 64-dim vector)
  K_current[0:5]: 0.5508, -0.3040, -0.7954, -0.8164, 1.3086
  K magnitude: 11.0346
  Unscaled Q·K: -11.1365 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-1.3921 
  Max scaled score: -1.3921
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.8223, out_val[0]: 0.8223
  V_current[1]: 0.1064, out_val[1]: 0.1064
  V_current[2]: 0.4236, out_val[2]: 0.4236
  V_current[3]: 0.0387, out_val[3]: 0.0387
  V_current[4]: 0.0106, out_val[4]: 0.0106
[DEEP_INVESTIGATION] After layer 11:
  Range: [-6.4414, 8.2422], Mean: -0.0075, Std: 2.3179

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.4390, -0.4268, -0.1416, 0.4829, -0.1432
  Q magnitude: 8.0731 (norm of 64-dim vector)
  K_current[0:5]: 0.1312, 1.2715, -2.2930, 2.4473, 0.2332
  K magnitude: 11.4635
  Unscaled Q·K: -27.3555 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-3.4194 
  Max scaled score: -3.4194
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7456, out_val[0]: -0.7456
  V_current[1]: -0.2164, out_val[1]: -0.2164
  V_current[2]: 0.2081, out_val[2]: 0.2081
  V_current[3]: -0.6685, out_val[3]: -0.6685
  V_current[4]: -0.9165, out_val[4]: -0.9165
[DEEP_INVESTIGATION] After layer 12:
  Range: [-6.8203, 7.7969], Mean: -0.0079, Std: 2.4852

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -1.9180, 0.8198, -0.6646, -2.7109, 1.7148
  Q magnitude: 8.1018 (norm of 64-dim vector)
  K_current[0:5]: 0.3398, 0.7856, -1.7656, 1.9072, 2.6074
  K magnitude: 15.7507
  Unscaled Q·K: -0.6298 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.0787 
  Max scaled score: -0.0787
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.2344, out_val[0]: 1.2344
  V_current[1]: 1.3535, out_val[1]: 1.3535
  V_current[2]: -0.7012, out_val[2]: -0.7012
  V_current[3]: -0.6450, out_val[3]: -0.6450
  V_current[4]: 0.4963, out_val[4]: 0.4963
[DEEP_INVESTIGATION] After layer 13:
  Range: [-7.7656, 8.7656], Mean: -0.0298, Std: 2.6946

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.9453, 0.1842, 0.9863, -1.6182, -3.0879
  Q magnitude: 11.5207 (norm of 64-dim vector)
  K_current[0:5]: -0.1390, 0.9023, -3.0879, 1.7119, 1.5273
  K magnitude: 13.1833
  Unscaled Q·K: 5.3185 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.6648 
  Max scaled score: 0.6648
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.6646, out_val[0]: -0.6646
  V_current[1]: -1.0693, out_val[1]: -1.0693
  V_current[2]: 0.1659, out_val[2]: 0.1659
  V_current[3]: 0.4358, out_val[3]: 0.4358
  V_current[4]: -0.8706, out_val[4]: -0.8706
[DEEP_INVESTIGATION] After layer 14:
  Range: [-9.5547, 10.1406], Mean: -0.0378, Std: 3.0317

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.6660, -0.6382, -1.4258, 0.0601, -0.7661
  Q magnitude: 6.8179 (norm of 64-dim vector)
  K_current[0:5]: 0.7046, 0.7773, -0.1870, 0.2139, -0.8071
  K magnitude: 9.3845
  Unscaled Q·K: 20.0408 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=2.5051 
  Max scaled score: 2.5051
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.0234, out_val[0]: 2.0234
  V_current[1]: 0.2576, out_val[1]: 0.2576
  V_current[2]: 1.4199, out_val[2]: 1.4199
  V_current[3]: 0.5645, out_val[3]: 0.5645
  V_current[4]: 1.7617, out_val[4]: 1.7617
[DEEP_INVESTIGATION] After layer 15:
  Range: [-13.1328, 9.8906], Mean: -0.0397, Std: 3.2089

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.2233, 0.0124, 0.6387, 0.2386, -0.1301
  Q magnitude: 8.1731 (norm of 64-dim vector)
  K_current[0:5]: -4.9883, -0.5200, -0.3044, 0.3787, 1.0371
  K magnitude: 14.4586
  Unscaled Q·K: 11.5284 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=1.4411 
  Max scaled score: 1.4411
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.2031, out_val[0]: 2.2031
  V_current[1]: 1.5811, out_val[1]: 1.5811
  V_current[2]: 1.4219, out_val[2]: 1.4219
  V_current[3]: 1.4561, out_val[3]: 1.4561
  V_current[4]: 0.1307, out_val[4]: 0.1307
[DEEP_INVESTIGATION] After layer 16:
  Range: [-15.2734, 13.0000], Mean: -0.1028, Std: 3.5216

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.1091, -0.4392, -1.3203, 0.6538, -0.7144
  Q magnitude: 8.7140 (norm of 64-dim vector)
  K_current[0:5]: 0.9268, -1.5781, -1.6914, 0.6162, -1.5674
  K magnitude: 8.2772
  Unscaled Q·K: 2.8803 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.3600 
  Max scaled score: 0.3600
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.1289, out_val[0]: -1.1289
  V_current[1]: -0.6191, out_val[1]: -0.6191
  V_current[2]: -0.5288, out_val[2]: -0.5288
  V_current[3]: 0.7153, out_val[3]: 0.7153
  V_current[4]: -0.5688, out_val[4]: -0.5688
[DEEP_INVESTIGATION] After layer 17:
  Range: [-14.6172, 13.3750], Mean: -0.1738, Std: 3.6164

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.0750, 0.8423, 1.0010, -0.0137, -1.1377
  Q magnitude: 9.2698 (norm of 64-dim vector)
  K_current[0:5]: -0.4409, 0.6353, -1.7451, -0.7656, 1.8506
  K magnitude: 10.0314
  Unscaled Q·K: -16.1514 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-2.0189 
  Max scaled score: -2.0189
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.5591, out_val[0]: -0.5591
  V_current[1]: 0.3755, out_val[1]: 0.3755
  V_current[2]: -0.0167, out_val[2]: -0.0167
  V_current[3]: -0.5264, out_val[3]: -0.5264
  V_current[4]: -0.0806, out_val[4]: -0.0806
[DEEP_INVESTIGATION] After layer 18:
  Range: [-13.1641, 14.0156], Mean: -0.1614, Std: 3.8800

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.3997, -0.5244, -0.0801, 1.0791, 0.7686
  Q magnitude: 7.0683 (norm of 64-dim vector)
  K_current[0:5]: 0.5825, -0.7905, -0.7969, 0.5977, -0.5063
  K magnitude: 7.1015
  Unscaled Q·K: 4.0677 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.5085 
  Max scaled score: 0.5085
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0439, out_val[0]: -0.0439
  V_current[1]: 0.1633, out_val[1]: 0.1633
  V_current[2]: 0.3477, out_val[2]: 0.3477
  V_current[3]: -0.7983, out_val[3]: -0.7983
  V_current[4]: 0.0318, out_val[4]: 0.0318
[DEEP_INVESTIGATION] After layer 19:
  Range: [-13.8047, 14.5781], Mean: -0.1812, Std: 4.0265

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.4844, -1.6514, 1.7754, -0.2593, -1.0566
  Q magnitude: 9.6426 (norm of 64-dim vector)
  K_current[0:5]: 0.2925, 1.2148, -0.3738, 0.2517, -1.3916
  K magnitude: 8.6042
  Unscaled Q·K: -6.2674 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.7834 
  Max scaled score: -0.7834
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5889, out_val[0]: 0.5889
  V_current[1]: 3.1992, out_val[1]: 3.1992
  V_current[2]: 1.1152, out_val[2]: 1.1152
  V_current[3]: -0.5322, out_val[3]: -0.5322
  V_current[4]: -0.2119, out_val[4]: -0.2119
[DEEP_INVESTIGATION] After layer 20:
  Range: [-17.6875, 17.9844], Mean: -0.1125, Std: 4.9251

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -0.3035, -2.5254, -0.5786, 2.2363, 1.0693
  Q magnitude: 8.7423 (norm of 64-dim vector)
  K_current[0:5]: 0.3455, 0.9062, 0.6812, -0.3948, -1.0430
  K magnitude: 8.0764
  Unscaled Q·K: -3.1844 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.3980 
  Max scaled score: -0.3980
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.1113, out_val[0]: 2.1113
  V_current[1]: 0.2167, out_val[1]: 0.2167
  V_current[2]: 6.4414, out_val[2]: 6.4414
  V_current[3]: -1.6846, out_val[3]: -1.6846
  V_current[4]: -0.6230, out_val[4]: -0.6230
[DEEP_INVESTIGATION] After layer 21:
  Range: [-23.6250, 20.5625], Mean: -0.0923, Std: 5.7447

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: 0.1306, -0.3564, -0.1278, 1.3633, 0.1240
  Q magnitude: 6.4837 (norm of 64-dim vector)
  K_current[0:5]: 0.2866, 0.4177, -1.0088, 1.2236, 0.6582
  K magnitude: 9.0364
  Unscaled Q·K: -2.5823 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=-0.3228 
  Max scaled score: -0.3228
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -2.2617, out_val[0]: -2.2617
  V_current[1]: 4.3555, out_val[1]: 4.3555
  V_current[2]: -5.0742, out_val[2]: -5.0742
  V_current[3]: 0.3489, out_val[3]: 0.3489
  V_current[4]: 0.5112, out_val[4]: 0.5112
[DEEP_INVESTIGATION] After layer 22:
  Range: [-22.3438, 20.1562], Mean: -0.1665, Std: 6.2063

[ATTENTION DEBUG] cache_len=0, q_head=0, kv_head=0
  Q[0:5]: -1.1084, 0.4309, -0.2900, 0.5273, -1.9326
  Q magnitude: 8.1935 (norm of 64-dim vector)
  K_current[0:5]: -0.1151, -1.1113, -1.1309, -1.9199, 0.4749
  K magnitude: 8.6966
  Unscaled Q·K: 0.8317 (before scale=0.1250)
  DEBUG: cache_len=0, should have 1 scores
  Scaled scores (after scale): [0]=0.1040 
  Max scaled score: 0.1040
  Softmax sum: 1.000000 (should be ~1.0)
  Attention weights (should have 1): [0]=1.0000 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.000000 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7476, out_val[0]: 0.7476
  V_current[1]: 0.6890, out_val[1]: 0.6890
  V_current[2]: 1.9785, out_val[2]: 1.9785
  V_current[3]: 0.7881, out_val[3]: 0.7881
  V_current[4]: -0.3293, out_val[4]: -0.3293
[DEEP_INVESTIGATION] After layer 23:
  Range: [-20.9688, 23.4062], Mean: -0.1518, Std: 6.7720
[DEEP_INVESTIGATION] After final RMSNorm:
  Range: [-32.8125, 31.2188], Mean: -0.1597, Std: 7.3213

[DEEP_INVESTIGATION] ========================================
[DEEP_INVESTIGATION] ANALYSIS COMPLETE
[DEEP_INVESTIGATION] ========================================


[TEAM_ALPHA] Hidden state before projection (first 20 values):
  -11.0391 -2.4102 8.1953 1.4717 6.7109 -3.0547 -5.0781 -2.2246 -2.2617 -1.1592 -1.1426 -5.7227 1.7373 -4.5312 -8.4531 -13.8125 -6.4805 23.9688 5.2852 -2.3789 
  Range: [-13.8125, 23.9688]
  ✅ Hidden state values look normal

[PEER_REVIEW] ========================================
[PEER_REVIEW] TEAM ALPHA VERIFICATION TEST SUITE
[PEER_REVIEW] Date: 2025-10-06 15:33 UTC
[PEER_REVIEW] ========================================

[PEER_REVIEW] === TEST 1: cuBLAS VERIFICATION ===
[PEER_REVIEW] Position 0:
  Manual:  3.197784
  cuBLAS:  3.197778
  Diff:    0.000006
  ✅ PASS (diff < 0.0001)
[PEER_REVIEW] Position 8850:
  Manual:  14.264349
  cuBLAS:  14.264330
  Diff:    0.000019
  ✅ PASS (diff < 0.0001)
[PEER_REVIEW] Position 44394:
  Manual:  12.341835
  cuBLAS:  12.341816
  Diff:    0.000019
  ✅ PASS (diff < 0.0001)
[PEER_REVIEW] Position 137131:
  Manual:  14.712263
  cuBLAS:  14.712248
  Diff:    0.000015
  ✅ PASS (diff < 0.0001)

[PEER_REVIEW] Test 1 Result: ✅ ALL TESTS PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

[PEER_REVIEW] === TEST 2: HIDDEN STATE VERIFICATION ===
[PEER_REVIEW] Hidden State Statistics:
  Range: [-32.8125, 31.2188]
  Mean: -0.1597
  Std Dev: 7.3213
  NaN count: 0
  Inf count: 0

[PEER_REVIEW] Checks:
  Range in [-20, 30]: ❌ FAIL
  No NaN values: ✅ PASS
  No Inf values: ✅ PASS

[PEER_REVIEW] Test 2 Result: ❌ SOME CHECKS FAILED
[PEER_REVIEW] Team Alpha Claim: DISPUTED ❌

[PEER_REVIEW] ========================================
[PEER_REVIEW] VERIFICATION COMPLETE
[PEER_REVIEW] Overall: ⚠️ SOME TESTS FAILED
[PEER_REVIEW] ========================================

First 10 logits: 3.20 -1.78 4.45 -4.69 3.27 -1.80 -0.91 1.52 1.05 1.25 
🔍 [ARGMAX DEBUG #0] First 10 logits: 3.20 -1.78 4.45 -4.69 3.27 -1.80 -0.91 1.52 1.05 1.25 
🔍 [ARGMAX DEBUG #0] Max: 14.71 at token_id=137131 (vocab_size=151936)

[PEER_REVIEW] === TEST 4: ARGMAX VERIFICATION ===
[PEER_REVIEW] Argmax Results:
  Original max: 14.712248 at token 137131
  Verified max: 14.712248 at token 137131

[PEER_REVIEW] Checks:
  Indices match: ✅ PASS
  Values match:  ✅ PASS
  Token is 137131: ✅ CONFIRMED (Team Alpha's observation)

[PEER_REVIEW] Test 4 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅


[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.2350, -0.0979, -0.1520, 0.1907, -0.0566
  Q magnitude: 2.1727 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.0058 [1]=-0.0249 
  Max scaled score: 0.0058
  Softmax sum: 1.969774 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5077 [1]=0.4923 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.969774 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0090, out_val[0]: 0.0074
  V_current[1]: 0.0571, out_val[1]: 0.0203
  V_current[2]: 0.0181, out_val[2]: -0.0002
  V_current[3]: 0.0071, out_val[3]: -0.0079
  V_current[4]: 0.0183, out_val[4]: -0.0023

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.5996, -0.0693, -0.3386, -0.8032, -0.4475
  Q magnitude: 4.9160 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.1787 [1]=-0.3054 
  Max scaled score: 0.1787
  Softmax sum: 1.616230 (should be ~1.0)
  Attention weights (should have 2): [0]=0.6187 [1]=0.3813 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.616230 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0544, out_val[0]: -0.0708
  V_current[1]: -0.1716, out_val[1]: -0.0051
  V_current[2]: -0.2251, out_val[2]: 0.1793
  V_current[3]: 0.0988, out_val[3]: -0.0143
  V_current[4]: 0.1467, out_val[4]: 0.0245

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.0155, 0.6807, -0.1249, -0.0489, 0.0239
  Q magnitude: 5.5511 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.4567 [1]=0.6410 
  Max scaled score: 0.6410
  Softmax sum: 1.831717 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4541 [1]=0.5459 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.831717 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.1895, out_val[0]: 0.3369
  V_current[1]: -0.0140, out_val[1]: -0.1238
  V_current[2]: 0.3618, out_val[2]: 0.0230
  V_current[3]: 0.0513, out_val[3]: 0.1688
  V_current[4]: 0.1368, out_val[4]: 0.3326

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.1953, -0.6113, 0.1626, 0.0797, 0.8574
  Q magnitude: 6.2033 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.7387 [1]=-1.1756 
  Max scaled score: -0.7387
  Softmax sum: 1.646040 (should be ~1.0)
  Attention weights (should have 2): [0]=0.6075 [1]=0.3925 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.646040 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.4609, out_val[0]: 0.3151
  V_current[1]: -0.9663, out_val[1]: -0.9654
  V_current[2]: 0.0401, out_val[2]: 0.0463
  V_current[3]: 0.5444, out_val[3]: 0.4250
  V_current[4]: 1.3740, out_val[4]: 1.2174

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.2384, -0.2739, 0.0346, 0.7407, 0.2656
  Q magnitude: 6.7459 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.5409 [1]=-0.7291 
  Max scaled score: -0.5409
  Softmax sum: 1.828484 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5469 [1]=0.4531 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.828484 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.3848, out_val[0]: -0.4304
  V_current[1]: 0.1505, out_val[1]: 0.0699
  V_current[2]: -0.2930, out_val[2]: -0.4903
  V_current[3]: -0.3936, out_val[3]: -0.4699
  V_current[4]: -0.8521, out_val[4]: -0.8579

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.8862, 0.4045, 0.6758, -1.7529, -0.9922
  Q magnitude: 6.5412 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.9499 [1]=-0.5687 
  Max scaled score: -0.5687
  Softmax sum: 1.683039 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4058 [1]=0.5942 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.683039 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.4270, out_val[0]: 0.4739
  V_current[1]: 0.4529, out_val[1]: 0.4004
  V_current[2]: 0.3569, out_val[2]: 0.4172
  V_current[3]: -0.4211, out_val[3]: -0.3563
  V_current[4]: -0.0244, out_val[4]: -0.2089

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.0055, 0.0430, -0.2064, -0.8384, -0.4111
  Q magnitude: 5.2592 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.1262 [1]=-0.0706 
  Max scaled score: -0.0706
  Softmax sum: 1.945907 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4861 [1]=0.5139 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.945907 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0371, out_val[0]: 0.0485
  V_current[1]: 0.0032, out_val[1]: -0.0437
  V_current[2]: -0.3152, out_val[2]: -0.3572
  V_current[3]: 0.0329, out_val[3]: 0.0601
  V_current[4]: -0.1517, out_val[4]: -0.0993

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -1.6719, 0.9961, 0.8823, -0.5239, 0.9824
  Q magnitude: 6.2733 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-1.0488 [1]=-0.9665 
  Max scaled score: -0.9665
  Softmax sum: 1.921012 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4794 [1]=0.5206 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.921012 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8110, out_val[0]: -0.7836
  V_current[1]: -0.6909, out_val[1]: -0.5915
  V_current[2]: -0.4363, out_val[2]: -0.3524
  V_current[3]: -0.2150, out_val[3]: -0.1630
  V_current[4]: -0.3906, out_val[4]: -0.4034

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.2166, -0.2312, -0.8613, -0.2170, -2.7285
  Q magnitude: 9.1599 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.3053 [1]=0.4510 
  Max scaled score: 0.4510
  Softmax sum: 1.469406 (should be ~1.0)
  Attention weights (should have 2): [0]=0.3195 [1]=0.6805 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.469406 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.2842, out_val[0]: -0.3234
  V_current[1]: -0.6431, out_val[1]: -0.5723
  V_current[2]: 0.1289, out_val[2]: 0.0991
  V_current[3]: -0.1825, out_val[3]: -0.1497
  V_current[4]: 0.7778, out_val[4]: 0.7877

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.2573, -0.3494, -0.6729, 1.3848, 0.5527
  Q magnitude: 7.7122 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-1.8092 [1]=-1.4474 
  Max scaled score: -1.4474
  Softmax sum: 1.696397 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4105 [1]=0.5895 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.696397 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7412, out_val[0]: -0.6803
  V_current[1]: 1.6299, out_val[1]: 1.5818
  V_current[2]: -0.5630, out_val[2]: -0.6177
  V_current[3]: -0.6045, out_val[3]: -0.5536
  V_current[4]: -0.4609, out_val[4]: -0.4197

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 1.5635, 0.0955, -0.5547, -1.3008, -1.8037
  Q magnitude: 8.3155 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.7543 [1]=1.0899 
  Max scaled score: 1.0899
  Softmax sum: 1.714929 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4169 [1]=0.5831 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.714929 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.8564, out_val[0]: 0.9175
  V_current[1]: -0.4346, out_val[1]: -0.2277
  V_current[2]: -0.7798, out_val[2]: -0.7440
  V_current[3]: 0.0181, out_val[3]: -0.1050
  V_current[4]: 0.0124, out_val[4]: 0.0708

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -1.3252, 0.2484, 0.2712, -0.1923, 2.1973
  Q magnitude: 10.6234 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-1.0881 [1]=-0.8104 
  Max scaled score: -0.8104
  Softmax sum: 1.757488 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4310 [1]=0.5690 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.757488 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7432, out_val[0]: 0.7773
  V_current[1]: 0.0022, out_val[1]: 0.0471
  V_current[2]: 0.6221, out_val[2]: 0.5365
  V_current[3]: 0.2756, out_val[3]: 0.1735
  V_current[4]: -0.1018, out_val[4]: -0.0533

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.0099, -0.6553, -0.2725, 0.1783, 0.5410
  Q magnitude: 8.2101 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-3.3951 [1]=-3.5830 
  Max scaled score: -3.3951
  Softmax sum: 1.828690 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5468 [1]=0.4532 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.828690 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8013, out_val[0]: -0.7708
  V_current[1]: -0.3669, out_val[1]: -0.2846
  V_current[2]: 0.1299, out_val[2]: 0.1727
  V_current[3]: -0.6704, out_val[3]: -0.6693
  V_current[4]: -0.9878, out_val[4]: -0.9488

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -1.4180, -1.0752, 1.1455, -2.4316, 1.0576
  Q magnitude: 7.9497 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.7724 [1]=-0.0567 
  Max scaled score: -0.0567
  Softmax sum: 1.488835 (should be ~1.0)
  Attention weights (should have 2): [0]=0.3283 [1]=0.6717 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.488835 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.2812, out_val[0]: 1.2659
  V_current[1]: 1.3994, out_val[1]: 1.3843
  V_current[2]: -0.4480, out_val[2]: -0.5311
  V_current[3]: -0.5239, out_val[3]: -0.5637
  V_current[4]: 0.4189, out_val[4]: 0.4444

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.5278, -0.6313, 1.8223, -0.8140, -4.0703
  Q magnitude: 11.6873 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.0883 [1]=0.3692 
  Max scaled score: 0.3692
  Softmax sum: 1.632865 (should be ~1.0)
  Attention weights (should have 2): [0]=0.3876 [1]=0.6124 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.632865 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.6445, out_val[0]: -0.6523
  V_current[1]: -1.0840, out_val[1]: -1.0783
  V_current[2]: 0.2549, out_val[2]: 0.2204
  V_current[3]: 0.6309, out_val[3]: 0.5553
  V_current[4]: -1.0566, out_val[4]: -0.9845

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.8403, 0.2393, -1.1592, -0.9712, -0.6348
  Q magnitude: 6.6123 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=2.5101 [1]=2.2100 
  Max scaled score: 2.5101
  Softmax sum: 1.740708 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5745 [1]=0.4255 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.740708 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.9404, out_val[0]: 1.9881
  V_current[1]: 0.1282, out_val[1]: 0.2025
  V_current[2]: 1.3721, out_val[2]: 1.3996
  V_current[3]: 0.5010, out_val[3]: 0.5374
  V_current[4]: 1.4385, out_val[4]: 1.6242

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.0484, 0.1348, 0.4021, 0.7041, -1.3125
  Q magnitude: 8.4507 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=1.8419 [1]=1.9571 
  Max scaled score: 1.9571
  Softmax sum: 1.891188 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4712 [1]=0.5288 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.891188 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.2949, out_val[0]: 2.2517
  V_current[1]: 1.3691, out_val[1]: 1.4690
  V_current[2]: 1.6152, out_val[2]: 1.5241
  V_current[3]: 1.2949, out_val[3]: 1.3709
  V_current[4]: 0.1582, out_val[4]: 0.1453

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.2188, -0.2070, -1.4658, -0.1500, -0.2830
  Q magnitude: 8.7548 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.0634 [1]=0.3635 
  Max scaled score: 0.3635
  Softmax sum: 1.652538 (should be ~1.0)
  Attention weights (should have 2): [0]=0.3949 [1]=0.6051 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.652538 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.1016, out_val[0]: -1.1124
  V_current[1]: -0.6118, out_val[1]: -0.6147
  V_current[2]: -0.5371, out_val[2]: -0.5338
  V_current[3]: 0.7290, out_val[3]: 0.7236
  V_current[4]: -0.5303, out_val[4]: -0.5455

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.7842, 0.3611, 0.8579, 0.5752, -0.9019
  Q magnitude: 9.3645 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-2.0418 [1]=-1.7575 
  Max scaled score: -1.7575
  Softmax sum: 1.752532 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4294 [1]=0.5706 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.752532 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.4265, out_val[0]: -0.4834
  V_current[1]: 0.3945, out_val[1]: 0.3864
  V_current[2]: 0.2576, out_val[2]: 0.1398
  V_current[3]: -0.4456, out_val[3]: -0.4803
  V_current[4]: -0.0748, out_val[4]: -0.0773

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.3230, -0.5146, -0.7500, 0.6851, 0.4512
  Q magnitude: 6.9784 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.5800 [1]=0.7807 
  Max scaled score: 0.7807
  Softmax sum: 1.818222 (should be ~1.0)
  Attention weights (should have 2): [0]=0.4500 [1]=0.5500 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.818222 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0446, out_val[0]: -0.0443
  V_current[1]: 0.2159, out_val[1]: 0.1923
  V_current[2]: 0.2761, out_val[2]: 0.3083
  V_current[3]: -0.8037, out_val[3]: -0.8013
  V_current[4]: 0.0264, out_val[4]: 0.0288

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 1.1045, -1.1807, 1.6143, 0.7847, -1.6133
  Q magnitude: 9.6335 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.4127 [1]=-0.5241 
  Max scaled score: -0.4127
  Softmax sum: 1.894564 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5278 [1]=0.4722 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.894564 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5347, out_val[0]: 0.5633
  V_current[1]: 3.1211, out_val[1]: 3.1623
  V_current[2]: 1.1992, out_val[2]: 1.1549
  V_current[3]: -0.7974, out_val[3]: -0.6574
  V_current[4]: -0.2166, out_val[4]: -0.2141

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 1.8994, -1.6836, -1.8027, 1.4189, 1.1094
  Q magnitude: 8.8758 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.0746 [1]=-0.5169 
  Max scaled score: -0.0746
  Softmax sum: 1.642500 (should be ~1.0)
  Attention weights (should have 2): [0]=0.6088 [1]=0.3912 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.642500 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.3789, out_val[0]: 2.2160
  V_current[1]: 0.0610, out_val[1]: 0.1558
  V_current[2]: 6.1211, out_val[2]: 6.3161
  V_current[3]: -2.3145, out_val[3]: -1.9310
  V_current[4]: -0.7085, out_val[4]: -0.6565

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: 0.3057, -0.1015, -0.8887, 0.9707, -0.0017
  Q magnitude: 6.5232 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=-0.1454 [1]=-0.1538 
  Max scaled score: -0.1454
  Softmax sum: 1.991712 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5021 [1]=0.4979 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.991712 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.8916, out_val[0]: -2.0774
  V_current[1]: 4.6172, out_val[1]: 4.4858
  V_current[2]: -5.0820, out_val[2]: -5.0781
  V_current[3]: 0.7251, out_val[3]: 0.5362
  V_current[4]: 0.5400, out_val[4]: 0.5256

[ATTENTION DEBUG] cache_len=1, q_head=0, kv_head=0
  Q[0:5]: -0.9053, -0.6431, -0.5278, 0.3179, -1.4355
  Q magnitude: 8.3339 (norm of 64-dim vector)
  DEBUG: cache_len=1, should have 2 scores
  Scaled scores (after scale): [0]=0.5053 [1]=0.3237 
  Max scaled score: 0.5053
  Softmax sum: 1.833962 (should be ~1.0)
  Attention weights (should have 2): [0]=0.5453 [1]=0.4547 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.833962 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.0098, out_val[0]: 0.8668
  V_current[1]: 0.7080, out_val[1]: 0.6976
  V_current[2]: 1.5771, out_val[2]: 1.7960
  V_current[3]: 1.0303, out_val[3]: 0.8982
  V_current[4]: -0.1771, out_val[4]: -0.2601
First 10 logits: 3.15 -1.77 4.15 -4.65 3.07 -1.66 -1.17 1.47 1.16 1.21 
🔍 [ARGMAX DEBUG #1] First 10 logits: 3.15 -1.77 4.15 -4.65 3.07 -1.66 -1.17 1.47 1.16 1.21 
🔍 [ARGMAX DEBUG #1] Max: 14.66 at token_id=137131 (vocab_size=151936)

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.1248, 0.0657, 0.2222, -0.1932, 0.0861
  Q magnitude: 1.4717 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.0104 [1]=0.0313 [2]=0.0406 
  Max scaled score: 0.0406
  Softmax sum: 2.941040 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3231 [1]=0.3369 [2]=0.3400 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.941040 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0197, out_val[0]: 0.0116
  V_current[1]: -0.0142, out_val[1]: 0.0094
  V_current[2]: 0.0054, out_val[2]: 0.0021
  V_current[3]: -0.0226, out_val[3]: -0.0126
  V_current[4]: 0.0192, out_val[4]: 0.0055

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.0119, 0.6948, 0.0720, -0.8320, -0.3503
  Q magnitude: 5.2129 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=0.5598 [1]=0.2430 [2]=0.1706 
  Max scaled score: 0.5598
  Softmax sum: 2.406121 (should be ~1.0)
  Attention weights (should have 3): [0]=0.4156 [1]=0.3028 [2]=0.2816 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.406121 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.6489, out_val[0]: 0.1327
  V_current[1]: -0.0961, out_val[1]: -0.0385
  V_current[2]: 0.3313, out_val[2]: 0.2032
  V_current[3]: 0.0534, out_val[3]: 0.0100
  V_current[4]: 0.1100, out_val[4]: 0.0543

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -1.4756, 0.4983, 0.7466, 0.0221, 0.4221
  Q magnitude: 6.3387 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.2856 [1]=0.7181 [2]=1.0442 
  Max scaled score: 1.0442
  Softmax sum: 1.986315 (should be ~1.0)
  Attention weights (should have 3): [0]=0.1332 [1]=0.3634 [2]=0.5034 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.986315 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0833, out_val[0]: 0.1793
  V_current[1]: 0.1440, out_val[1]: 0.0334
  V_current[2]: -0.1285, out_val[2]: 0.0156
  V_current[3]: -0.1414, out_val[3]: -0.0112
  V_current[4]: 0.4219, out_val[4]: 0.3377

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.0972, -0.4543, -0.1015, 0.3420, 0.9790
  Q magnitude: 6.7463 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.5501 [1]=-0.9775 [2]=-1.4572 
  Max scaled score: -0.5501
  Softmax sum: 2.055893 (should be ~1.0)
  Attention weights (should have 3): [0]=0.4864 [1]=0.3172 [2]=0.1964 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.055893 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0854, out_val[0]: 0.2705
  V_current[1]: -0.9487, out_val[1]: -0.9621
  V_current[2]: 0.1462, out_val[2]: 0.0659
  V_current[3]: 0.3994, out_val[3]: 0.4204
  V_current[4]: 1.3125, out_val[4]: 1.2365

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.3914, -0.4089, -0.6016, 0.7090, 0.2654
  Q magnitude: 6.4525 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.4929 [1]=-0.9418 [2]=-1.1883 
  Max scaled score: -0.4929
  Softmax sum: 2.137274 (should be ~1.0)
  Attention weights (should have 3): [0]=0.4679 [1]=0.2987 [2]=0.2334 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.137274 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.5654, out_val[0]: -0.4660
  V_current[1]: 0.0858, out_val[1]: 0.0664
  V_current[2]: -0.0745, out_val[2]: -0.4108
  V_current[3]: -0.3772, out_val[3]: -0.4551
  V_current[4]: -0.6582, out_val[4]: -0.8118

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.0896, 1.1348, 1.4590, -1.1426, -1.5098
  Q magnitude: 6.4453 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-1.0519 [1]=-0.8414 [2]=-0.3847 
  Max scaled score: -0.3847
  Softmax sum: 2.146565 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2391 [1]=0.2951 [2]=0.4659 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.146565 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.4744, out_val[0]: 0.4767
  V_current[1]: 0.4045, out_val[1]: 0.3994
  V_current[2]: 0.4617, out_val[2]: 0.4412
  V_current[3]: -0.4673, out_val[3]: -0.4045
  V_current[4]: -0.1089, out_val[4]: -0.1725

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.0654, 0.0711, 0.5596, -0.9790, -0.5498
  Q magnitude: 5.2596 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.0120 [1]=0.0549 [2]=-0.1370 
  Max scaled score: 0.0549
  Softmax sum: 2.760639 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3388 [1]=0.3622 [2]=0.2990 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.760639 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.2070, out_val[0]: -0.0283
  V_current[1]: 0.0630, out_val[1]: -0.0116
  V_current[2]: -0.3459, out_val[2]: -0.3537
  V_current[3]: 0.0645, out_val[3]: 0.0613
  V_current[4]: -0.2286, out_val[4]: -0.1382

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -1.6943, -0.4531, 0.8667, 0.0781, 1.0713
  Q magnitude: 6.4300 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.8813 [1]=-0.8721 [2]=-0.8015 
  Max scaled score: -0.8015
  Softmax sum: 2.855127 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3234 [1]=0.3264 [2]=0.3502 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.855127 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8125, out_val[0]: -0.7931
  V_current[1]: -0.6704, out_val[1]: -0.6167
  V_current[2]: -0.5435, out_val[2]: -0.4172
  V_current[3]: -0.3953, out_val[3]: -0.2430
  V_current[4]: -0.3875, out_val[4]: -0.3981

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.3608, 0.1971, -0.6079, -0.7231, -5.1875
  Q magnitude: 9.1018 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-1.0072 [1]=0.0398 [2]=0.3491 
  Max scaled score: 0.3491
  Softmax sum: 1.991580 (should be ~1.0)
  Attention weights (should have 3): [0]=0.1294 [1]=0.3685 [2]=0.5021 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.991580 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.1389, out_val[0]: -0.2271
  V_current[1]: -0.6953, out_val[1]: -0.6407
  V_current[2]: 0.2607, out_val[2]: 0.1830
  V_current[3]: -0.1326, out_val[3]: -0.1442
  V_current[4]: 0.5332, out_val[4]: 0.6590

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.4746, -0.0058, -1.5537, 0.8506, 1.1221
  Q magnitude: 7.7617 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-1.8096 [1]=-1.3512 [2]=-1.3363 
  Max scaled score: -1.3363
  Softmax sum: 2.608243 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2389 [1]=0.3777 [2]=0.3834 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.608243 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.6855, out_val[0]: -0.6844
  V_current[1]: 1.7324, out_val[1]: 1.6412
  V_current[2]: -0.4260, out_val[2]: -0.5423
  V_current[3]: -0.5928, out_val[3]: -0.5704
  V_current[4]: -0.5371, out_val[4]: -0.4662

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.8965, 1.4141, 0.3638, -1.3223, -2.4844
  Q magnitude: 8.4567 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=0.5786 [1]=0.8054 [2]=0.9880 
  Max scaled score: 0.9880
  Softmax sum: 2.497201 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2659 [1]=0.3336 [2]=0.4004 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.497201 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7275, out_val[0]: 0.8438
  V_current[1]: -0.4226, out_val[1]: -0.2978
  V_current[2]: -0.7305, out_val[2]: -0.7372
  V_current[3]: 0.2302, out_val[3]: 0.0245
  V_current[4]: 0.2113, out_val[4]: 0.1293

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.7681, -0.9448, 0.2910, 0.1685, 2.0039
  Q magnitude: 10.7790 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-1.2486 [1]=-0.9598 [2]=-0.9572 
  Max scaled score: -0.9572
  Softmax sum: 2.744614 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2722 [1]=0.3634 [2]=0.3644 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.744614 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7461, out_val[0]: 0.7658
  V_current[1]: -0.0960, out_val[1]: -0.0052
  V_current[2]: 0.6089, out_val[2]: 0.5632
  V_current[3]: 0.4634, out_val[3]: 0.2795
  V_current[4]: -0.0954, out_val[4]: -0.0689

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.6816, -0.4453, -0.3257, 0.0305, 1.0166
  Q magnitude: 8.4097 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-3.6095 [1]=-3.8672 [2]=-3.7803 
  Max scaled score: -3.6095
  Softmax sum: 2.615748 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3823 [1]=0.2954 [2]=0.3223 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.615748 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8628, out_val[0]: -0.7998
  V_current[1]: -0.4365, out_val[1]: -0.3318
  V_current[2]: 0.1262, out_val[2]: 0.1586
  V_current[3]: -0.5059, out_val[3]: -0.6166
  V_current[4]: -1.0791, out_val[4]: -0.9900

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.2188, -1.6533, 2.4043, -1.1123, 0.2510
  Q magnitude: 7.8345 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.5846 [1]=-0.3941 [2]=0.2779 
  Max scaled score: 0.2779
  Softmax sum: 1.932827 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2184 [1]=0.2642 [2]=0.5174 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.932827 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.2490, out_val[0]: 1.2543
  V_current[1]: 1.4014, out_val[1]: 1.3904
  V_current[2]: -0.3921, out_val[2]: -0.4744
  V_current[3]: -0.4844, out_val[3]: -0.5299
  V_current[4]: 0.4055, out_val[4]: 0.4289

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.2891, -0.7329, 2.0664, 0.4128, -4.1094
  Q magnitude: 11.7861 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.4422 [1]=-0.4870 [2]=-0.1390 
  Max scaled score: -0.1390
  Softmax sum: 2.444513 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3021 [1]=0.2888 [2]=0.4091 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.444513 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7012, out_val[0]: -0.6737
  V_current[1]: -1.1904, out_val[1]: -1.1231
  V_current[2]: 0.3123, out_val[2]: 0.2515
  V_current[3]: 0.7861, out_val[3]: 0.6355
  V_current[4]: -1.1016, out_val[4]: -1.0188

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.2571, 0.7432, -0.2803, -1.5557, -0.4082
  Q magnitude: 6.7228 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=2.5417 [1]=2.4105 [2]=2.1827 
  Max scaled score: 2.5417
  Softmax sum: 2.575487 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3883 [1]=0.3405 [2]=0.2712 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.575487 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.7998, out_val[0]: 1.9345
  V_current[1]: -0.0297, out_val[1]: 0.1356
  V_current[2]: 1.3223, out_val[2]: 1.3771
  V_current[3]: 0.3528, out_val[3]: 0.4854
  V_current[4]: 1.3135, out_val[4]: 1.5301

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.2729, -0.0591, -0.1014, 0.7988, -2.3438
  Q magnitude: 8.5588 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=1.7694 [1]=1.9424 [2]=1.8478 
  Max scaled score: 1.9424
  Softmax sum: 2.750918 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3058 [1]=0.3635 [2]=0.3307 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.750918 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.5859, out_val[0]: 2.3631
  V_current[1]: 1.2695, out_val[1]: 1.4010
  V_current[2]: 1.5557, out_val[2]: 1.5364
  V_current[3]: 1.0938, out_val[3]: 1.2777
  V_current[4]: 0.0714, out_val[4]: 0.1211

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.2607, 0.0781, -1.2295, -0.8481, 0.1345
  Q magnitude: 8.8039 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.3637 [1]=0.2047 [2]=0.6757 
  Max scaled score: 0.6757
  Softmax sum: 1.978067 (should be ~1.0)
  Attention weights (should have 3): [0]=0.1788 [1]=0.3157 [2]=0.5055 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.978067 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.2051, out_val[0]: -1.1588
  V_current[1]: -0.6416, out_val[1]: -0.6282
  V_current[2]: -0.5996, out_val[2]: -0.5672
  V_current[3]: 0.6738, out_val[3]: 0.6987
  V_current[4]: -0.6011, out_val[4]: -0.5730

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: -0.6660, -0.5181, 0.4329, 0.9019, -0.6182
  Q magnitude: 9.3556 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-1.9397 [1]=-1.7400 [2]=-1.5278 
  Max scaled score: -1.5278
  Softmax sum: 2.471181 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2680 [1]=0.3273 [2]=0.4047 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.471181 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.3535, out_val[0]: -0.4325
  V_current[1]: 0.4536, out_val[1]: 0.4133
  V_current[2]: 0.3694, out_val[2]: 0.2293
  V_current[3]: -0.3352, out_val[3]: -0.4226
  V_current[4]: -0.1500, out_val[4]: -0.1068

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.5830, 0.0176, -0.8882, 0.0258, 0.2219
  Q magnitude: 6.9247 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=0.3568 [1]=0.6549 [2]=0.8153 
  Max scaled score: 0.8153
  Softmax sum: 2.484020 (should be ~1.0)
  Attention weights (should have 3): [0]=0.2545 [1]=0.3429 [2]=0.4026 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.484020 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0742, out_val[0]: -0.0563
  V_current[1]: 0.3804, out_val[1]: 0.2687
  V_current[2]: 0.2708, out_val[2]: 0.2922
  V_current[3]: -0.7490, out_val[3]: -0.7803
  V_current[4]: 0.0494, out_val[4]: 0.0370

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 1.5312, 0.2825, 0.8354, 1.5391, -1.8936
  Q magnitude: 9.6120 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.0182 [1]=-0.2144 [2]=-0.4297 
  Max scaled score: -0.0182
  Softmax sum: 2.484497 (should be ~1.0)
  Attention weights (should have 3): [0]=0.4025 [1]=0.3308 [2]=0.2667 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.484497 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5249, out_val[0]: 0.5539
  V_current[1]: 3.1523, out_val[1]: 3.1609
  V_current[2]: 1.0664, out_val[2]: 1.1300
  V_current[3]: -0.9058, out_val[3]: -0.7196
  V_current[4]: -0.1996, out_val[4]: -0.2102

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 2.4199, 0.6450, -2.3340, 0.0532, 0.9102
  Q magnitude: 8.8840 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=0.4841 [1]=-0.1247 [2]=-0.5444 
  Max scaled score: 0.4841
  Softmax sum: 1.901591 (should be ~1.0)
  Attention weights (should have 3): [0]=0.5259 [1]=0.2861 [2]=0.1880 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 1.901591 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.6504, out_val[0]: 2.2892
  V_current[1]: 0.0501, out_val[1]: 0.1408
  V_current[2]: 6.2031, out_val[2]: 6.3050
  V_current[3]: -2.6699, out_val[3]: -2.0501
  V_current[4]: -0.7407, out_val[4]: -0.6696

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.2015, 0.1425, -1.2783, 0.2764, -0.1589
  Q magnitude: 6.6046 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=-0.0865 [1]=0.0032 [2]=0.0122 
  Max scaled score: 0.0122
  Softmax sum: 2.897132 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3127 [1]=0.3421 [2]=0.3452 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.897132 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.8711, out_val[0]: -2.0003
  V_current[1]: 4.5586, out_val[1]: 4.5151
  V_current[2]: -5.1133, out_val[2]: -5.0904
  V_current[3]: 0.9053, out_val[3]: 0.6696
  V_current[4]: 0.5903, out_val[4]: 0.5484

[ATTENTION DEBUG] cache_len=2, q_head=0, kv_head=0
  Q[0:5]: 0.0291, -1.0244, -0.5962, -0.0144, -0.5840
  Q magnitude: 8.3747 (norm of 64-dim vector)
  DEBUG: cache_len=2, should have 3 scores
  Scaled scores (after scale): [0]=0.6862 [1]=0.6172 [2]=0.3399 
  Max scaled score: 0.6862
  Softmax sum: 2.640648 (should be ~1.0)
  Attention weights (should have 3): [0]=0.3787 [1]=0.3535 [2]=0.2678 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.640648 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.0889, out_val[0]: 0.9317
  V_current[1]: 0.6187, out_val[1]: 0.6769
  V_current[2]: 1.4922, out_val[2]: 1.7064
  V_current[3]: 1.0840, out_val[3]: 0.9529
  V_current[4]: -0.1694, out_val[4]: -0.2327
First 10 logits: 3.11 -1.68 4.05 -4.53 2.91 -1.77 -1.31 1.41 1.27 1.31 
🔍 [ARGMAX DEBUG #2] First 10 logits: 3.11 -1.68 4.05 -4.53 2.91 -1.77 -1.31 1.41 1.27 1.31 
🔍 [ARGMAX DEBUG #2] Max: 14.45 at token_id=137131 (vocab_size=151936)

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.0276, -0.1610, 0.1917, 0.0259, 0.0948
  Q magnitude: 1.2072 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.0326 [1]=-0.0153 [2]=-0.0016 [3]=-0.0060 
  Max scaled score: -0.0016
  Softmax sum: 3.951662 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2454 [1]=0.2496 [2]=0.2531 [3]=0.2520 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.951662 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0214, out_val[0]: 0.0141
  V_current[1]: -0.0171, out_val[1]: 0.0026
  V_current[2]: -0.0258, out_val[2]: -0.0050
  V_current[3]: -0.0248, out_val[3]: -0.0157
  V_current[4]: -0.0159, out_val[4]: -0.0000

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.6816, -0.3879, 0.0209, -0.2223, -0.0801
  Q magnitude: 5.5419 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.6329 [1]=0.5183 [2]=-0.1910 [3]=1.0088 
  Max scaled score: 1.0088
  Softmax sum: 2.600268 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2641 [1]=0.2355 [2]=0.1159 [3]=0.3846 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.600268 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.1429, out_val[0]: 0.0960
  V_current[1]: -0.1621, out_val[1]: -0.0881
  V_current[2]: 0.3616, out_val[2]: 0.2376
  V_current[3]: 0.2446, out_val[3]: 0.1013
  V_current[4]: 0.1026, out_val[4]: 0.0733

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.0280, -1.5459, -0.0062, 0.2529, -0.0109
  Q magnitude: 6.3514 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.8120 [1]=1.3318 [2]=1.2260 [3]=-0.5432 
  Max scaled score: 1.3318
  Softmax sum: 2.170196 (should be ~1.0)
  Attention weights (should have 4): [0]=0.0540 [1]=0.4608 [2]=0.4145 [3]=0.0707 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.170196 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.3677, out_val[0]: 0.1756
  V_current[1]: -0.2271, out_val[1]: 0.0234
  V_current[2]: -0.1576, out_val[2]: 0.0815
  V_current[3]: -0.3782, out_val[3]: -0.0449
  V_current[4]: 0.2810, out_val[4]: 0.2885

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.5811, -0.0340, -0.9238, 0.0740, 1.0879
  Q magnitude: 6.5214 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.6291 [1]=-1.0523 [2]=-1.6662 [3]=-1.6992 
  Max scaled score: -0.6291
  Softmax sum: 2.352441 (should be ~1.0)
  Attention weights (should have 4): [0]=0.4251 [1]=0.2784 [2]=0.1507 [3]=0.1458 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.352441 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0156, out_val[0]: 0.2374
  V_current[1]: -0.9697, out_val[1]: -0.9635
  V_current[2]: 0.0627, out_val[2]: 0.0637
  V_current[3]: 0.4023, out_val[3]: 0.4183
  V_current[4]: 1.1328, out_val[4]: 1.2200

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.5513, 0.2430, -1.0205, 0.2358, 0.0733
  Q magnitude: 6.8671 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.2416 [1]=-0.8736 [2]=-1.2400 [3]=-1.0603 
  Max scaled score: -0.2416
  Softmax sum: 2.341047 (should be ~1.0)
  Attention weights (should have 4): [0]=0.4272 [1]=0.2271 [2]=0.1574 [3]=0.1884 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.341047 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.5415, out_val[0]: -0.4784
  V_current[1]: 0.3665, out_val[1]: 0.1180
  V_current[2]: -0.1605, out_val[2]: -0.3878
  V_current[3]: -0.2798, out_val[3]: -0.4292
  V_current[4]: -0.8271, out_val[4]: -0.8214

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -1.0605, 0.4543, 1.6904, -0.0520, -1.6953
  Q magnitude: 6.3620 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-1.0878 [1]=-1.2200 [2]=-0.8913 [3]=-0.4214 
  Max scaled score: -0.4214
  Softmax sum: 2.588496 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1984 [1]=0.1738 [2]=0.2415 [3]=0.3863 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.588496 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.4446, out_val[0]: 0.4681
  V_current[1]: 0.4155, out_val[1]: 0.4011
  V_current[2]: 0.4048, out_val[2]: 0.4302
  V_current[3]: -0.5308, out_val[3]: -0.4430
  V_current[4]: 0.0355, out_val[4]: -0.1119

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.0182, -0.0956, 1.0117, -0.5005, -0.7324
  Q magnitude: 5.3711 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.0302 [1]=0.1158 [2]=-0.0883 [3]=-0.2210 
  Max scaled score: 0.1158
  Softmax sum: 3.447478 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2663 [1]=0.2901 [2]=0.2365 [3]=0.2071 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.447478 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.2976, out_val[0]: -0.0844
  V_current[1]: 0.1356, out_val[1]: 0.0191
  V_current[2]: -0.2881, out_val[2]: -0.3399
  V_current[3]: 0.0774, out_val[3]: 0.0645
  V_current[4]: -0.1549, out_val[4]: -0.1419

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.7451, -1.6094, 0.6636, 0.6289, 1.0635
  Q magnitude: 6.4430 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.8913 [1]=-0.9692 [2]=-0.9760 [3]=-1.0011 
  Max scaled score: -0.8913
  Softmax sum: 3.740031 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2674 [1]=0.2474 [2]=0.2457 [3]=0.2396 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.740031 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7959, out_val[0]: -0.7925
  V_current[1]: -0.6753, out_val[1]: -0.6267
  V_current[2]: -0.6270, out_val[2]: -0.4615
  V_current[3]: -0.4900, out_val[3]: -0.2962
  V_current[4]: -0.4333, out_val[4]: -0.4072

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.0490, 0.4719, -0.0350, -0.9697, -7.0938
  Q magnitude: 9.4682 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-1.7165 [1]=-0.6216 [2]=-0.0473 [3]=0.2248 
  Max scaled score: 0.2248
  Softmax sum: 2.334332 (should be ~1.0)
  Attention weights (should have 4): [0]=0.0615 [1]=0.1838 [2]=0.3264 [3]=0.4284 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.334332 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0255, out_val[0]: -0.1335
  V_current[1]: -0.6768, out_val[1]: -0.6609
  V_current[2]: 0.3218, out_val[2]: 0.2488
  V_current[3]: -0.0975, out_val[3]: -0.1235
  V_current[4]: 0.5200, out_val[4]: 0.5894

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.2795, 0.4143, -1.7344, -0.2954, 1.6289
  Q magnitude: 7.9507 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-1.6858 [1]=-1.1354 [2]=-0.9845 [3]=-1.2289 
  Max scaled score: -0.9845
  Softmax sum: 3.139031 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1580 [1]=0.2739 [2]=0.3186 [3]=0.2495 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.139031 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7119, out_val[0]: -0.6927
  V_current[1]: 1.6641, out_val[1]: 1.6526
  V_current[2]: -0.3789, out_val[2]: -0.4945
  V_current[3]: -0.7119, out_val[3]: -0.6080
  V_current[4]: -0.6782, out_val[4]: -0.5236

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.7549, 1.5596, 1.0742, -0.8379, -2.6660
  Q magnitude: 8.7234 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.7845 [1]=0.9671 [2]=1.1149 [3]=1.4188 
  Max scaled score: 1.4188
  Softmax sum: 2.904693 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1826 [1]=0.2191 [2]=0.2540 [3]=0.3443 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.904693 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5767, out_val[0]: 0.7541
  V_current[1]: -0.4399, out_val[1]: -0.3428
  V_current[2]: -0.6240, out_val[2]: -0.6979
  V_current[3]: 0.2832, out_val[3]: 0.1094
  V_current[4]: 0.2365, out_val[4]: 0.1656

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.2542, -1.1768, 0.1711, 0.4197, 1.2070
  Q magnitude: 10.7629 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-1.4614 [1]=-1.1226 [2]=-1.0898 [3]=-0.9434 
  Max scaled score: -0.9434
  Softmax sum: 3.295548 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1808 [1]=0.2537 [2]=0.2621 [3]=0.3034 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.295548 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7334, out_val[0]: 0.7553
  V_current[1]: -0.1266, out_val[1]: -0.0438
  V_current[2]: 0.6338, out_val[2]: 0.5863
  V_current[3]: 0.6104, out_val[3]: 0.3836
  V_current[4]: -0.0017, out_val[4]: -0.0494

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.7690, 0.2230, -0.3145, -0.1819, 1.5293
  Q magnitude: 8.5262 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-3.6513 [1]=-3.9068 [2]=-3.8492 [3]=-3.4394 
  Max scaled score: -3.4394
  Softmax sum: 3.099418 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2610 [1]=0.2022 [2]=0.2142 [3]=0.3226 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.099418 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8701, out_val[0]: -0.8221
  V_current[1]: -0.5420, out_val[1]: -0.3990
  V_current[2]: 0.1030, out_val[2]: 0.1409
  V_current[3]: -0.5054, out_val[3]: -0.5814
  V_current[4]: -1.1709, out_val[4]: -1.0478

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 1.5527, -0.7026, 2.5918, 0.6387, -0.4392
  Q magnitude: 7.8194 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.1842 [1]=-0.5330 [2]=-0.4146 [3]=0.1265 
  Max scaled score: 0.1265
  Softmax sum: 2.832228 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2588 [1]=0.1826 [2]=0.2055 [3]=0.3531 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.832228 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.3486, out_val[0]: 1.2863
  V_current[1]: 1.4297, out_val[1]: 1.3986
  V_current[2]: -0.3267, out_val[2]: -0.4592
  V_current[3]: -0.4580, out_val[3]: -0.5239
  V_current[4]: 0.4414, out_val[4]: 0.4441

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.6572, -0.2366, 1.3105, 1.5186, -3.2617
  Q magnitude: 11.7460 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.0617 [1]=-0.5029 [2]=-0.6539 [3]=-0.2180 
  Max scaled score: 0.0617
  Softmax sum: 2.813517 (should be ~1.0)
  Attention weights (should have 4): [0]=0.3554 [1]=0.2021 [2]=0.1738 [3]=0.2687 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.813517 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8320, out_val[0]: -0.7119
  V_current[1]: -1.4111, out_val[1]: -1.1852
  V_current[2]: 0.3311, out_val[2]: 0.2537
  V_current[3]: 0.8511, out_val[3]: 0.6477
  V_current[4]: -1.0117, out_val[4]: -0.9863

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.3967, 0.5913, 0.8018, -1.4170, -0.0368
  Q magnitude: 6.7922 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=2.4284 [1]=2.4694 [2]=2.3442 [3]=2.2392 
  Max scaled score: 2.4694
  Softmax sum: 3.636447 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2639 [1]=0.2750 [2]=0.2426 [3]=0.2184 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.636447 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.8027, out_val[0]: 1.8981
  V_current[1]: -0.0730, out_val[1]: 0.0801
  V_current[2]: 1.3516, out_val[2]: 1.3681
  V_current[3]: 0.2429, out_val[3]: 0.4254
  V_current[4]: 1.3525, out_val[4]: 1.4747

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.1058, -0.3953, -0.5303, 0.5342, -2.9590
  Q magnitude: 8.5751 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=1.6050 [1]=2.0238 [2]=2.0715 [3]=1.9715 
  Max scaled score: 2.0715
  Softmax sum: 3.485490 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1799 [1]=0.2735 [2]=0.2869 [3]=0.2596 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.485490 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.5664, out_val[0]: 2.4324
  V_current[1]: 1.2734, out_val[1]: 1.3539
  V_current[2]: 1.6709, out_val[2]: 1.5778
  V_current[3]: 1.1338, out_val[3]: 1.2244
  V_current[4]: 0.0458, out_val[4]: 0.0992

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.0943, 0.2072, -0.5703, -1.4551, 0.4866
  Q magnitude: 8.8428 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.7227 [1]=-0.1056 [2]=0.5093 [3]=0.6943 
  Max scaled score: 0.6943
  Softmax sum: 2.522862 (should be ~1.0)
  Attention weights (should have 4): [0]=0.0961 [1]=0.1781 [2]=0.3294 [3]=0.3964 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.522862 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.2012, out_val[0]: -1.1778
  V_current[1]: -0.7002, out_val[1]: -0.6574
  V_current[2]: -0.6206, out_val[2]: -0.5900
  V_current[3]: 0.6382, out_val[3]: 0.6735
  V_current[4]: -0.5957, out_val[4]: -0.5832

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.1125, -0.8618, -0.1830, 0.9741, -0.2045
  Q magnitude: 9.4135 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-1.8314 [1]=-1.7204 [2]=-1.6030 [3]=-1.4063 
  Max scaled score: -1.4063
  Softmax sum: 3.205716 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2039 [1]=0.2279 [2]=0.2563 [3]=0.3119 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.205716 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.3008, out_val[0]: -0.3956
  V_current[1]: 0.5205, out_val[1]: 0.4451
  V_current[2]: 0.4338, out_val[2]: 0.2853
  V_current[3]: -0.3232, out_val[3]: -0.3956
  V_current[4]: -0.1910, out_val[4]: -0.1315

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.2747, 0.4700, -0.6255, -0.6016, -0.0393
  Q magnitude: 6.8581 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.1379 [1]=0.4993 [2]=0.7519 [3]=0.9552 
  Max scaled score: 0.9552
  Softmax sum: 2.891538 (should be ~1.0)
  Attention weights (should have 4): [0]=0.1527 [1]=0.2192 [2]=0.2822 [3]=0.3458 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.891538 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0211, out_val[0]: -0.0447
  V_current[1]: 0.5063, out_val[1]: 0.3547
  V_current[2]: 0.1962, out_val[2]: 0.2579
  V_current[3]: -0.7178, out_val[3]: -0.7577
  V_current[4]: 0.0746, out_val[4]: 0.0504

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.6367, 1.3691, -0.2096, 1.6943, -1.9355
  Q magnitude: 9.4977 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.2594 [1]=0.2345 [2]=-0.0831 [3]=-0.3870 
  Max scaled score: 0.2594
  Softmax sum: 3.209377 (should be ~1.0)
  Attention weights (should have 4): [0]=0.3116 [1]=0.3039 [2]=0.2212 [3]=0.1633 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.209377 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5479, out_val[0]: 0.5515
  V_current[1]: 3.1133, out_val[1]: 3.1511
  V_current[2]: 1.0049, out_val[2]: 1.1119
  V_current[3]: -0.7915, out_val[3]: -0.7378
  V_current[4]: -0.1532, out_val[4]: -0.2010

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.7817, 2.3086, -1.8516, -1.3408, 0.5166
  Q magnitude: 8.8406 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.9529 [1]=0.4737 [2]=-0.1021 [3]=-0.5371 
  Max scaled score: 0.9529
  Softmax sum: 2.192921 (should be ~1.0)
  Attention weights (should have 4): [0]=0.4560 [1]=0.2824 [2]=0.1588 [3]=0.1028 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.192921 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.8496, out_val[0]: 2.3484
  V_current[1]: -0.0442, out_val[1]: 0.1194
  V_current[2]: 5.8867, out_val[2]: 6.2561
  V_current[3]: -2.8105, out_val[3]: -2.1346
  V_current[4]: -0.7881, out_val[4]: -0.6828

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: -0.0203, 0.2168, -1.1650, -0.5513, -0.4104
  Q magnitude: 6.6742 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=-0.1204 [1]=0.0335 [2]=0.1172 [3]=0.0901 
  Max scaled score: 0.1172
  Softmax sum: 3.681611 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2142 [1]=0.2498 [2]=0.2716 [3]=0.2644 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.681611 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.8262, out_val[0]: -1.9480
  V_current[1]: 4.4688, out_val[1]: 4.5060
  V_current[2]: -5.1875, out_val[2]: -5.1167
  V_current[3]: 0.8091, out_val[3]: 0.7157
  V_current[4]: 0.5723, out_val[4]: 0.5560

[ATTENTION DEBUG] cache_len=3, q_head=0, kv_head=0
  Q[0:5]: 0.8408, -0.5386, -0.4697, -0.3323, 0.3542
  Q magnitude: 8.4048 (norm of 64-dim vector)
  DEBUG: cache_len=3, should have 4 scores
  Scaled scores (after scale): [0]=0.7377 [1]=0.8470 [2]=0.6614 [3]=0.3224 
  Max scaled score: 0.8470
  Softmax sum: 3.318873 (should be ~1.0)
  Attention weights (should have 4): [0]=0.2701 [1]=0.3013 [2]=0.2503 [3]=0.1783 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.318873 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.3506, out_val[0]: 1.0195
  V_current[1]: 0.5215, out_val[1]: 0.6472
  V_current[2]: 1.5420, out_val[2]: 1.6580
  V_current[3]: 1.1074, out_val[3]: 0.9921
  V_current[4]: -0.1647, out_val[4]: -0.2141
🔍 [ARGMAX DEBUG #3] First 10 logits: 3.10 -1.53 4.08 -4.40 2.81 -1.79 -1.48 1.31 1.34 1.35 
🔍 [ARGMAX DEBUG #3] Max: 14.35 at token_id=137131 (vocab_size=151936)

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.1655, -0.0299, 0.0121, -0.0776, 0.0209
  Q magnitude: 1.2177 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.0378 [1]=0.0691 [2]=0.0735 [3]=-0.0219 [4]=-0.0191 
  Max scaled score: 0.0735
  Softmax sum: 4.780987 (should be ~1.0)
  Attention weights (should have 5): [0]=0.2018 [1]=0.2082 [2]=0.2092 [3]=0.1901 [4]=0.1907 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.780987 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0108, out_val[0]: 0.0092
  V_current[1]: 0.0340, out_val[1]: 0.0090
  V_current[2]: 0.0035, out_val[2]: -0.0030
  V_current[3]: -0.0301, out_val[3]: -0.0182
  V_current[4]: -0.0146, out_val[4]: -0.0025

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.2457, 0.1249, -0.2769, 0.3997, 0.1932
  Q magnitude: 4.6261 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.5681 [1]=-0.2480 [2]=-0.1271 [3]=-0.1322 [4]=-0.4291 
  Max scaled score: -0.1271
  Softmax sum: 4.263839 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1509 [1]=0.2078 [2]=0.2345 [3]=0.2333 [4]=0.1734 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.263839 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.4414, out_val[0]: 0.2386
  V_current[1]: 0.0546, out_val[1]: -0.0719
  V_current[2]: -0.0956, out_val[2]: 0.1634
  V_current[3]: -0.1733, out_val[3]: 0.0474
  V_current[4]: 0.2007, out_val[4]: 0.1074

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.2922, -1.6387, -0.3171, 0.1209, -0.0457
  Q magnitude: 6.2317 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.2869 [1]=1.3611 [2]=1.3810 [3]=0.1963 [4]=0.5667 
  Max scaled score: 1.3810
  Softmax sum: 2.917768 (should be ~1.0)
  Attention weights (should have 5): [0]=0.0647 [1]=0.3360 [2]=0.3427 [3]=0.1048 [4]=0.1518 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.917768 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.2566, out_val[0]: 0.2029
  V_current[1]: -0.1302, out_val[1]: -0.0155
  V_current[2]: 0.1691, out_val[2]: 0.0618
  V_current[3]: -0.0623, out_val[3]: -0.0603
  V_current[4]: 0.3396, out_val[4]: 0.3083

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.7339, 0.7197, -0.3333, -0.2600, 0.7095
  Q magnitude: 7.1607 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-1.1218 [1]=-1.4525 [2]=-2.0119 [3]=-2.0994 [4]=-2.1015 
  Max scaled score: -1.1218
  Softmax sum: 2.880658 (should be ~1.0)
  Attention weights (should have 5): [0]=0.3471 [1]=0.2494 [2]=0.1425 [3]=0.1306 [4]=0.1303 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.880658 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0980, out_val[0]: 0.2186
  V_current[1]: -0.6914, out_val[1]: -0.9279
  V_current[2]: 0.0090, out_val[2]: 0.0577
  V_current[3]: 0.6538, out_val[3]: 0.4512
  V_current[4]: 1.0996, out_val[4]: 1.2085

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.0158, 0.4792, -1.1143, -0.3923, -0.4807
  Q magnitude: 6.8161 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.2637 [1]=-0.3774 [2]=-0.6961 [3]=-0.5802 [4]=-0.6523 
  Max scaled score: 0.2637
  Softmax sum: 2.739930 (should be ~1.0)
  Attention weights (should have 5): [0]=0.3650 [1]=0.1922 [2]=0.1398 [3]=0.1570 [4]=0.1460 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.739930 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.4871, out_val[0]: -0.4800
  V_current[1]: 0.4326, out_val[1]: 0.1627
  V_current[2]: 0.0562, out_val[2]: -0.3223
  V_current[3]: -0.3611, out_val[3]: -0.4196
  V_current[4]: -0.8174, out_val[4]: -0.8199

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -1.0791, -0.8076, 1.3096, 1.0791, -1.6064
  Q magnitude: 6.3101 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.7860 [1]=-1.1300 [2]=-1.0555 [3]=-0.7085 [4]=-0.5756 
  Max scaled score: -0.5756
  Softmax sum: 3.879104 (should be ~1.0)
  Attention weights (should have 5): [0]=0.2089 [1]=0.1481 [2]=0.1595 [3]=0.2257 [4]=0.2578 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.879104 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.3757, out_val[0]: 0.4494
  V_current[1]: 0.4807, out_val[1]: 0.4169
  V_current[2]: 0.3533, out_val[2]: 0.4145
  V_current[3]: -0.5708, out_val[3]: -0.4585
  V_current[4]: 0.0483, out_val[4]: -0.1006

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.0009, -0.0105, 1.1699, 0.2216, -0.8408
  Q magnitude: 5.5061 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.1179 [1]=0.2505 [2]=0.0516 [3]=-0.0471 [4]=-0.2565 
  Max scaled score: 0.2505
  Softmax sum: 4.040472 (should be ~1.0)
  Attention weights (should have 5): [0]=0.2168 [1]=0.2475 [2]=0.2029 [3]=0.1838 [4]=0.1491 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.040472 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.3438, out_val[0]: -0.1270
  V_current[1]: 0.1410, out_val[1]: 0.0393
  V_current[2]: -0.3643, out_val[2]: -0.3425
  V_current[3]: 0.0199, out_val[3]: 0.0577
  V_current[4]: -0.1440, out_val[4]: -0.1434

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.9551, -1.4131, 0.2083, 0.7285, 0.8613
  Q magnitude: 6.3554 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.6880 [1]=-0.7805 [2]=-0.8509 [3]=-0.9743 [4]=-0.6081 
  Max scaled score: -0.6081
  Softmax sum: 4.242702 (should be ~1.0)
  Attention weights (should have 5): [0]=0.2176 [1]=0.1984 [2]=0.1849 [3]=0.1634 [4]=0.2357 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.242702 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.7544, out_val[0]: -0.7831
  V_current[1]: -0.6558, out_val[1]: -0.6312
  V_current[2]: -0.6245, out_val[2]: -0.4935
  V_current[3]: -0.4873, out_val[3]: -0.3338
  V_current[4]: -0.4736, out_val[4]: -0.4224

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.4517, 0.3855, 0.5547, -0.8989, -7.3828
  Q magnitude: 9.6138 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-2.3502 [1]=-1.4732 [2]=-0.7969 [3]=-0.2683 [4]=0.2735 
  Max scaled score: 0.2735
  Softmax sum: 2.171521 (should be ~1.0)
  Attention weights (should have 5): [0]=0.0334 [1]=0.0803 [2]=0.1579 [3]=0.2679 [4]=0.4605 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.171521 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.0483, out_val[0]: -0.0429
  V_current[1]: -0.6445, out_val[1]: -0.6536
  V_current[2]: 0.2827, out_val[2]: 0.2691
  V_current[3]: -0.0331, out_val[3]: -0.0796
  V_current[4]: 0.4946, out_val[4]: 0.5407

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.0804, 0.5649, -1.1650, -1.3486, 2.0410
  Q magnitude: 8.1527 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-1.5569 [1]=-1.0257 [2]=-0.8127 [3]=-0.9455 [4]=-1.2352 
  Max scaled score: -0.8127
  Softmax sum: 3.814366 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1246 [1]=0.2119 [2]=0.2622 [3]=0.2296 [4]=0.1718 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.814366 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.6440, out_val[0]: -0.6847
  V_current[1]: 1.6338, out_val[1]: 1.6507
  V_current[2]: -0.3865, out_val[2]: -0.4711
  V_current[3]: -0.8086, out_val[3]: -0.6457
  V_current[4]: -0.6382, out_val[4]: -0.5487

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -1.7539, 0.2487, 1.4033, -0.0522, -2.2285
  Q magnitude: 8.7347 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.8700 [1]=1.1081 [2]=1.3445 [3]=1.5427 [4]=1.7642 
  Max scaled score: 1.7642
  Softmax sum: 3.386335 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1208 [1]=0.1532 [2]=0.1941 [3]=0.2366 [4]=0.2953 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.386335 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.3889, out_val[0]: 0.6448
  V_current[1]: -0.4500, out_val[1]: -0.3781
  V_current[2]: -0.6216, out_val[2]: -0.6763
  V_current[3]: 0.3618, out_val[3]: 0.1879
  V_current[4]: 0.1925, out_val[4]: 0.1741

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 1.0332, -0.4363, -0.1477, 0.3516, 0.2847
  Q magnitude: 10.9004 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-1.7570 [1]=-1.2913 [2]=-1.2008 [3]=-1.0785 [4]=-1.0297 
  Max scaled score: -1.0297
  Softmax sum: 4.048157 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1194 [1]=0.1902 [2]=0.2082 [3]=0.2353 [4]=0.2470 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.048157 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.7979, out_val[0]: 0.7644
  V_current[1]: -0.1788, out_val[1]: -0.0808
  V_current[2]: 0.5991, out_val[2]: 0.5927
  V_current[3]: 0.6445, out_val[3]: 0.4563
  V_current[4]: 0.0593, out_val[4]: -0.0237

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.3018, 0.7827, -0.2148, -0.3550, 1.7695
  Q magnitude: 8.7578 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-3.8217 [1]=-4.0021 [2]=-3.9724 [3]=-3.5934 [4]=-3.4598 
  Max scaled score: -3.4598
  Softmax sum: 3.751475 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1856 [1]=0.1550 [2]=0.1596 [3]=0.2332 [4]=0.2666 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.751475 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.8804, out_val[0]: -0.8379
  V_current[1]: -0.5869, out_val[1]: -0.4496
  V_current[2]: 0.1462, out_val[2]: 0.1419
  V_current[3]: -0.4189, out_val[3]: -0.5383
  V_current[4]: -0.9927, out_val[4]: -1.0331

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 1.4238, 1.0527, 1.6074, 2.0605, -1.1611
  Q magnitude: 7.8200 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.5438 [1]=-0.1123 [2]=-0.5027 [3]=-0.5280 [4]=0.2821 
  Max scaled score: 0.5438
  Softmax sum: 2.982179 (should be ~1.0)
  Attention weights (should have 5): [0]=0.3353 [1]=0.1740 [2]=0.1178 [3]=0.1148 [4]=0.2581 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.982179 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.3076, out_val[0]: 1.2763
  V_current[1]: 1.5000, out_val[1]: 1.4137
  V_current[2]: -0.2052, out_val[2]: -0.4497
  V_current[3]: -0.4609, out_val[3]: -0.5360
  V_current[4]: 0.4294, out_val[4]: 0.4486

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.5693, 0.3909, 0.1625, 1.9834, -2.0391
  Q magnitude: 11.8345 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.7608 [1]=-0.1085 [2]=-0.6838 [3]=-0.7032 [4]=-0.2691 
  Max scaled score: 0.7608
  Softmax sum: 2.243387 (should be ~1.0)
  Attention weights (should have 5): [0]=0.4458 [1]=0.1869 [2]=0.1051 [3]=0.1031 [4]=0.1591 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.243387 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.9106, out_val[0]: -0.7211
  V_current[1]: -1.4600, out_val[1]: -1.1822
  V_current[2]: 0.4060, out_val[2]: 0.2531
  V_current[3]: 0.8491, out_val[3]: 0.6177
  V_current[4]: -1.0635, out_val[4]: -0.9749

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.7354, 0.0336, 1.4248, -0.6387, 0.2976
  Q magnitude: 6.8551 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=2.2248 [1]=2.3788 [2]=2.3655 [3]=2.3274 [4]=2.2195 
  Max scaled score: 2.3788
  Softmax sum: 4.646534 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1845 [1]=0.2152 [2]=0.2124 [3]=0.2044 [4]=0.1835 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.646534 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.7207, out_val[0]: 1.8574
  V_current[1]: -0.1646, out_val[1]: 0.0237
  V_current[2]: 1.4297, out_val[2]: 1.3767
  V_current[3]: 0.1870, out_val[3]: 0.3708
  V_current[4]: 1.2920, out_val[4]: 1.4271

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.3291, -0.3340, -0.6997, 0.1378, -3.1992
  Q magnitude: 8.6823 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=1.1939 [1]=1.7070 [2]=2.0458 [3]=2.1438 [4]=1.9516 
  Max scaled score: 2.1438
  Softmax sum: 3.764638 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1027 [1]=0.1716 [2]=0.2408 [3]=0.2656 [4]=0.2192 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.764638 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 2.5508, out_val[0]: 2.4838
  V_current[1]: 1.2734, out_val[1]: 1.3205
  V_current[2]: 1.7549, out_val[2]: 1.6264
  V_current[3]: 1.0342, out_val[3]: 1.1631
  V_current[4]: 0.0582, out_val[4]: 0.0827

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.0640, 0.1923, 0.3594, -1.4941, 0.8032
  Q magnitude: 8.8585 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.9531 [1]=-0.4290 [2]=0.2253 [3]=0.5409 [4]=0.7664 
  Max scaled score: 0.7664
  Softmax sum: 2.861947 (should be ~1.0)
  Attention weights (should have 5): [0]=0.0626 [1]=0.1057 [2]=0.2034 [3]=0.2789 [4]=0.3494 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.861947 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.2764, out_val[0]: -1.2132
  V_current[1]: -0.7036, out_val[1]: -0.6751
  V_current[2]: -0.7324, out_val[2]: -0.6408
  V_current[3]: 0.6724, out_val[3]: 0.6718
  V_current[4]: -0.5469, out_val[4]: -0.5711

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.8262, -0.3052, -0.6729, 0.7412, 0.1716
  Q magnitude: 9.4947 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-1.6496 [1]=-1.6470 [2]=-1.6513 [3]=-1.5339 [4]=-1.3275 
  Max scaled score: -1.3275
  Softmax sum: 3.988148 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1817 [1]=0.1822 [2]=0.1814 [3]=0.2040 [4]=0.2507 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.988148 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.2390, out_val[0]: -0.3647
  V_current[1]: 0.4827, out_val[1]: 0.4496
  V_current[2]: 0.4531, out_val[2]: 0.3130
  V_current[3]: -0.2314, out_val[3]: -0.3616
  V_current[4]: -0.2227, out_val[4]: -0.1503

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.2126, 0.5381, -0.0393, -0.8574, -0.1892
  Q magnitude: 6.8510 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.0916 [1]=0.2313 [2]=0.5165 [3]=0.7893 [4]=0.9411 
  Max scaled score: 0.9411
  Softmax sum: 3.361092 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1059 [1]=0.1463 [2]=0.1946 [3]=0.2556 [4]=0.2975 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.361092 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -0.0090, out_val[0]: -0.0337
  V_current[1]: 0.5718, out_val[1]: 0.4225
  V_current[2]: 0.1973, out_val[2]: 0.2388
  V_current[3]: -0.7046, out_val[3]: -0.7410
  V_current[4]: 0.0676, out_val[4]: 0.0560

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.7446, 1.2383, -1.1416, 1.1709, -1.6045
  Q magnitude: 9.4509 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.1604 [1]=0.4683 [2]=0.2891 [3]=-0.1106 [4]=-0.3255 
  Max scaled score: 0.4683
  Softmax sum: 3.583526 (should be ~1.0)
  Attention weights (should have 5): [0]=0.2051 [1]=0.2791 [2]=0.2333 [3]=0.1564 [4]=0.1262 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 3.583526 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 0.5103, out_val[0]: 0.5425
  V_current[1]: 2.9219, out_val[1]: 3.1180
  V_current[2]: 1.0332, out_val[2]: 1.0997
  V_current[3]: -0.9429, out_val[3]: -0.7857
  V_current[4]: -0.1840, out_val[4]: -0.1976

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -1.4951, 1.9229, -0.6606, -2.1387, 0.0995
  Q magnitude: 8.8386 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=1.0405 [1]=0.8552 [2]=0.4283 [3]=-0.1700 [4]=-0.6094 
  Max scaled score: 1.0405
  Softmax sum: 2.863134 (should be ~1.0)
  Attention weights (should have 5): [0]=0.3493 [1]=0.2902 [2]=0.1894 [3]=0.1041 [4]=0.0671 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 2.863134 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 3.2285, out_val[0]: 2.4429
  V_current[1]: -0.2227, out_val[1]: 0.0833
  V_current[2]: 5.8047, out_val[2]: 6.2029
  V_current[3]: -2.9805, out_val[3]: -2.2581
  V_current[4]: -0.7866, out_val[4]: -0.6983

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: -0.1752, 0.1086, -0.6113, -1.1035, -0.6260
  Q magnitude: 6.7159 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=-0.1559 [1]=-0.0075 [2]=0.1190 [3]=0.1541 [4]=0.1593 
  Max scaled score: 0.1593
  Softmax sum: 4.531502 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1610 [1]=0.1868 [2]=0.2120 [3]=0.2195 [4]=0.2207 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.531502 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: -1.6094, out_val[0]: -1.8702
  V_current[1]: 4.4141, out_val[1]: 4.4852
  V_current[2]: -5.1875, out_val[2]: -5.1338
  V_current[3]: 0.8530, out_val[3]: 0.7494
  V_current[4]: 0.5464, out_val[4]: 0.5545

[ATTENTION DEBUG] cache_len=4, q_head=0, kv_head=0
  Q[0:5]: 0.9033, 0.4004, -0.1909, -0.5098, 1.2148
  Q magnitude: 8.3937 (norm of 64-dim vector)
  DEBUG: cache_len=4, should have 5 scores
  Scaled scores (after scale): [0]=0.6319 [1]=0.8689 [2]=0.8585 [3]=0.6047 [4]=0.3012 
  Max scaled score: 0.8689
  Softmax sum: 4.113344 (should be ~1.0)
  Attention weights (should have 5): [0]=0.1918 [1]=0.2431 [2]=0.2406 [3]=0.1867 [4]=0.1378 
  Weight sum: 1.000000 (should be ~1.0)

[PEER_REVIEW] === TEST 3: SOFTMAX VERIFICATION ===
[PEER_REVIEW] Softmax Statistics:
  Sum before norm: 4.113344 (Team Alpha reported: ~1.97)
  Sum after norm:  1.000000 (should be 1.0)

[PEER_REVIEW] Checks:
  Weight sum ≈ 1.0: ✅ PASS (diff=0.000000)

[PEER_REVIEW] Test 3 Result: ✅ TEST PASSED
[PEER_REVIEW] Team Alpha Claim: VERIFIED ✅

  V_current[0]: 1.4893, out_val[0]: 1.1082
  V_current[1]: 0.4377, out_val[1]: 0.6108
  V_current[2]: 1.4922, out_val[2]: 1.6154
  V_current[3]: 1.2285, out_val[3]: 1.0385
  V_current[4]: -0.1318, out_val[4]: -0.1959
🔍 [ARGMAX DEBUG #4] First 10 logits: 3.15 -1.49 3.83 -4.32 2.87 -1.90 -1.58 1.35 1.52 1.41 
🔍 [ARGMAX DEBUG #4] Max: 14.28 at token_id=137131 (vocab_size=151936)
🔍 [ARGMAX DEBUG #5] First 10 logits: 3.23 -1.48 3.84 -4.28 2.84 -2.10 -1.62 1.45 1.75 1.31 
🔍 [ARGMAX DEBUG #5] Max: 14.40 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #6] First 10 logits: 3.25 -1.48 3.86 -4.33 2.84 -2.20 -1.66 1.47 1.84 1.28 
🔍 [ARGMAX DEBUG #6] Max: 14.59 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #7] First 10 logits: 3.16 -1.52 3.85 -4.29 2.85 -2.33 -1.75 1.47 1.89 1.31 
🔍 [ARGMAX DEBUG #7] Max: 14.79 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #8] First 10 logits: 2.98 -1.45 3.77 -4.23 2.81 -2.43 -1.81 1.40 2.01 1.42 
🔍 [ARGMAX DEBUG #8] Max: 14.96 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #9] First 10 logits: 2.99 -1.42 3.75 -4.24 2.80 -2.48 -1.90 1.36 2.07 1.44 
🔍 [ARGMAX DEBUG #9] Max: 15.07 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #10] First 10 logits: 2.99 -1.40 3.67 -4.29 2.79 -2.48 -1.95 1.29 2.15 1.48 
🔍 [ARGMAX DEBUG #10] Max: 15.19 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #11] First 10 logits: 3.04 -1.44 3.66 -4.26 2.79 -2.52 -2.07 1.36 2.20 1.52 
🔍 [ARGMAX DEBUG #11] Max: 15.40 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #12] First 10 logits: 3.07 -1.51 3.58 -4.26 2.77 -2.55 -2.07 1.34 2.21 1.60 
🔍 [ARGMAX DEBUG #12] Max: 15.45 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #13] First 10 logits: 3.07 -1.53 3.50 -4.31 2.71 -2.58 -2.10 1.29 2.20 1.64 
🔍 [ARGMAX DEBUG #13] Max: 15.44 at token_id=44394 (vocab_size=151936)
🔍 [ARGMAX DEBUG #14] First 10 logits: 3.06 -1.56 3.42 -4.31 2.71 -2.63 -2.11 1.30 2.23 1.66 
🔍 [ARGMAX DEBUG #14] Max: 15.49 at token_id=44394 (vocab_size=151936)
{"timestamp":"2025-10-06T16:14:05.600887Z","level":"INFO","fields":{"message":"✅ Prefill complete, starting generation from token ID=8"}}

🎨 GENERATING 100 TOKENS...
.....

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ Generated 100 tokens

📊 DEBUG SUMMARY (First 10 tokens):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  [0] ID= 44394 → "coholic"
  [1] ID= 44394 → "coholic"
  [2] ID= 44394 → "coholic"
  [3] ID= 44394 → "coholic"
  [4] ID= 44394 → "coholic"
  [5] ID= 44394 → "coholic"
  [6] ID= 44394 → "coholic"
  [7] ID= 44394 → "coholic"
  [8] ID= 44394 → "coholic"
  [9] ID= 44394 → "coholic"

⚠️  WARNING: All tokens are identical (ID=44394)
⚠️  This indicates a broken attention mechanism!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

{"timestamp":"2025-10-06T16:14:06.709102Z","level":"INFO","fields":{"message":"Inference complete","job_id":"m0-haiku-anti-cheat-d2d5e8d0-d82c-4168-9bd2-627b0da220e1","tokens":100}}
✅ Got response with status: 200 OK
❌ QUALITY CHECK FAILED: Minute word 'fourteen' not found in output (found 0 times)
📊 Status: Pipeline ✅ | Matrix Layout ✅ | KV Cache ✅ | Attention ✅ | Bias ❌
🔍 Current Issue: Bias values contain outliers (-14, -34) - under investigation

🎨 M0 Haiku Anti-Cheat Test PASSED
Minute: 14 ("fourteen")
Nonce: XGU0qDs0
Tokens: 100
Time: 2.468122653s

Haiku:
coholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholiccoholicstraÃŁecoholicstraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁestraÃŁe

Artifacts: .test-results/haiku/d2d5e8d0-d82c-4168-9bd2-627b0da220e1
ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 5 filtered out; finished in 6.76s

