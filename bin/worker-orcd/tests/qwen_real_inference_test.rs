// Qwen Tokenizer Integration Test - GT-057
//
// Tests the complete tokenizer pipeline:
// - Loading from GGUF
// - Encoding text to tokens
// - Decoding tokens to text
// - Roundtrip verification
//
// Spec: M0-W-1420

use worker_gguf::GGUFMetadata;
use worker_tokenizer::Tokenizer;

#[test]
#[ignore] // Requires actual GGUF model file
fn test_qwen_tokenizer_from_gguf() {
    println!("\n{}", "=".repeat(60));
    println!("üß™ Qwen Tokenizer Integration Test");
    println!("{}\n", "=".repeat(60));

    // Model path - update this to your actual model location
    let model_path =
        "/home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-q4_k_m.gguf";

    // 1. Verify GGUF metadata extraction
    println!("üìÇ Step 1: Extracting tokenizer metadata from GGUF...");
    let metadata = GGUFMetadata::from_file(model_path).unwrap();

    let tokens = metadata.tokenizer_tokens().unwrap();
    let merges = metadata.tokenizer_merges().unwrap();
    let bos = metadata.bos_token_id().unwrap();
    let eos = metadata.eos_token_id().unwrap();

    println!("   ‚úÖ Extracted {} tokens", tokens.len());
    println!("   ‚úÖ Extracted {} merge rules", merges.len());
    println!("   ‚úÖ BOS token: {}", bos);
    println!("   ‚úÖ EOS token: {}", eos);

    assert_eq!(tokens.len(), 151936, "Should have 151,936 tokens");
    assert_eq!(bos, 151643, "BOS should be 151643");
    assert_eq!(eos, 151643, "EOS should be 151643");

    // 2. Load tokenizer from GGUF
    println!("\nüìö Step 2: Loading tokenizer from GGUF...");
    let tokenizer = Tokenizer::from_gguf(model_path).unwrap();
    println!("   ‚úÖ Tokenizer loaded successfully!");

    // 3. Test encoding
    let test_cases = vec![
        "Write a haiku about mountains",
        "Hello, world!",
        "The quick brown fox jumps over the lazy dog",
    ];

    println!("\nüìù Step 3: Testing encoding...");
    for (i, prompt) in test_cases.iter().enumerate() {
        println!("   Test {}: '{}'", i + 1, prompt);
        let tokens = tokenizer.encode(prompt, true).unwrap();
        println!("      ‚Üí {} tokens: {:?}...", tokens.len(), &tokens[..tokens.len().min(5)]);

        // Verify BOS token
        assert_eq!(tokens[0], 151643, "First token should be BOS");
        assert!(tokens.len() > 1, "Should have more than just BOS");
    }
    println!("   ‚úÖ All encoding tests passed!");

    // 4. Test decoding
    println!("\nüîÑ Step 4: Testing decoding...");
    for (i, prompt) in test_cases.iter().enumerate() {
        let tokens = tokenizer.encode(prompt, true).unwrap();
        let decoded = tokenizer.decode(&tokens[1..], false).unwrap();
        println!("   Test {}: '{}'", i + 1, prompt);
        println!("      ‚Üí Decoded: '{}'", decoded);

        // Verify key words are preserved (BPE may add/remove spaces)
        let original_words: Vec<&str> = prompt.split_whitespace().collect();
        for word in original_words {
            let word_lower =
                word.to_lowercase().trim_matches(|c: char| !c.is_alphanumeric()).to_string();
            if word_lower.len() > 2 {
                // Skip very short words
                assert!(
                    decoded.to_lowercase().contains(&word_lower),
                    "Decoded text should contain '{}' from original",
                    word_lower
                );
            }
        }
    }
    println!("   ‚úÖ All decoding tests passed!");

    // 5. Test special tokens
    println!("\nüéØ Step 5: Testing special token handling...");
    let with_bos = tokenizer.encode("test", true).unwrap();
    let without_bos = tokenizer.encode("test", false).unwrap();

    println!("   With BOS: {:?}", &with_bos[..with_bos.len().min(3)]);
    println!("   Without BOS: {:?}", &without_bos[..without_bos.len().min(3)]);

    assert_eq!(with_bos[0], 151643, "Should start with BOS");
    assert_ne!(without_bos[0], 151643, "Should not start with BOS");
    assert_eq!(with_bos.len(), without_bos.len() + 1, "BOS adds one token");
    println!("   ‚úÖ Special token handling works!");

    // 6. Summary
    println!("\n{}", "=".repeat(60));
    println!("üéâ ALL TESTS PASSED!");
    println!("{}", "=".repeat(60));
    println!("\n‚úÖ Tokenizer Implementation Complete:");
    println!("   ‚úÖ GGUF metadata extraction");
    println!("   ‚úÖ Vocabulary loading (151,936 tokens)");
    println!("   ‚úÖ BPE merge table loading");
    println!("   ‚úÖ Text encoding");
    println!("   ‚úÖ Token decoding");
    println!("   ‚úÖ Special token handling");
    println!("   ‚úÖ Roundtrip verification");
    println!("\nüöÄ Ready for inference integration!");
    println!();
}

/// ‚ö†Ô∏è  DOCUMENTATION TEST - NOT A FUNCTIONAL TEST
///
/// This test exists ONLY to document that the tokenizer implementation exists.
/// It does NOT validate tokenizer functionality.
///
/// **What this test validates**:
/// - ‚úÖ Test file compiles
/// - ‚úÖ Dependencies are correct
///
/// **What this test DOES NOT validate**:
/// - ‚ùå Tokenizer loads from GGUF
/// - ‚ùå Encoding works
/// - ‚ùå Decoding works
/// - ‚ùå Special tokens work
///
/// **To run REAL tokenizer test**:
/// ```bash
/// cargo test -p worker-tokenizer test_tokenizer_from_gguf_full -- --ignored --nocapture
/// ```
///
/// **Status**: Real implementation exists in worker-tokenizer/src/backend.rs::from_gguf()
#[test]
fn test_qwen_tokenizer_documentation() {
    println!("\n‚ö†Ô∏è  DOCUMENTATION TEST - This is NOT a functional test");
    println!("‚ö†Ô∏è  This test only documents that tokenizer code exists");
    println!("\n‚úÖ Tokenizer implementation location:");
    println!("   worker-tokenizer/src/backend.rs::from_gguf()");
    println!("\nüìù To run REAL tokenizer test:");
    println!(
        "   cargo test -p worker-tokenizer test_tokenizer_from_gguf_full -- --ignored --nocapture"
    );
}

/// ‚ö†Ô∏è  DOCUMENTATION TEST - NOT A FUNCTIONAL TEST
///
/// This test exists ONLY to document that inference requires CUDA.
/// It does NOT validate any inference functionality.
///
/// **What this test validates**:
/// - ‚úÖ Test file compiles
///
/// **What this test DOES NOT validate**:
/// - ‚ùå CUDA inference works
/// - ‚ùå Model loading works
/// - ‚ùå Token generation works
///
/// **Status**: Inference implementation is complete but requires C++ build fixes
#[test]
fn test_qwen_inference_documentation() {
    println!("\n‚ö†Ô∏è  DOCUMENTATION TEST - This is NOT a functional test");
    println!("‚ö†Ô∏è  This test only documents CUDA requirements");
    println!("\nüìù Inference requires:");
    println!("   - CUDA-enabled GPU");
    println!("   - GGUF model file");
    println!("   - C++ build system fixes (duplicate symbols)");
}

// ---
// Crafted by GPT-Gamma ü§ñ
