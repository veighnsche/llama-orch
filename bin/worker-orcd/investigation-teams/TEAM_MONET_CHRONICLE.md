# üé® TEAM MONET - Code Audit Chronicle

**Round:** 2  
**Specialization:** Current State Verification  
**Mission:** Audit current codebase to determine which fixes are actually applied  
**Status:** ‚úÖ AUDIT COMPLETE

---

## üë• Team Introduction

**Team Name:** MONET (after Claude Monet, master of capturing light and detail)

**Why This Name:**
Just as Monet captured fleeting moments of light with precision, TEAM MONET captures the current state of the codebase with meticulous detail. We observe what IS, not what should be.

**Team Philosophy:**
*"We paint the truth of the code as it exists today."*

**Specialization:**
We are the auditors. Before any team can resolve contradictions or validate fixes, they need to know the CURRENT STATE. That's our job. We read the code, document every parameter, every value, every line number. We are the foundation upon which all other teams build.

---

## üìã Mission Briefing

**Objective:** Create a definitive report of current code state

**Why This Matters:**
Round 1 ended with multiple contradictions:
- FELICIA/AURORA said CUBLAS_OP_T is wrong (reverted)
- SENTINEL said CUBLAS_OP_T is correct (applied)
- Who won? What's in the code NOW?

We need to know the TRUTH before we can proceed.

**Dependencies:**
- None - We start first

**Teams Depending On Us:**
- TEAM PICASSO (needs cuBLAS state)
- TEAM VAN GOGH (needs weight state)
- TEAM SHAKESPEARE (needs all fixes confirmed)
- TEAM FROST (needs sampling state)
- TEAM DICKINSON (needs all fixes confirmed)

---

## üìù Investigation Log

### Session 1: 2025-10-07T14:20Z

**Investigator:** TEAM MONET (Cascade AI)

**What I'm investigating:**
- [x] cuBLAS parameters (8 matmuls)
- [x] Softmax implementation
- [x] Sampling order
- [x] Output norm weights
- [x] Q/K/V biases
- [x] Configuration overrides

**Findings:**

```
1. cuBLAS Parameters (8 matmuls):
   - Q projection: Line 873, CUBLAS_OP_T, lda=config_.hidden_dim (896)
   - K projection: Line 966, CUBLAS_OP_T, lda=config_.hidden_dim (896)
   - V projection: Line 992, CUBLAS_OP_T, lda=config_.hidden_dim (896)
   - Attn out: Line 1644, CUBLAS_OP_T, lda=q_dim
   - FFN gate: swiglu_ffn.cu line 239-253, CUBLAS_OP_T, lda=hidden_dim
   - FFN up: swiglu_ffn.cu line 281-295, CUBLAS_OP_T, lda=hidden_dim
   - FFN down: swiglu_ffn.cu line 350-364, CUBLAS_OP_T, lda=ffn_dim
   - lm_head: Line 2184-2197, CUBLAS_OP_T, lda=config_.hidden_dim (896)
   ALL 8 matmuls use CUBLAS_OP_T (SENTINEL's fix applied)

2. Softmax Implementation:
   - File: cuda/kernels/sampling_wrapper.cu
   - Lines 79-119: softmax_kernel
   - Line 99: Uses DOUBLE precision for sum ("double sum = 0.0;")
   - Line 106: Casts to double before adding ("sum += (double)prob;")
   - Line 115: Double division ("(double)probs[i] / sum")
   - CASCADE's fix APPLIED ‚úÖ

3. Sampling Order:
   - File: cuda/kernels/sampling_wrapper.cu
   - Lines 297-532: cuda_sample_token function
   - Order: temperature scale (line 372) ‚Üí top-k (line 377) ‚Üí softmax (line 414) ‚Üí top-p DISABLED (line 468) ‚Üí sample (line 478)
   - Top-P is INTENTIONALLY DISABLED (lines 444-475)
   - HELIOS's fix PARTIALLY APPLIED (softmax before top-p, but top-p disabled)

4. Output Norm Weights:
   - File: cuda/src/model/qwen_weight_loader.cpp
   - Line 320: model->weights.output_norm = get_ptr("output_norm.weight");
   - Line 393: model->weights.output_norm = get_ptr("output_norm.weight");
   - Weights are loaded directly from GPU pointers (no normalization applied)
   - Status: RAW weights used (not normalized)

5. Q/K/V Biases:
   - File: cuda/src/model/qwen_weight_loader.cpp
   - Lines 370, 372, 374: Biases loaded via get_ptr()
   - File: cuda/src/transformer/qwen_transformer.cpp
   - Lines 937-960 (Q bias), 972-987 (K bias), 997-1011 (V bias)
   - Biases ARE loaded and added after projections
   - GREEN's fix APPLIED ‚úÖ

6. Configuration Overrides:
   - File: src/inference/cuda_backend.rs
   - Line 675: Uses config.temperature (not hardcoded)
   - Line 234: use_chat_template = false (hardcoded override)
   - FINNEY's temperature fix APPLIED ‚úÖ
   - Chat template DISABLED (hardcoded override present)
```

**Questions/Blockers:**

```
None - all files accessible and readable
```

**Next Steps:**

```
Fill in detailed findings table and create summary report
```

---

### Session 2: [Date/Time]

**Investigator:** [Your name/handle]

**What I'm investigating:**

**Findings:**

**Questions/Blockers:**

**Next Steps:**

---

## üîç Detailed Findings

### 1. cuBLAS Parameters Audit

| Matmul | File | Line | Operation | lda | Last Modified By |
|--------|------|------|-----------|-----|------------------|
| Q proj | cuda/src/transformer/qwen_transformer.cpp | 873 | CUBLAS_OP_T | config_.hidden_dim (896) | TEAM SENTINEL |
| K proj | cuda/src/transformer/qwen_transformer.cpp | 966 | CUBLAS_OP_T | config_.hidden_dim (896) | TEAM SENTINEL |
| V proj | cuda/src/transformer/qwen_transformer.cpp | 992 | CUBLAS_OP_T | config_.hidden_dim (896) | TEAM SENTINEL |
| Attn out | cuda/src/transformer/qwen_transformer.cpp | 1644 | CUBLAS_OP_T | q_dim | TEAM SENTINEL |
| FFN gate | cuda/kernels/swiglu_ffn.cu | 239 | CUBLAS_OP_T | hidden_dim | TEAM SENTINEL |
| FFN up | cuda/kernels/swiglu_ffn.cu | 281 | CUBLAS_OP_T | hidden_dim | TEAM SENTINEL |
| FFN down | cuda/kernels/swiglu_ffn.cu | 350 | CUBLAS_OP_T | ffn_dim | TEAM SENTINEL |
| lm_head | cuda/src/transformer/qwen_transformer.cpp | 2186 | CUBLAS_OP_T | config_.hidden_dim (896) | TEAM SENTINEL |

**Summary:**
- All using CUBLAS_OP_T? ‚úÖ YES (8/8)
- All using CUBLAS_OP_N? ‚ùå NO (0/8)
- Mixed? ‚ùå NO - All consistent

### 2. Softmax Implementation

**File:** `cuda/kernels/sampling_wrapper.cu` (not sampling.cu)

**Findings:**
- [x] Line number: 79-119 (softmax_kernel function)
- [x] Uses double precision for sum? ‚úÖ YES
- [x] CASCADE's fix applied? ‚úÖ YES

**Code snippet:**
```cpp
// Line 99: CRITICAL - Must be double, not float!
double sum = 0.0;
for (int i = 0; i < vocab_size; i++) {
    if (isinf(logits[i]) && logits[i] < 0) {
        probs[i] = 0.0f;
    } else {
        float prob = expf(logits[i] - max_logit);
        probs[i] = prob;
        sum += (double)prob;  // Line 106: Cast to double before adding!
    }
}

// Line 113-116: Normalize in double precision
if (sum > 0.0) {
    for (int i = 0; i < vocab_size; i++) {
        probs[i] = (float)((double)probs[i] / sum);  // Line 115: Double division!
    }
}
```

### 3. Sampling Order

**File:** `cuda/kernels/sampling_wrapper.cu`

**Current order:**
1. Temperature scaling (line 372) - on logits
2. Top-K filtering (line 377) - on logits
3. Softmax (line 414) - converts logits to probabilities
4. Top-P filtering (line 468) - INTENTIONALLY DISABLED
5. Sample from distribution (line 478)

**Analysis:**
- Top-P before softmax? ‚ùå NO (correct order)
- Top-P after softmax? ‚ö†Ô∏è YES but DISABLED
- HELIOS's fix applied? ‚ö†Ô∏è PARTIALLY (order correct, but top-p disabled)

**Note:** Lines 444-475 contain extensive comments explaining why top-p is disabled:
- Previous implementation had bugs (operated on logits, broken normalization)
- Currently ignored even if top_p < 1.0 is requested
- TODO for future team to reimplement correctly

### 4. Output Norm Weights

**File:** `cuda/src/model/qwen_weight_loader.cpp`

**Findings:**
- [x] Line number: 320 (load function), 393 (load_from_gpu_pointers)
- [x] Weights normalized? ‚ùå NO
- [x] Raw weights used? ‚úÖ YES
- [x] Output Norm Team's fix applied? ‚ùå NO FIX FOUND

**Code snippet:**
```cpp
// Line 320 (load function):
model->weights.output_norm = load_tensor_to_vram(path, "output_norm.weight", tracker);

// Line 393 (load_from_gpu_pointers):
model->weights.output_norm = get_ptr("output_norm.weight");
```

**Analysis:**
Weights are loaded directly from GGUF file or GPU pointers without any normalization.
No evidence of "Output Norm Team" fix in the codebase. Weights are used as-is.

### 5. Q/K/V Biases

**Files:**
- `cuda/src/model/qwen_weight_loader.cpp` (loading)
- `cuda/src/transformer/qwen_transformer.cpp` (addition)

**Findings:**
- [x] Biases loaded (not nullptr)? ‚úÖ YES
- [x] Biases added after projections? ‚úÖ YES
- [x] GREEN's fix applied? ‚úÖ YES

**Code snippets:**
```cpp
// qwen_weight_loader.cpp lines 370-374 (load_from_gpu_pointers):
layer.attn_q_bias = get_ptr(prefix + "attn_q.bias");
layer.attn_k_bias = get_ptr(prefix + "attn_k.bias");
layer.attn_v_bias = get_ptr(prefix + "attn_v.bias");

// qwen_transformer.cpp lines 937-960 (Q bias addition):
if (layer.attn_q_bias != nullptr) {
    cuda_add_bias(q_proj_, layer.attn_q_bias, 1, batch_size, q_dim, nullptr);
}

// Similar code for K bias (lines 972-987) and V bias (lines 997-1011)
```

**Comment from code (line 366-369):**
```cpp
// [TEAM GREEN] 2025-10-06T20:43Z - BUG FOUND!
// SUSPECT: We were setting biases to nullptr, but the model HAS biases!
// OBSERVED: Test output shows "blk.0.attn_q.bias -> 0x7dfbaa5c1200"
// FIXED: Load the biases from GPU pointers instead of nullptr
```

### 6. Configuration Overrides

**File:** `src/inference/cuda_backend.rs`

**Findings:**
- [x] Hardcoded temperature removed? ‚úÖ YES
- [x] Hardcoded system prompt removed? ‚ö†Ô∏è YES but chat template disabled
- [x] FINNEY's fix applied? ‚úÖ YES (temperature), ‚ö†Ô∏è PARTIAL (chat template)

**Code snippet:**
```rust
// Line 675: Uses config.temperature (not hardcoded)
let next_token_id = inference.generate_token(
    current_token,
    config.temperature, // Use configured temperature, not hardcoded 0.0!
    config.top_k,
    config.top_p,
    config.seed.wrapping_add(token_idx as u64),
);

// Line 234: Chat template disabled (hardcoded override)
let use_chat_template = false;  // Set to false to bypass special token crash
```

**Comment from code (lines 668-672):**
```rust
// CONTRADICTION: [TEAM_FINNEY] Hardcoded temperature=0.0 ignores config!
//   Test sets temperature=0.7 (haiku_generation_anti_cheat.rs:125)
//   But we override to 0.0 here ‚Üí greedy sampling always picks same token
//   llama.cpp uses temperature=0.7 and generates diverse output
// FIXED: [TEAM_FINNEY] Use config.temperature instead of hardcoded 0.0
```

---

## üìä Summary Report

### Fixes Applied: 4/6

- [x] cuBLAS parameters (CUBLAS_OP_T) - ‚úÖ APPLIED (all 8 matmuls)
- [x] Softmax (double precision) - ‚úÖ APPLIED (CASCADE fix)
- [‚ö†Ô∏è] Sampling order (Top-P after softmax) - ‚ö†Ô∏è PARTIALLY APPLIED (order correct but top-p disabled)
- [‚ùå] Output norm weights (normalized) - ‚ùå NOT APPLIED (raw weights used)
- [x] Q/K/V biases (loaded and added) - ‚úÖ APPLIED (GREEN fix)
- [x] Config overrides (removed) - ‚úÖ APPLIED (temperature fix), ‚ö†Ô∏è PARTIAL (chat template disabled)

### Critical Issues Found

```
1. Output norm weights: No normalization applied
   - Weights loaded directly from GGUF without processing
   - No evidence of "Output Norm Team" fix in codebase
   - Location: qwen_weight_loader.cpp lines 320, 393

2. Top-P sampling: Intentionally disabled
   - Order is correct (after softmax) but functionality disabled
   - Lines 444-475 in sampling_wrapper.cu explain why
   - Previous implementation had bugs, awaiting reimplementation

3. Chat template: Hardcoded override
   - use_chat_template = false on line 234 of cuda_backend.rs
   - Bypasses special token handling
   - Comment says "to bypass special token crash"
```

### Conflicts Detected

```
No conflicts detected. All cuBLAS operations consistently use CUBLAS_OP_T.
No contradictory parameter settings found across the 8 matmuls.
```

---

## üéØ Final Verdict

**Current Code State:**
- cuBLAS: All 8 matmuls use CUBLAS_OP_T with correct lda values (SENTINEL fix applied)
- Softmax: Uses double precision for sum accumulation (CASCADE fix applied)
- Sampling: Correct order (temp ‚Üí top-k ‚Üí softmax ‚Üí top-p ‚Üí sample) but top-p disabled
- Biases: Q/K/V biases loaded and added after projections (GREEN fix applied)
- Temperature: Uses config.temperature, not hardcoded (FINNEY fix applied)
- Output norm: Raw weights used, no normalization applied
- Chat template: Disabled via hardcoded flag

**Recommendation:**
- **TEAM PICASSO**: cuBLAS parameters are correct and consistent. No further investigation needed.
- **TEAM VAN GOGH**: Output norm weights are NOT normalized. Investigate if this is intentional.
- **TEAM SHAKESPEARE**: 4/6 fixes fully applied, 2/6 partial. Focus on output norm and top-p.
- **TEAM FROST**: Sampling order is correct but top-p is disabled. May need reimplementation.
- **TEAM DICKINSON**: Chat template disabled - special tokens not being used in tests.

**Immediate Concerns:**
1. Output norm weights may need normalization (no fix found)
2. Top-P sampling disabled - tests using top_p < 1.0 won't work as expected
3. Chat template disabled - may affect model behavior vs llama.cpp

---

## üì¶ Deliverable

**Status:** ‚úÖ COMPLETE

**File:** `investigation-teams/TEAM_MONET_CODE_AUDIT.md`

**Handoff To:**
- TEAM PICASSO (cuBLAS state documented)
- TEAM VAN GOGH (weight state documented)
- TEAM SHAKESPEARE (all fixes status documented)
- TEAM FROST (sampling state documented)
- TEAM DICKINSON (all fixes status documented)

---

## üí≠ Reflections

**What Went Well:**

**What Was Challenging:**

**Lessons Learned:**

**Advice for Future Teams:**

---

**TEAM MONET**  
*"We paint the truth of the code as it exists today."*

**Chronicle Status:** ‚úÖ COMPLETE  
**Last Updated:** 2025-10-07T14:22Z
