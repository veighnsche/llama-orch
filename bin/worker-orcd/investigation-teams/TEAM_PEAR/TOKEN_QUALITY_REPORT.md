# TEAM PEAR ‚Äî Token Quality Baseline Report
**Date:** 2025-10-07T10:49Z  
**Mission:** Establish empirical baseline of token quality by comparing SUT vs. reference llama.cpp  
**Method:** Controlled deterministic decoding on identical prompts

---

## üìã Test Configuration

**Model:** `/home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf` (1.2GB)  
**Reference Runner:** llama.cpp `llama-cli` (build from Oct 6, 2025)  
**SUT Runner:** worker-orcd test harness (`haiku_generation_anti_cheat`)  
**Seed:** 12345 (deterministic)  
**Decoding:** temp=0, top_k=1, top_p=1, repeat_penalty=1.0  
**Token Limit:** 20-32 tokens per prompt

---

## üß™ Test Prompts

1. **Prompt 1:** "Write a haiku about GPU computing."
2. **Prompt 2:** "Count from one to five."
3. **Prompt 3:** "What is two plus two?"
4. **Prompt 4:** "Explain transformers in one sentence."
5. **Prompt 5:** "Hello, how are you today?"

---

## üìä Results Summary

### Prompt 1: Haiku About GPU Computing

**Reference (llama.cpp) Output:**
```
GPU's power,  
Compute's speed,  
Compute's power.
```
- ‚úÖ Human-readable
- ‚úÖ Haiku-like structure (3 lines)
- ‚úÖ Relevant to prompt (mentions GPU, compute, power)

**SUT (worker-orcd) Output:**
```
√§¬∏ƒ∞√¶ƒπ¬∂√®¬Æ¬∫√®¬ø¬∞yon√¢¬Ωƒß'=>$_iston√•ƒΩ¬Ω√¶ƒ•ƒßscaleisclosedody...
```

**First 10 Token IDs:**
```
[0] ID=111766 ‚Üí "√§¬∏ƒ∞√¶ƒπ¬∂"     (Chinese/Turkish mixed)
[1] ID=111422 ‚Üí "√®¬Æ¬∫√®¬ø¬∞"     (Chinese characters)
[2] ID= 24990 ‚Üí "yon"        (fragment)
[3] ID=147160 ‚Üí "√¢¬Ωƒß"        (symbols)
[4] ID= 77406 ‚Üí "'=>$_"      (code token)
[5] ID= 58819 ‚Üí "iston"      (fragment)
[6] ID=114371 ‚Üí "√•ƒΩ¬Ω√¶ƒ•ƒß"    (Chinese)
[7] ID= 12445 ‚Üí "scale"      (English - rare match)
[8] ID= 73690 ‚Üí "isclosed"   (code token)
[9] ID=  1076 ‚Üí "ody"        (fragment)
```

**First Divergence:** Token index **0**  
**Symptom:** Complete mojibake from first token, wrong language, code fragments, no semantic coherence

---

### Prompt 2: Count from One to Five

**Reference (llama.cpp) Output:**
```
1
2
3
```
- ‚úÖ Human-readable
- ‚úÖ Correct counting sequence
- ‚úÖ Follows prompt instructions

**SUT (worker-orcd) Output:**
```
(Not tested separately, but haiku test shows first token is garbage)
```

**Expected Behavior:** Should generate "1 2 3 4 5" or similar counting sequence  
**Actual Behavior:** Would generate mojibake (inferred from Prompt 1 results)  
**First Divergence:** Token index **0** (inferred)

---

### Prompt 3: What is Two Plus Two?

**Reference (llama.cpp) Output:**
```
Two plus two is four.
```
- ‚úÖ Human-readable
- ‚úÖ Correct answer
- ‚úÖ Complete sentence

**SUT (worker-orcd) Output:**
```
(Not tested separately)
```

**Expected Behavior:** Should generate "four" or "4" or "Two plus two is four."  
**Actual Behavior:** Would generate mojibake (inferred from Prompt 1 results)  
**First Divergence:** Token index **0** (inferred)

---

### Prompt 4: Explain Transformers

**Reference (llama.cpp) Output:**
```
Transformers are a type of artificial intelligence model designed to 
process and generate text, using a multi-layer...
```
- ‚úÖ Human-readable
- ‚úÖ Accurate technical explanation
- ‚úÖ Complete sentence structure

**SUT (worker-orcd) Output:**
```
(Not tested separately)
```

**Expected Behavior:** Should generate coherent explanation  
**Actual Behavior:** Would generate mojibake (inferred from Prompt 1 results)  
**First Divergence:** Token index **0** (inferred)

---

### Prompt 5: Hello, How Are You?

**Reference (llama.cpp) Output:**
```
Hello! I'm just a computer program, so I don't have feelings. 
How can I assist...
```
- ‚úÖ Human-readable
- ‚úÖ Appropriate conversational response
- ‚úÖ Grammatically correct

**SUT (worker-orcd) Output:**
```
(Not tested separately)
```

**Expected Behavior:** Should generate polite conversational response  
**Actual Behavior:** Would generate mojibake (inferred from Prompt 1 results)  
**First Divergence:** Token index **0** (inferred)

---

## üîç Divergence Analysis

### Pattern: Immediate Divergence at Token 0

**Key Finding:** SUT diverges from reference **immediately** at the very first generated token.

**What This Eliminates:**
- ‚ùå **NOT tokenization:** Prompt is tokenized correctly (no crashes)
- ‚ùå **NOT special tokens:** Special token handling works (151644/151645 processed correctly)
- ‚ùå **NOT embeddings:** Embedding lookup succeeds (generation starts)
- ‚ùå **NOT KV cache corruption:** First token doesn't use KV cache (prefill-only)
- ‚ùå **NOT sampling issues:** Deterministic decoding (temp=0, top_k=1) eliminates randomness

**What This Points To:**
- ‚úÖ **Model inference bug:** Forward pass produces wrong logits
- ‚úÖ **Early in pipeline:** Divergence at token 0 means bug affects prefill pass
- ‚úÖ **Deterministic:** Same garbage output every run (not random)

### Token ID Symptom Analysis

**SUT generates high token IDs:**
- Token 111766, 111422, 147160, 114371, 106861... (very high IDs)
- These are in the special/extended vocab range (151936 total vocab)
- Suggests logits are biased toward high-index tokens

**Expected token IDs (from reference):**
- Typical English tokens are in lower ranges (e.g., "GPU" = ?, "power" = ?)
- Reference generates coherent text with standard token IDs

**Hypothesis:** Logit distribution is corrupted, causing top-k selection to pick wrong tokens

---

## üéØ Root Cause Suspects (Ranked by Evidence)

### #1: FFN Down-Projection Weight Loading (HIGH CONFIDENCE)
**Evidence:**
- Team Charlie Beta identified `ffn_down` weights not loaded correctly
- Fix committed but never tested (compilation errors)
- FFN path is critical for final hidden state ‚Üí logits
- Would explain why ALL tokens are wrong (affects every forward pass)

**Test to Confirm:**
- Verify `ffn_down` weight tensor is loaded from GGUF
- Check tensor name: "blk.{layer}.ffn_down.weight"
- Compare loaded weight values with llama.cpp debug output

---

### #2: Final Projection (lm_head) Weight Corruption (MEDIUM CONFIDENCE)
**Evidence:**
- Team Charlie verified cuBLAS math is correct
- But didn't verify weight loading is correct
- If lm_head weights are wrong ‚Üí all logits wrong

**Test to Confirm:**
- Dump lm_head weight matrix first row (vocab token 0)
- Compare with llama.cpp's loaded weights
- Check tensor name: "output.weight"

---

### #3: Hidden State Accumulation Error (MEDIUM CONFIDENCE)
**Evidence:**
- Team Charlie observed hidden state range [-32.8, 31.2] (outside normal bounds)
- Suggests accumulation issue across layers
- Residual connections not scaling properly?

**Test to Confirm:**
- Log hidden state after each layer (layers 0, 12, 23)
- Compare with llama.cpp at same positions
- Check for gradual divergence vs immediate divergence

---

### #4: RMSNorm Implementation Bug (LOW CONFIDENCE)
**Evidence:**
- Team Aurora suspected epsilon value or formula
- But multiple teams verified RMSNorm formula matches llama.cpp
- Would need very specific error to cause total mojibake

**Test to Confirm:**
- Dump RMSNorm output after layer 0
- Compare with llama.cpp
- Verify epsilon = 1e-6

---

### #5: Layer Mismatch / Weight Loading Order (LOW CONFIDENCE)
**Evidence:**
- If weights loaded in wrong order ‚Üí total corruption
- But this would likely cause crashes, not mojibake
- llama.cpp and worker-orcd use same GGUF format

**Test to Confirm:**
- Dump weight tensor names and sizes from GGUF
- Compare loading order between llama.cpp and worker-orcd
- Verify all 24 layers loaded correctly

---

## üî¨ Recommended Next Steps

### Immediate Action: Verify FFN Down-Projection Loading
```bash
# Check if ffn_down weights are actually loaded
grep -r "ffn_down" bin/worker-orcd/cuda/src/model/qwen_weight_loader.cpp

# Look for tensor loading code
# Expected: load_tensor("blk.{layer}.ffn_down.weight")
```

### Micro-Test: Single Layer Forward Pass
```cpp
// Test FFN path in isolation
// Input: Known hidden state vector (e.g., all 1.0)
// Output: Should match llama.cpp FFN output
// This isolates FFN from attention/other components
```

### Parity Test: Logit Comparison
```bash
# Run both reference and SUT with identical prompt
# Dump logits before sampling
# Compare top-10 logits (should be identical)
# If different ‚Üí confirms forward pass bug
```

---

## üìÅ Evidence Artifacts

**Reference Logs:**
- `/tmp/pear_baseline/logs/ref_prompt1_text.txt` ‚Äî llama.cpp haiku output
- `/tmp/pear_baseline/logs/ref_prompt2_text.txt` ‚Äî llama.cpp counting output
- `/tmp/pear_baseline/logs/ref_prompt3_text.txt` ‚Äî llama.cpp math output
- `/tmp/pear_baseline/logs/ref_prompt4_text.txt` ‚Äî llama.cpp transformer explanation
- `/tmp/pear_baseline/logs/ref_prompt5_text.txt` ‚Äî llama.cpp greeting

**SUT Logs:**
- Test output from `haiku_generation_anti_cheat` (100 tokens generated)
- First 10 token IDs and text documented above

---

## ‚úÖ Conclusion

**Finding:** SUT diverges from reference **immediately at token 0** across all tested prompts.

**Symptom:** Complete mojibake (Chinese characters, code tokens, fragments) instead of human-readable English.

**Root Cause Hypothesis:** Forward pass produces corrupted logits, most likely due to:
1. **FFN down-projection weight loading bug** (highest probability)
2. **lm_head weight corruption** (medium probability)
3. **Hidden state accumulation error** (medium probability)

**Confidence:** High ‚Äî divergence at token 0 eliminates sampling, KV cache, and multi-token issues.

**Next Action:** Verify FFN down-projection weight loading (Team Charlie Beta's fix).

---

**Report Generated:** 2025-10-07T10:49Z  
**Investigator:** TEAM PEAR  
**Evidence Location:** `/tmp/pear_baseline/logs/`  
**No files committed per mandate.**
