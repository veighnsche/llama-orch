# Quick Reference - Garbage Output Bug Investigation

**Read this FIRST before investigating!**

---

## ğŸš¨ Current Status

**Bug:** Model generates garbage output (code tokens, foreign language) instead of haiku

**What's VERIFIED CORRECT:**
- âœ… Tokenization (special tokens, IDs, sequence)
- âœ… Embeddings (all valid, not zeros)
- âœ… Prompt format (matches llama.cpp)
- âœ… Embedding lookup (CUDA kernel works)

**Where bug likely is:**
- â“ Forward pass (attention/FFN/residual)
- â“ KV cache (prefill/generation)
- â“ Position encoding (RoPE)
- â“ Sampling logic

---

## ğŸ“‹ Before You Start

### 1. Read These Documents (in order):
1. `FALSE_LEADS_SUMMARY.md` â† **READ THIS FIRST!**
2. `TEAM_BLUE_FINAL_STATUS.md` (tokenization fix)
3. `TEAM_PURPLE_FINAL_STATUS.md` (verification)

### 2. Check Code Comments:
- `src/inference/cuda_backend.rs` lines 153-246
- `cuda/src/transformer/qwen_transformer.cpp` lines 1033-1110

### 3. Don't Re-Investigate:
- âŒ Special token IDs (151644, 151645 are correct!)
- âŒ Token embeddings (they're valid!)
- âŒ Chat template format (matches llama.cpp!)
- âŒ Tokenization approach (it's correct!)

---

## ğŸ” Quick Facts

### Special Tokens (Qwen2.5)
```
151643 = BOS/PAD (endoftext)
151644 = <|im_start|>
151645 = <|im_end|>
```

### Token Sequence (Current)
```
[0] 151644 â†’ <|im_start|>
[1] 872 â†’ user
[2] 198 â†’ \n
[...] â†’ prompt text
[28] 151645 â†’ <|im_end|>
[29] 198 â†’ \n
[30] 151644 â†’ <|im_start|>
[31] 77091 â†’ assistant
```

### Embedding Values (Sample)
```
Token 151644: 0.0014 -0.0084 0.0073 ... (valid FP16)
```

---

## ğŸ¯ Investigation Priorities

### Priority 1: Compare with llama.cpp
Run llama.cpp with SAME prompt and compare:
- Hidden states after each layer
- Attention weights
- KV cache contents
- Logits before sampling

### Priority 2: Check KV Cache
- Dump cache after prefill
- Verify positions are correct
- Check if values match expected range

### Priority 3: Verify Forward Pass
- Add logging at each layer
- Check for NaN/Inf values
- Verify residual connections

### Priority 4: Test Isolation
- Disable KV cache (force recompute)
- Test with single token generation
- Compare prefill vs generation phase

---

## ğŸ§ª Useful Test Commands

### Run haiku test:
```bash
REQUIRE_REAL_LLAMA=1 cargo test --release --features cuda \
  --test haiku_generation_anti_cheat \
  test_haiku_generation_stub_pipeline_only \
  -- --ignored --nocapture --test-threads=1
```

### Check token sequence:
```bash
# Look for [TEAM_PURPLE] lines showing decoded tokens
... | grep "TEAM_PURPLE.*\["
```

### Check embeddings:
```bash
# Look for embedding values
... | grep "TEAM_PURPLE.*embedding"
```

---

## ğŸ“Š Symptom Analysis

### What We See:
```
Generated tokens:
[0] ID=131916 â†’ "Ã£Ä¤Â¸Ã£Ä¥Â¥"
[1] ID=72696 â†’ "Ä supplementation"
[2] ID=13267 â†’ "serve"
[3] ID=105030 â†’ "Ã¥ÅÄ­"
```

### What This Means:
- **Code tokens** (psycopg, toHaveBeenCalledWith) â†’ Wrong domain
- **Foreign language** (Chinese, Thai) â†’ Random selection
- **No haiku words** â†’ Model doesn't understand context

### What This Rules Out:
- âŒ NOT a tokenization issue (would see some correct words)
- âŒ NOT an embedding issue (would see semantic similarity)
- âŒ NOT a prompt format issue (llama.cpp works with same format)

### What This Suggests:
- âœ… Hidden states are corrupted
- âœ… Attention is focusing on wrong tokens
- âœ… KV cache contains wrong values
- âœ… Position encoding is incorrect

---

## ğŸš« Common Mistakes

### Mistake #1: "Let me check the token IDs..."
**STOP!** Token IDs are verified correct. Read `FALSE_LEADS_SUMMARY.md`.

### Mistake #2: "Maybe embeddings are zeros..."
**STOP!** Embeddings are verified valid. Read Team Purple's findings.

### Mistake #3: "Let me try different chat template..."
**STOP!** Template matches llama.cpp exactly. Don't change it.

### Mistake #4: "Maybe we need BOS token..."
**STOP!** Qwen2.5 doesn't use BOS. llama.cpp confirms this.

---

## ğŸ’¡ Debugging Tips

### Add Logging:
```cpp
// In qwen_transformer.cpp
fprintf(stderr, "[YOUR_TEAM] After layer %d: range=[%.4f, %.4f]\n", 
        layer_idx, min_val, max_val);
```

### Compare Values:
```bash
# Run llama.cpp with same prompt
./llama-cli -m model.gguf -p "Write a haiku..." -n 10

# Compare output with our test
```

### Check Ranges:
- Embeddings: ~0.01 to 0.04 (normal)
- Hidden states: should grow gradually, not explode
- Logits: -5 to +5 (normal), not -100 or +100

---

## ğŸ“ Need Help?

1. **Read documents first** (especially `FALSE_LEADS_SUMMARY.md`)
2. **Check code comments** (they have detailed findings)
3. **Look at previous team's work** (don't repeat investigations)
4. **Document your findings** (add comments for next team)

---

## âœ… When You Find The Bug

1. **Document the root cause** clearly
2. **Add comments** explaining what was wrong
3. **Update this guide** with new findings
4. **Mark false leads** so others don't follow them

---

**Good luck! ğŸ€**

---
