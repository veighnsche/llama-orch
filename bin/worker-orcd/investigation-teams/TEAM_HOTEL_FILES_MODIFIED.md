# Team HOTEL - Files Modified

**Date:** 2025-10-06 20:09-20:15 UTC

## Summary

Fixed Team GEMMA DELTA's dimension swap bug by correcting tensor dimension interpretation and adding comprehensive forensic comments.

---

## Files Modified

### 1. `src/inference/cuda_backend.rs`
**Lines:** 174-265  
**Changes:**
- Replaced Team GEMMA DELTA's incorrect comment block
- Fixed dimension extraction: `dimensions[0]` = hidden_dim, `dimensions[1]` = padded_vocab_size
- Added logic to get vocab_size from tokenizer metadata (not tensor!)
- Added cross-check between tensor hidden_dim and metadata hidden_dim
- Added detailed forensic comments explaining the bug

**Key Fix:**
```rust
// BEFORE (WRONG):
let vocab_size = output_tensor.dimensions.get(0)?;  // Actually gets hidden_dim!
let padded_vocab_size = output_tensor.dimensions.get(1)?;

// AFTER (CORRECT):
let hidden_dim_from_tensor = output_tensor.dimensions.get(0)?;  // 896
let padded_vocab_size = output_tensor.dimensions.get(1)?;  // 151936
let vocab_size = self.metadata.vocab_size()? as u32;  // 151643 from tokenizer
```

---

### 2. `src/cuda/model.rs`
**Lines:** 74-145  
**Changes:**
- Added warning comment block explaining this code is WRONG
- Explained why it doesn't break everything (cuda_backend.rs overrides the value)
- Added forensic comments to the dimension extraction
- Changed debug output to warn that the value is wrong

**Key Addition:**
```rust
// [TEAM_HOTEL] ‚ö†Ô∏è  WARNING: THIS CODE IS WRONG! (2025-10-06 20:10 UTC)
// This reads dimensions[0] = 896 (hidden_dim), not vocab_size!
// Actual tensor: [896, 151936] = [hidden_dim, padded_vocab_size]
```

---

### 3. `cuda/src/transformer/qwen_transformer.h`
**Lines:** 12-31  
**Changes:**
- Added comprehensive comment block explaining vocab_size vs padded_vocab_size
- Documented the three critical values and their usage
- Explained tensor dimensions [896, 151936]

**Key Addition:**
```cpp
// ============================================================================
// [TEAM_HOTEL] CRITICAL UNDERSTANDING: vocab_size vs padded_vocab_size
// ============================================================================
//
// The output.weight (lm_head) tensor in GGUF has dimensions [896, 151936]:
//   - dimensions[0] = 896 = hidden_dim (input to matrix multiply)
//   - dimensions[1] = 151936 = padded_vocab_size (output, includes padding)
//
// The tokenizer metadata has vocab_size = 151643 (logical valid tokens)
```

---

### 4. `cuda/src/transformer/qwen_transformer.cpp`
**Lines:** 612-652  
**Changes:**
- Replaced Team GEMMA DELTA's comment with detailed bug explanation
- Fixed cuBLAS call to use `padded_vocab_size` for m, lda, and ldc
- Added forensic comments explaining why position 8850 failed

**Key Fix:**
```cpp
// BEFORE (WRONG):
cublasGemmEx(...,
    config_.vocab_size,        // m = 896 (WRONG!)
    ...,
    logits, CUDA_R_32F, config_.vocab_size,  // ldc = 896
    ...);

// AFTER (CORRECT):
cublasGemmEx(...,
    config_.padded_vocab_size,  // m = 151936 ‚úì
    ...,
    logits, CUDA_R_32F, config_.padded_vocab_size,  // ldc = 151936 ‚úì
    ...);
```

**Lines:** 744-748  
**Changes:**
- Fixed verification code to copy `padded_vocab_size` logits, not `vocab_size`
- Added comment explaining why this is necessary

---

### 5. `cuda/src/ffi_inference.cpp`
**Lines:** 94-119  
**Changes:**
- Added detailed comment block explaining buffer allocation
- Fixed buffer allocation to use `padded_vocab_size` instead of `vocab_size`
- Fixed initialization vector size

**Key Fix:**
```cpp
// BEFORE (WRONG):
cudaMalloc(&logits, vocab_size * sizeof(float));  // Only 151643 floats
std::vector<float> init_logits(vocab_size, -INFINITY);

// AFTER (CORRECT):
cudaMalloc(&logits, padded_vocab_size * sizeof(float));  // Full 151936 floats
std::vector<float> init_logits(padded_vocab_size, -INFINITY);
```

---

### 6. `cuda/src/adapters/gpt_adapter.cpp`
**Lines:** 401-404  
**Changes:**
- Added comment explaining argmax should use `vocab_size` (logical), not `padded_vocab_size`
- Confirmed existing code is CORRECT (no code change needed)

**Key Comment:**
```cpp
// [TEAM_HOTEL] CRITICAL: Only scan vocab_size (151643) positions, not padded_vocab_size!
//   The logits buffer has 151936 positions, but the last 293 are padding values.
//   Scanning them would potentially pick garbage tokens from the padding region.
//   This is CORRECT - we use config_.vocab_size (logical size) for argmax.
```

---

## New Files Created

### 1. `investigation-teams/TEAM_HOTEL_FINDINGS.md`
Complete investigation report with:
- Bug discovery process
- Root cause analysis
- All code fixes
- Forensic comments summary
- Lessons learned

### 2. `investigation-teams/TEAM_HOTEL_FILES_MODIFIED.md`
This file - summary of all changes made.

---

## Testing

**Command:**
```bash
REQUIRE_REAL_LLAMA=1 cargo test --release --features cuda \
  --test haiku_generation_anti_cheat \
  test_haiku_generation_stub_pipeline_only \
  -- --ignored --nocapture --test-threads=1
```

**Status:** Compiling (as of 20:15 UTC)

---

## Summary of Changes

| File | Lines Changed | Type |
|------|--------------|------|
| `src/inference/cuda_backend.rs` | ~90 | Code + Comments |
| `src/cuda/model.rs` | ~40 | Comments only |
| `cuda/src/transformer/qwen_transformer.h` | ~20 | Comments only |
| `cuda/src/transformer/qwen_transformer.cpp` | ~50 | Code + Comments |
| `cuda/src/ffi_inference.cpp` | ~30 | Code + Comments |
| `cuda/src/adapters/gpt_adapter.cpp` | ~5 | Comments only |

**Total:** ~235 lines of forensic comments and code fixes

---

Built by Team HOTEL üè®
