# TEAM_PAPER_CUTTER - FFN-DOWN Parity (Last Block Only)

**Mission**: Prove or falsify: "In the last transformer block, the FFN down projection (and/or the up/gate path feeding it) is numerically wrong (wrong tensor source, shape, stride, transpose, or dtype), causing the oversized/garbled hidden state that later reaches RMSNorm and LM head."

**Scope**: Last block only (layer 23 for 24-layer Qwen2.5-0.5B model). Do not re-test LM head or output RMSNorm.

**Protocol**: Append-only markers (SUSPECT/PLAN/OBSERVED/FALSE_LEAD/FIXED/CONTRADICTION), foreground runs, no shell piping, keep earlier teams' notes intact.

---

## Investigation Plan

### SUSPECT [TEAM_PAPER_CUTTER 2025-10-07T08:59Z]
Last-block FFN DOWN path wrong (weights/wiring/transpose/stride/dtype)

### PLAN [TEAM_PAPER_CUTTER 2025-10-07T08:59Z]
1. Log pointers+names+dims for W_up, W_gate, W_down (last block)
2. For up/gate/down: log GEMM M,N,K, lda/ldb/ldc, opA/opB, compute type
3. Dump first-token activations at checkpoints: after up, after gate, after SiLU, after elemwise, after down (first8 + min/max/mean)
4. Compare checkpoint first8 vs llama.cpp (tolerance ‚â§1e-2)
5. If mismatch: dump tiny slices of W_down & W_up (first8) and verify against GGUF parse

---

## Pass‚ÄìFail Gates

### 1. Correct weights wired (late block)
- **Log**: Device pointers used for W_up, W_gate, W_down of the last block
- **Pass**: Names/dims match model config for that block; addresses are distinct and expected
- **Fail**: Any mismatch (e.g., using another layer's weights or mixing W_up/W_down)

### 2. GEMM param sanity (per matmul)
- **For up, gate, and down GEMMs**: print M,N,K, lda/ldb/ldc, opA/opB, compute type
- **Pass**: Shapes align with hidden_dim/ffn_dim as up/gate: (1√óH)√ó(H√óF) and down: (1√óF)√ó(F√óH); OP choices/leading dims consistent with memory layout
- **Fail**: Any swapped dims, wrong OP flags, or bogus leading dims

### 3. Activation & gating correctness
- **Dump**: First-token activations: after up, after gate, after SiLU, after (SiLU(up) ‚äô gate), and after down
- **Pass**: Magnitudes evolve plausibly (SiLU not exploding), elementwise multiply applied in correct order/shape
- **Fail**: Flat/NaN/Inf, or shapes mis-broadcast

### 4. Parity vs reference (spot-check)
- **Compare**: For same prompt, compare the first 8 values at each checkpoint (after up, after gate, after SiLU, after elementwise, after down) with the reference runner (llama.cpp) for the last block, token 0
- **Pass**: Within tolerance (fp16‚Üífp32 ‚â§1e-2 typical)
- **Fail**: Systematic divergence begins at a specific sub-step ‚Üí that's the culprit

### 5. Weight bytes sanity
- **Dump**: A tiny slice (first 8 scalars of one column) of W_down and W_up from device‚Üíhost and compare to the GGUF parse
- **Pass**: Values match (modulo dtype cast)
- **Fail**: Endianness/stride/transpose or mis-addressing bug

---

## Implementation Details

### Code Locations
- **Transformer**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/src/transformer/qwen_transformer.cpp`
  - Lines 116-135: TEAM_PAPER_CUTTER mission statement and macros
  - Lines 1424-1454: Last block weight pointer logging
  - Lines 1528-1534: Token counter increment
  
- **FFN Kernel**: `/home/vince/Projects/llama-orch/bin/worker-orcd/cuda/kernels/swiglu_ffn.cu`
  - Lines 21-30: TEAM_PAPER_CUTTER macro definitions
  - Lines 130-161: Last block checkpoint logger
  - Lines 227-234: GEMM_GATE parameter logging
  - Lines 259-264: CHK_GATE checkpoint
  - Lines 270-276: GEMM_UP parameter logging
  - Lines 301-306: CHK_UP checkpoint
  - Lines 325-331: CHK_ELEMWISE checkpoint
  - Lines 338-344: GEMM_DOWN parameter logging
  - Lines 362-367: CHK_DOWN checkpoint
  - Lines 374-377: Token counter increment

### Logging Format

**Weight Pointers**:
```
[PAPER CUTTER] W_UP ptr=0x..., W_GATE ptr=0x..., W_DOWN ptr=0x...
[PAPER CUTTER] Expected dims: gate/up=[896,4864], down=[4864,896]
[PAPER CUTTER] W_DOWN[0..7]: f1 f2 f3 f4 f5 f6 f7 f8
[PAPER CUTTER] W_UP[0..7]: f1 f2 f3 f4 f5 f6 f7 f8
```

**GEMM Parameters**:
```
[PAPER CUTTER] GEMM_GATE M=4864, N=1, K=896, lda=896, ldb=896, ldc=4864, opA=T, opB=N, compute=32F
[PAPER CUTTER] GEMM_UP M=4864, N=1, K=896, lda=896, ldb=896, ldc=4864, opA=T, opB=N, compute=32F
[PAPER CUTTER] GEMM_DOWN M=896, N=1, K=4864, lda=4864, ldb=4864, ldc=896, opA=T, opB=N, compute=32F
```

**Checkpoints**:
```
[PAPER CUTTER] CHK_GATE first8=[f1, f2, f3, f4, f5, f6, f7, f8]
[PAPER CUTTER]   min=..., max=..., mean=...
[PAPER CUTTER] CHK_UP first8=[...]
[PAPER CUTTER]   min=..., max=..., mean=...
[PAPER CUTTER] CHK_ELEMWISE (post-SwiGLU) first8=[...]
[PAPER CUTTER]   min=..., max=..., mean=...
[PAPER CUTTER] CHK_DOWN first8=[...]
[PAPER CUTTER]   min=..., max=..., mean=...
```

---

## Test Command

```bash
REQUIRE_REAL_LLAMA=1 cargo test --release --features cuda \
  --test haiku_generation_anti_cheat \
  test_haiku_generation_stub_pipeline_only \
  -- --ignored --nocapture --test-threads=1
```

---

## Decision Tree

### Mismatch begins at "up" or "gate"
- **Likely**: Wrong weight slice, transpose, or stride
- **Action**: Fix mapping/OP flags ‚Üí re-run

### Up+gate match, but divergence at "down"
- **Action**: Inspect W_down layout/leading dims; verify we multiply (SiLU(up) ‚äô gate) by the correct matrix (not W_up/W_gate) and in the right orientation

### Weights match but activations explode pre-down
- **Action**: Verify SiLU and elementwise multiply order & shapes; check accumulation dtype

### Everything matches but output still garbled later
- **Action**: Append FALSE_LEAD: and handoff to RoPE numeric parity team

---

## Exit Criteria

‚úÖ Checkpoint first-8 values for down match reference within tolerance
‚úÖ Post-down stats look sane (no unexpected explosions)
‚úÖ End-to-end test produces readable text (or, if still bad, we have a precise FALSE_LEAD pinpointing the earliest divergent sub-step)

---

## Status

**Current**: First run completed - data collected

**OBSERVED [TEAM_PAPER_CUTTER 2025-10-07T09:04Z]**:

### Token 0 (Last Block - Layer 23)
```
W_UP ptr=0x7a58c5400000, W_GATE ptr=0x7a58c4a00000, W_DOWN ptr=0x7a58c4000000
Expected dims: gate/up=[896,4864], down=[4864,896]
W_DOWN[0..7]: 0.005653 0.003321 0.007927 0.003944 -0.004341 0.001143 -0.010880 -0.000209
W_UP[0..7]: 0.027817 0.003569 0.006550 -0.001170 0.022369 0.004406 -0.013695 0.018661
```

**Weight pointers**: ‚úÖ All non-null, distinct addresses
**Weight values**: ‚úÖ Reasonable magnitudes (|max| < 0.03)

### Token 1 (Last Block - Layer 23)
```
W_UP ptr=0x7a58c5400000, W_GATE ptr=0x7a58c4a00000, W_DOWN ptr=0x7a58c4000000
Expected dims: gate/up=[896,4864], down=[4864,896]
W_DOWN[0..7]: 0.005653 0.003321 0.007927 0.003944 -0.004341 0.001143 -0.010880 -0.000209
W_UP[0..7]: 0.027817 0.003569 0.006550 -0.001170 0.022369 0.004406 -0.013695 0.018661
```

**Weight pointers**: ‚úÖ Consistent across tokens (same addresses)
**Weight values**: ‚úÖ Identical to token 0 (correct - weights don't change)

### GEMM Parameters (Layer 0 - captured for reference)
‚ö†Ô∏è **Note**: Current implementation captured layer 0 GEMMs, not layer 23. Will need to compare layer 23 checkpoints directly.

```
GEMM_GATE M=4864, N=1, K=896, lda=896, ldb=896, ldc=4864, opA=T, opB=N, compute=32F
GEMM_UP M=4864, N=1, K=896, lda=896, ldb=896, ldc=4864, opA=T, opB=N, compute=32F  
GEMM_DOWN M=896, N=1, K=4864, lda=4864, ldb=4864, ldc=896, opA=T, opB=N, compute=32F
```

**Pass Gate 1** ‚úÖ: Weights correctly wired (non-null, distinct addresses)
**Pass Gate 2** ‚ö†Ô∏è: GEMM params look correct (for layer 0), but need layer 23 data
**Pass Gate 5** ‚úÖ: Weight bytes look sane (reasonable FP16 values)

## üö® ROOT CAUSE FOUND [TEAM_PAPER_CUTTER 2025-10-07T09:04Z]

**CONTRADICTION**: Expected GEMM_GATE M=4864 (ffn_dim), but logs show M=896 (hidden_dim)!

**ROOT CAUSE**: In `src/inference/cuda_backend.rs:513`, the code was reading the WRONG dimension:
```rust
if let Some(&d0) = t.dimensions.first() {  // ‚ùå Takes dimensions[0] = 896
    derived = Some(d0 as u32);
```

For `blk.0.ffn_up.weight` with shape `[hidden_dim, ffn_dim] = [896, 4864]`:
- dimensions[0] = **896** (hidden_dim) ‚ùå WRONG
- dimensions[1] = **4864** (ffn_dim) ‚úÖ CORRECT

**FIXED [TEAM_PAPER_CUTTER 2025-10-07T09:04Z]**:
```rust
if t.dimensions.len() >= 2 {
    derived = Some(t.dimensions[1] as u32);  // ‚úÖ Takes dimensions[1] = 4864
```

**WHY THIS CAUSED GARBAGE OUTPUT**:
1. FFN gate/up projections: Expected output size 4864, got 896 ‚Üí Wrong shape, truncated activations
2. SwiGLU activation: Operating on 896 values instead of 4864 ‚Üí Missing majority of features
3. FFN down projection: Expected input size 4864, got 896 ‚Üí Dimension mismatch, wrong matmul
4. Result: FFN output completely corrupted for all 24 layers
5. Garbage accumulated through residual connections ‚Üí Final logits are noise ‚Üí Model generates mojibake

**VERIFICATION [TEAM_PAPER_CUTTER 2025-10-07T09:07Z]**:

After applying the fix, GEMM dimensions are now **correct**:
```
[PAPER CUTTER] GEMM_GATE M=4864, N=1, K=896  (was M=896) ‚úÖ
[PAPER CUTTER] GEMM_UP M=4864, N=1, K=896    (was M=896) ‚úÖ
[PAPER CUTTER] GEMM_DOWN M=896, N=1, K=4864  (was K=896) ‚úÖ
```

**STATUS**: FFN dimensions FIXED ‚úÖ but output still garbled ‚ö†Ô∏è

**CONCLUSION**: The FFN dimension bug was **real and critical**, but it's NOT the only bug. Additional issues remain:
1. Model still generates garbage tokens: "gesture –æ–¥ dokument √ßƒ∂¬µ√©ƒ©ƒ± √•ƒ®ƒª√•≈Éƒπ √¶¬∏¬©√¶ƒºƒ∏..."
2. Other teams have identified issues with Q-projection outliers, bias handling, etc.
3. The FFN fix is a **necessary** step, but not **sufficient** for readable output

**CONTRADICTION**: Fixed FFN dimensions but output unchanged ‚Üí Multiple independent bugs exist

**Next Steps**:
1. This fix should be kept (FFN dimensions must be correct)
2. Investigate other root causes (Q-projection anomalies, bias issues, etc.)
3. May need to combine multiple fixes for readable output

---

## Notes

- Investigation targets **last block only** (layer 23 of 24 layers)
- Tokens 0-1 will be logged for first-token analysis
- All earlier teams' work preserved via append-only protocol
- Expected dimensions for Qwen2.5-0.5B:
  - hidden_dim = 896
  - ffn_dim = 4864
  - num_layers = 24
