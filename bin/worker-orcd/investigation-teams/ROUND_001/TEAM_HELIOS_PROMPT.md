# ğŸ›°ï¸ **TEAM HELIOS â€” Next Investigation Prompt**

**Date:** 2025-10-08
**Focus:** FP16 model â€” Sampling & generation logic (post-logits phase)
**Predecessor:** Team AEGIS
**Status:** All 8 matmuls fixed & verified, weight loading matches llama.cpp, output still mojibake.

---

## ğŸ“ **Summary of Current State**

### âœ… Confirmed Fixed

* âœ… Q4_K dequantization (scale/min decoding + formula)
* âœ… Weight loading parity checks for FP16 tensors (Team VANGUARD + SENTINEL)
* âœ… All 8 matrix multiplications corrected (Team SENTINEL)
* âœ… lm_head cuBLAS parameters reverted correctly (Team AEGIS final)
* âœ… UTF-8 decoding is correct (Team AEGIS byte-level instrumentation)
* âœ… Prefill temperature = 0.0 is expected behavior

### âŒ Still Broken

* âŒ Model generates **mojibake / repetitive tokens**
* âŒ â€œeightâ€ sometimes appears but not deterministically â†’ indicates **partial parity** but **sampling or logits bug** remains
* âŒ Top-10 logits are sane, but **temperature during generation** was not properly logged â€” false lead

---

## ğŸš¨ **Mission Focus for Team HELIOS**

The remaining bug is **almost certainly downstream of logits**:

* Sampling temperature handling
* RNG seeding / deterministic sampling paths
* Top-p / top-k cutoff logic
* Argmax vs sampling mismatch between llama.cpp and our implementation

---

## ğŸ§­ **Investigation Plan**

### 1. **Temperature & Sampling Parity**

* Instrument **first 20 generated tokens** (after prefill!)
* Log temperature, top_k, top_p, RNG seed, and sampled token ID at each step.
* Run **llama.cpp with the same model + same prompt + temperature 0.7** and capture its first 20 tokens.
* Compare **token IDs** and **logits distributions** step by step.

ğŸ‘‰ **Goal:** Confirm whether our logits or sampling diverge first.

---

### 2. **Top-p / Top-k Implementation Check**

* Cross-read `cuda_sample_token` against llama.cppâ€™s sampling implementation.
* Check:

  * Is top-p cumulative distribution correct?
  * Are logits sorted or truncated the same way?
  * Is RNG seeded identically?
  * Are tokens masked the same way (e.g., special tokens, BOS/EOS)?

---

### 3. **Argmax vs Probabilistic**

* Test with:

  * `temperature = 0.0` (pure argmax)
  * `temperature = 0.7`
  * `temperature = 1.0`

Run haiku test for each and compare:

* If `temperature=0.0` matches llama.cpp, logits are likely correct and **sampling is the culprit**.
* If it still diverges, thereâ€™s still a **logits / softmax bug** lurking.

---

## ğŸ“š **Rules & Expectations**

* ğŸ§± **APPEND-ONLY COMMENTS** â€” Never delete or overwrite previous team notes.
* ğŸ“ Use `SUSPECT`, `PLAN`, `OBSERVED`, `FALSE_FIX`, `LESSON` consistently.
* ğŸš« Do **not** retest cuBLAS or weight loading â€” theyâ€™re verified.
* ğŸ“Š Always **compare against llama.cpp ground truth**, not just internal math.
* ğŸ§ª Focus on **generation phase**, not prefill.

---

## ğŸ“ Useful Tools

* âœ… `investigation-teams/llama_cpp_weight_dumper.cpp` â€” for weight parity
* âœ… Logit/top-k logging added by Team AEGIS in `ffi_inference.cpp`
* âœ… Byte-level token logging in `cuda_backend.rs`

---

## ğŸ¯ **Success Criteria**

Team HELIOS succeeds if it can demonstrate **first 20 generated token IDs** match llama.cpp at `temperature=0.7`.
Even partial matching with clear divergence point is a success if logged cleanly.
