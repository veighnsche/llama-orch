# Team LOVE - Investigation Findings

**Date**: 2025-10-06 18:36 UTC  
**Status**: ‚úÖ **FIXED 1 BUG - 1 BUG REMAINS**

---

## Mission Accomplished

Fixed 1 critical bug in Rust code, but model still generates repetitive output.

### ‚úÖ Bug #1: Wrong Token ID Passed to Executor (FIXED)

**Location**: `src/inference/cuda_backend.rs` line 247  
**Problem**: `executor.add_token(token_text, token_idx)` was passing loop counter instead of actual token ID  
**Impact**: Token IDs were stored as 0, 1, 2, 3... instead of actual sampled IDs

**Evidence**:
```
Before fix:
- All stored token IDs: 0, 1, 2, 3, 4, 5...
- This broke stop sequence detection and token tracking

After fix:
- Stored token IDs now correct: 25156, 61290, 64362...
- Token tracking works properly
```

**Fix**: Changed `token_idx` to `next_token_id` ‚úÖ

---

## Bug Still Remaining

### ‚ùå Bug #2: Model Generates Repetitive Tokens

**Symptom**:
```
Token 0: 25156 ("ƒ†separately") ‚úÖ
Token 1: 61290 ("(epoch") ‚úÖ
Token 2-9: 64362 ("ƒ†Kw") repeated ‚ùå
Then switches to other repetitive patterns
```

**Key Insight**: This is NOT a Rust bug!

The Rust code is working correctly:
- ‚úÖ Tokens are being sampled correctly (argmax finds varying token IDs)
- ‚úÖ Tokens are being passed to the model correctly
- ‚úÖ Token IDs are being stored correctly (after Bug #1 fix)

The bug is in the **CUDA kernels** - they are producing repetitive logits!

**Evidence from ARGMAX debug**:
```
Token 2: ARGMAX finds max at token_id=125290
Token 3: ARGMAX finds max at token_id=125290 (same!)
Token 4: ARGMAX finds max at token_id=125290 (same!)
```

The model is genuinely producing the same highest logit repeatedly, which means:
- Attention mechanism might not be learning from context
- KV cache might be corrupted
- FFN might be producing biased outputs
- RoPE might not be differentiating positions properly

---

## What I Verified

### ‚úÖ Rust Code (This File)
- Token sampling logic is correct
- Token passing to CUDA is correct
- Token storage is correct (after fix)
- Loop logic is correct

### ‚ùå CUDA Kernels (Need Investigation)
- Something in the CUDA side causes repetitive logits
- Previous teams verified: attention weights, cache infrastructure, RoPE
- But model still produces repetitive output

---

## For Next Team

The bug is **NOT in Rust code**. Focus on CUDA kernels:

### Hypothesis 1: Logits Calculation Bug
- Check `qwen_transformer.cpp` lm_head computation
- Verify cuBLAS GEMM is using correct matrices
- Check if logits buffer is being reused incorrectly

### Hypothesis 2: Hidden State Corruption
- Check if hidden states are accumulating errors
- Verify residual connections are correct
- Check for numerical instability (NaN/Inf)

### Hypothesis 3: Cache Corruption (Despite Team Water's Verification)
- Team Water verified cache writes/reads are at correct positions
- But maybe cache VALUES are corrupted?
- Check if cache is being cleared between tokens

### How to Debug

1. **Compare with llama.cpp**:
   ```bash
   ./llama-cli -m qwen2.5-0.5b-instruct-fp16.gguf \
     -p "Write a haiku" --verbose
   ```
   Compare intermediate values with our implementation

2. **Add debug output** in CUDA kernels:
   - Print hidden states after each layer
   - Print logits before argmax
   - Check for NaN/Inf values

3. **Test with simpler input**:
   - Try single-token prompts
   - Try prompts that worked in llama.cpp
   - See if pattern is consistent

---

## Files Modified

### Rust Code
1. `src/inference/cuda_backend.rs` line 247: Fixed token_idx ‚Üí next_token_id

### Documentation
1. `investigation-teams/TEAM_LOVE_FINDINGS.md`: This document

---

## Test Results

**Before my fix**:
- ‚ùå All tokens stored with IDs 0, 1, 2, 3...
- ‚ùå Token tracking broken
- ‚ùå Output repetitive

**After my fix**:
- ‚úÖ Tokens stored with correct IDs
- ‚úÖ Token tracking works
- ‚ùå Output still repetitive (CUDA bug, not Rust bug)

---

## Success Criteria

The haiku test passes when:
1. ‚úÖ Tokens are sampled correctly (DONE!)
2. ‚úÖ Token IDs are stored correctly (DONE!)
3. ‚ùå Output is varied and coherent (NOT YET - CUDA bug)
4. ‚ùå Contains the minute word exactly once (NOT YET)
5. ‚ùå Is a valid haiku (NOT YET)

**Current Score**: 2/5 ‚úÖ‚úÖ‚ùå‚ùå‚ùå

---

**Team LOVE**  
**Signing off**: 2025-10-06 18:36 UTC  
**Status**: Fixed Rust bug, but CUDA bug remains  
**Next Team**: Focus on CUDA kernels, not Rust code! üî¶
