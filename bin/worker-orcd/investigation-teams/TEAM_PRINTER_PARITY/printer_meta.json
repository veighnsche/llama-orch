{
  "team": "TEAM_PRINTER",
  "mission": "Parity Data Sweep - Utility Team",
  "timestamp": "2025-10-07T01:24:35Z",
  "environment": {
    "gpu_0": "NVIDIA GeForce RTX 3060",
    "gpu_1": "NVIDIA GeForce RTX 3090",
    "driver_version": "550.163.01",
    "compute_capability": "8.6",
    "cuda_version": "12.0.140",
    "cublas_version": "12.0"
  },
  "repositories": {
    "llama_orch_commit": "5fc5fecc1278f161b9209b57acc288d00f9e6701",
    "llama_cpp_commit": "c08002a1988348403a5fd59b1fa3de3a10a6f92f"
  },
  "model": {
    "path": "/home/vince/Projects/llama-orch/.test-models/qwen/qwen2.5-0.5b-instruct-fp16.gguf",
    "sha256": "8e0ae26000627ed62de0e78e41860af70094558b9d2913385c842a6aa06cf3fc",
    "architecture": "qwen2",
    "size_bytes": 1288490240,
    "format": "GGUF FP16"
  },
  "test_config": {
    "prompt": "Write a haiku about GPU computing",
    "temperature": 0.0,
    "top_p": 1.0,
    "top_k": 0,
    "seed": 12345,
    "tokens_to_capture": [0, 1],
    "sampling_mode": "greedy"
  },
  "checkpoints": [
    "embedding_input_ids",
    "embedding_output",
    "layer0_attn_norm_output",
    "layer0_q_pre_rope",
    "layer0_k_pre_rope",
    "layer0_v_pre_rope",
    "layer0_q_post_rope",
    "layer0_k_post_rope",
    "layer0_attention_output",
    "layer0_ffn_input",
    "layer0_ffn_output",
    "layer0_residual_output",
    "final_pre_lm_head_hidden",
    "lm_head_logits_top64",
    "chosen_token_id",
    "decoded_utf8_string"
  ],
  "notes": [
    "All tensors saved as FP32 for apples-to-apples comparison",
    "Greedy sampling (temperature=0.0) ensures deterministic output",
    "Fixed seed=12345 for reproducibility",
    "Capturing tokens 0 (BOS) and 1 only to minimize data volume",
    "LM head logits: saving top 64 values + indices, plus full checksum"
  ]
}
