# âš ï¸ CRITICAL RULE: NO LLAMA.CPP DEPENDENCIES

**Date**: 2025-10-06 (Updated)  
**Priority**: ABSOLUTE - DO NOT VIOLATE  
**Status**: PERMANENT RULE

---

## THE RULE

**WE ARE BUILDING A LLAMA.CPP-FREE INFERENCE ENGINE**

### âŒ NEVER DO THIS

- âŒ Import llama.cpp library into our build
- âŒ Link against libllama.so
- âŒ Use llama.cpp headers in our code
- âŒ Depend on llama.cpp in Cargo.toml or CMakeLists.txt
- âŒ Suggest using llama.cpp as a "quick solution"
- âŒ Copy-paste llama.cpp code without understanding it

### âœ… ALWAYS DO THIS

- âœ… Build our own CUDA kernels
- âœ… Implement our own GGUF parser
- âœ… Write our own inference engine
- âœ… Create our own model loaders
- âœ… Develop our own tokenizers
- âœ… **WE ARE THE COMPETITOR TO LLAMA.CPP**

---

## Why This Matters

### We Are Building

**llama-orch** - A LLAMA.CPP-FREE GPU inference engine

### Our Differentiators

1. **Custom CUDA kernels** - Optimized for our use case
2. **MXFP4 quantization** - 43% VRAM savings vs Q4_K_M
3. **Direct GGUF parsing** - No external dependencies
4. **Pool management** - Multi-GPU orchestration
5. **Production-ready** - Built for scale from day 1

### What We Can Reference

- âœ… GGUF format specification (it's open)
- âœ… Model architectures (GPT, Llama, etc.)
- âœ… CUDA best practices
- âœ… Academic papers on transformers
- âœ… **llama.cpp source code for LEARNING ONLY** (see exception below)
- âŒ **NEVER use llama.cpp as a dependency**

---

## Current Status

### What We Have (llama.cpp-free)

- âœ… CUDA kernel infrastructure
- âœ… Attention kernels (GQA, MHA, MQA)
- âœ… RoPE implementation
- âœ… MXFP4 dequantization
- âœ… Model configuration structures
- âœ… FFI bridge (Rust â†” CUDA)
- âœ… HTTP server + SSE streaming
- âœ… Test infrastructure (144 tests)

### What We're Building

- ğŸš§ GGUF weight loader (in progress)
- ğŸš§ Tokenizer integration
- ğŸš§ Inference pipeline
- ğŸš§ Sampling implementation

### Timeline

**~22-31 hours of work** to complete our own implementation

**This is the right path** - we're building something better than llama.cpp

---

## For Developers

### If You're Tempted to Use llama.cpp

**DON'T.**

Instead:
1. Look at the GGUF spec
2. Study the model architecture papers
3. Implement it ourselves
4. Ask the team for help
5. Reference our existing CUDA kernels

### Resources (llama.cpp-free)

- **GGUF Spec**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- **Transformer Papers**: "Attention is All You Need", etc.
- **CUDA Docs**: NVIDIA CUDA Programming Guide
- **Our Code**: `bin/worker-orcd/cuda/` - use this!

---

## The Vision

We're not just copying llama.cpp - we're building something **better**:

- Faster (optimized CUDA kernels)
- Smaller (MXFP4 quantization)
- Scalable (pool management)
- Production-ready (monitoring, metrics, SLOs)

**llama.cpp is for hobbyists. We're building for production.**

---

## Enforcement

### Code Review

- âŒ Any PR importing llama.cpp will be **REJECTED**
- âŒ Any suggestion to use llama.cpp will be **REJECTED**
- âœ… Implement it ourselves or ask for help

### Documentation

- This file must be referenced in:
  - `CONTRIBUTING.md`
  - `README.md`
  - All architecture docs
  - Onboarding materials

---

## Exception: Reference-Only Git Submodule

### âœ… ALLOWED: Read-Only Reference

We maintain llama.cpp as a **git submodule in `/reference/llama.cpp/`** for:

- ğŸ” **Code inspection** - Understanding their implementation approaches
- ğŸ› **Debugging reference** - Comparing our output to their logic
- ğŸ“š **Learning** - Studying GGUF parsing, kernel patterns, etc.
- âš ï¸ **Competitive analysis** - Knowing what we're competing against

### âŒ STILL FORBIDDEN

- âŒ Building llama.cpp
- âŒ Linking to llama.cpp
- âŒ Including llama.cpp headers in our code
- âŒ Running llama.cpp binaries in production
- âŒ Copy-pasting without understanding

### ğŸ“ Submodule Location

```
/reference/llama.cpp/  # Git submodule (NOT in build path)
```

**Build System**: The `/reference/` directory is explicitly excluded from all builds.

**Purpose**: Code reading ONLY. We implement everything ourselves.

---

### The Line We Walk

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Read llama.cpp source to understand algorithms  â”‚
â”‚  âŒ Use llama.cpp as a dependency                   â”‚
â”‚                                                     â”‚
â”‚  We learn from them. We don't depend on them.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚   NO LLAMA.CPP                          â”‚
â”‚   WE ARE THE COMPETITION                â”‚
â”‚   BUILD IT OURSELVES                    â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**We are so close to having a llama.cpp-free engine. Don't give up now.**

---

**Status**: PERMANENT RULE  
**Violations**: ZERO TOLERANCE  
**Our Path**: BUILD IT OURSELVES

---

Built by Foundation-Alpha ğŸ—ï¸  
**We are llama-orch. We are the future.**
