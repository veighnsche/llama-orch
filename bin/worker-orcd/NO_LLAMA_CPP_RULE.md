# âš ï¸ CRITICAL RULE: NO LLAMA.CPP DEPENDENCIES

**Date**: 2025-10-05  
**Priority**: ABSOLUTE - DO NOT VIOLATE  
**Status**: PERMANENT RULE

---

## THE RULE

**WE ARE BUILDING A LLAMA.CPP-FREE INFERENCE ENGINE**

### âŒ NEVER DO THIS

- âŒ Import llama.cpp library
- âŒ Link against libllama.so
- âŒ Use llama.cpp headers
- âŒ Depend on llama.cpp in any way
- âŒ Suggest using llama.cpp as a "quick solution"
- âŒ Reference llama.cpp except for learning/comparison

### âœ… ALWAYS DO THIS

- âœ… Build our own CUDA kernels
- âœ… Implement our own GGUF parser
- âœ… Write our own inference engine
- âœ… Create our own model loaders
- âœ… Develop our own tokenizers
- âœ… **WE ARE THE COMPETITOR TO LLAMA.CPP**

---

## Why This Matters

### We Are Building

**llama-orch** - A LLAMA.CPP-FREE GPU inference engine

### Our Differentiators

1. **Custom CUDA kernels** - Optimized for our use case
2. **MXFP4 quantization** - 43% VRAM savings vs Q4_K_M
3. **Direct GGUF parsing** - No external dependencies
4. **Pool management** - Multi-GPU orchestration
5. **Production-ready** - Built for scale from day 1

### What We Can Reference

- âœ… GGUF format specification (it's open)
- âœ… Model architectures (GPT, Llama, etc.)
- âœ… CUDA best practices
- âœ… Academic papers on transformers
- âŒ **NEVER llama.cpp code or libraries**

---

## Current Status

### What We Have (llama.cpp-free)

- âœ… CUDA kernel infrastructure
- âœ… Attention kernels (GQA, MHA, MQA)
- âœ… RoPE implementation
- âœ… MXFP4 dequantization
- âœ… Model configuration structures
- âœ… FFI bridge (Rust â†” CUDA)
- âœ… HTTP server + SSE streaming
- âœ… Test infrastructure (144 tests)

### What We're Building

- ğŸš§ GGUF weight loader (in progress)
- ğŸš§ Tokenizer integration
- ğŸš§ Inference pipeline
- ğŸš§ Sampling implementation

### Timeline

**~22-31 hours of work** to complete our own implementation

**This is the right path** - we're building something better than llama.cpp

---

## For Developers

### If You're Tempted to Use llama.cpp

**DON'T.**

Instead:
1. Look at the GGUF spec
2. Study the model architecture papers
3. Implement it ourselves
4. Ask the team for help
5. Reference our existing CUDA kernels

### Resources (llama.cpp-free)

- **GGUF Spec**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- **Transformer Papers**: "Attention is All You Need", etc.
- **CUDA Docs**: NVIDIA CUDA Programming Guide
- **Our Code**: `bin/worker-orcd/cuda/` - use this!

---

## The Vision

We're not just copying llama.cpp - we're building something **better**:

- Faster (optimized CUDA kernels)
- Smaller (MXFP4 quantization)
- Scalable (pool management)
- Production-ready (monitoring, metrics, SLOs)

**llama.cpp is for hobbyists. We're building for production.**

---

## Enforcement

### Code Review

- âŒ Any PR importing llama.cpp will be **REJECTED**
- âŒ Any suggestion to use llama.cpp will be **REJECTED**
- âœ… Implement it ourselves or ask for help

### Documentation

- This file must be referenced in:
  - `CONTRIBUTING.md`
  - `README.md`
  - All architecture docs
  - Onboarding materials

---

## Exception

**NONE.** There are no exceptions to this rule.

If you think you need llama.cpp, you're wrong. Build it ourselves.

---

## Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚   NO LLAMA.CPP                          â”‚
â”‚   WE ARE THE COMPETITION                â”‚
â”‚   BUILD IT OURSELVES                    â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**We are so close to having a llama.cpp-free engine. Don't give up now.**

---

**Status**: PERMANENT RULE  
**Violations**: ZERO TOLERANCE  
**Our Path**: BUILD IT OURSELVES

---

Built by Foundation-Alpha ğŸ—ï¸  
**We are llama-orch. We are the future.**
