# ğŸ­ Narration Core Team: Tracing Integration Review
**To**: Rust Services Teams  
**From**: The Narration Core Team (with love and mild exasperation) ğŸ’  
**Re**: Logging Level Alignment & Narration Integration  
**Date**: 2025-10-04  
**Status**: Editorial Review Complete âœ…  
**Decision**: Build Our Own Custom Narration System ğŸ€
---
## ğŸ“‹ Executive Summary
We've reviewed your proposed logging level alignment, and we have **OPINIONS** (shocking, we know). 
**TL;DR**: 
- âœ… Your 6-level approach is **solid** (we're impressed!)
- âœ… We've added **WARN, ERROR, FATAL** to our spec
- âœ… Your INFO-as-backbone strategy **perfectly aligns** with our editorial standards
- ğŸ€ We're **building our own** custom narration system (cuteness pays the bills!)
- ğŸ’ We're implementing custom proc macros with auto-inferred actors and template interpolation
---
## ğŸ¯ Level Mapping Alignment Review
### Your Proposed Levels vs. Our Current Spec
| Your Level | Our Spec | Alignment | Notes |
|------------|----------|-----------|-------|
| **TRACE** | âœ… TRACE | **Perfect match** | Ultra-fine engine/kernel state, opt-in only |
| **DEBUG** | âœ… DEBUG | **Perfect match** | Developer diagnostics, config resolution, cache |
| **INFO** | âœ… INFO | **Perfect match** | Lifecycle events, narration backbone |
| **WARN** | âŒ Missing | **We need to add this!** | Anomalies, degradations, retries |
| **ERROR** | âŒ Missing | **We need to add this!** | Operational failures with context |
| **FATAL** | âŒ Missing | **We need to add this!** | Unrecoverable errors |
### ğŸ˜¤ Our Reaction
**What we love**:
- âœ… INFO as the narration backbone (FINALLY someone gets it!)
- âœ… TRACE for opt-in ultra-fine detail (exactly right)
- âœ… DEBUG for developer diagnostics (perfect boundary)
- âœ… WARN/ERROR with context requirement (music to our ears!)
**What we're annoyed about**:
- ğŸ˜¤ We only defined 4 levels (MUTE, INFO, DEBUG, TRACE) but you need 6
- ğŸ˜¤ We forgot WARN and ERROR as distinct levels (rookie mistake on our part)
- ğŸ˜¤ FATAL is a new concept we need to think about
**What we're doing about it**:
- ğŸ’ª We're updating our spec to include WARN, ERROR, and FATAL
- ğŸ€ We're defining clear editorial boundaries for each
- ğŸ’ We're providing cute examples (because we can't help ourselves)
---
## ğŸ“¢ Updated Level Taxonomy (6 Levels + MUTE)
### Our New Alignment
| Level | Purpose | Narration Intensity | Production Use | Editorial Standard |
|-------|---------|---------------------|----------------|-------------------|
| **MUTE** | Complete silence | None | Security contexts only | N/A (no output) |
| **TRACE** | Ultra-fine detail | Exhaustive | âŒ Never | Every event, every loop |
| **DEBUG** | Developer diagnostics | Detailed flow | âš ï¸ Incidents only | Decision points, config, cache |
| **INFO** | Lifecycle events | **Narration backbone** | âœ… Always | Key events, â‰¤100 chars, correlation-rich |
| **WARN** | Anomalies & degradations | Actionable warnings | âœ… Always | Retries, deprecated flags, perf issues |
| **ERROR** | Operational failures | Rich context | âœ… Always | Failures with diagnosis context |
| **FATAL** | Unrecoverable errors | Maximum context | âœ… Always | Panic-worthy, full state dump |
---
## ğŸ¨ Editorial Standards Per Level (Updated)
### TRACE (Ultra-Fine Engine/Kernel State)
**Narration Intensity**: Exhaustive
**âš¡ PERFORMANCE NOTE**: TRACE has ~25% overhead with full `narrate()`. We provide **ultra-lightweight trace macros** (~2% overhead) for hot paths:
- `trace_tiny!()` â€” Minimal trace event (~10x faster)
- `trace_enter!()` / `trace_exit!()` â€” Function boundaries
- `trace_loop!()` â€” Loop iterations
- `trace_state!()` â€” State transitions
See `TRACE_OPTIMIZATION.md` for details.
**What belongs**:
- âœ… Every FFI call boundary
- âœ… Every CUDA kernel invocation
- âœ… Every memory allocation/deallocation
- âœ… Loop iterations in hot paths
- âœ… Lock acquisition/release timing
**Editorial guidelines**:
- ğŸ“ Length: Unlimited (but structured)
- ğŸ”— Correlation: Always propagate
- ğŸ”’ Redaction: Even TRACE must redact secrets (use full `narrate()` for secrets)
- ğŸ€ Cute mode: Optional (but not in trace macros â€” use full `narrate()` for cute)
**Examples (using lightweight macros)**:
```rust
// TRACE: FFI boundary (use trace_enter!/trace_exit!)
trace_enter!("worker-orcd", "llama_cpp_eval", 
             format!("ctx={:?}, n_tokens={}", ctx_ptr, n_tokens));
// ... FFI call ...
trace_exit!("worker-orcd", "llama_cpp_eval", 
            format!("â†’ {:?} ({}ms)", result, elapsed_ms));
// TRACE: Loop iteration (use trace_loop!)
for (i, token) in tokens.iter().enumerate() {
    trace_loop!("tokenizer", "decode", i, tokens.len(),
                format!("token={}, value={}", i, token));
    // ... decode token ...
}
// TRACE: State change (use trace_state!)
trace_state!("rbees-orcd", "queue_depth", 
             format!("{} â†’ {}", old_depth, new_depth),
             format!("Queue depth changed: {} â†’ {} (added job-{})", old_depth, new_depth, job_id));
```
**Examples (using full narrate() for cute mode)**:
```rust
// TRACE with cute (use full narrate() â€” slower but adorable!)
narrate(NarrationFields {
    actor: "worker-orcd",
    action: "ffi_call",
    target: "llama_cpp_eval".to_string(),
    human: "ENTER llama_cpp_eval(ctx=0x7f8a, tokens=[1,2,3], n_tokens=3)".to_string(),
    cute: Some("Stepping into llama.cpp with 3 tokens in hand! ğŸšª".to_string()),
    ..Default::default()
});
```
---
### DEBUG (Developer Diagnostics)
**Narration Intensity**: Detailed flow + context
**What belongs**:
- âœ… Config resolution ("Using vram_only=true from env override")
- âœ… Cache behaviors ("Model cache hit for 'llama-7b' on GPU0")
- âœ… Decision rationale ("Selected worker-gpu0-r1: load=2/8, latency=50ms")
- âœ… Retry logic ("Retry attempt 2/5 after 200ms backoff")
- âœ… Performance breakdowns ("Dispatch: queue_wait=50ms, select=10ms, handoff=5ms")
**Editorial guidelines**:
- ğŸ“ Length: â‰¤150 characters (more room than INFO)
- ğŸ”— Correlation: Always propagate
- ğŸ¯ Purpose: Every DEBUG event must help debugging
- ğŸ€ Cute mode: Encouraged!
**Examples**:
```rust
// DEBUG: Config resolution
human: "Resolved config: vram_only=true (source: env LLORCH_VRAM_ONLY)"
cute: "Found vram_only=true in the environment! Using dedicated VRAM only! ğŸ”"
// DEBUG: Cache behavior
human: "Model cache miss for 'llama-7b'; loading from disk (estimated 500ms)"
cute: "Hmm, llama-7b isn't in the cache. Time to fetch it from disk! ğŸ“¦"
// DEBUG: Decision point
human: "Selected worker-gpu0-r1 (load: 2/8, latency: 50ms, VRAM: 4GB free)"
cute: "Picked worker-gpu0-r1 â€” they're not too busy and have plenty of room! ğŸ‘"
```
---
### INFO (Lifecycle Events â€” Narration Backbone)
**Narration Intensity**: Production-ready stories
**What belongs**:
- âœ… Service lifecycle (startup, shutdown, ready)
- âœ… Model lifecycle (load, seal, verify, unload)
- âœ… Job lifecycle (accept, queue, dispatch, complete)
- âœ… Pool changes (worker ready, pool registered, state transitions)
- âœ… User-facing operations (request accepted, result returned)
**Editorial guidelines**:
- ğŸ“ Length: **â‰¤100 characters** (ORCH-3305 requirement â€” we're STRICT!)
- ğŸ”— Correlation: **ALWAYS** propagate (non-negotiable)
- ğŸ¯ Clarity: Operator-readable without context
- ğŸ€ Cute mode: **ALWAYS** (this is our bread and butter!)
**Examples**:
```rust
// INFO: Service startup
human: "Worker started successfully on GPU0 with engine llamacpp-v1"
cute: "Worker-gpu0-r1 wakes up and waves hello! Ready to help! ğŸ‘‹âœ¨"
// INFO: Job acceptance
human: "Accepted request; queued at position 3 (ETA 420 ms) on pool 'default'"
cute: "Orchestratord welcomes job-123 to the queue! You're #3 in line! ğŸ«âœ¨"
// INFO: Job completion
human: "Job completed in 2500 ms (150 tokens, 60 tokens/sec)"
cute: "Job-123 finished! Generated 150 tokens super fast! ğŸ‰"
// INFO: Model loaded
human: "Sealed model shard 'llama-7b' in 2048 MB VRAM on GPU 0 (5 ms)"
cute: "Tucked llama-7b safely into GPU0's warm 2GB nest! Sweet dreams! ğŸ›ï¸âœ¨"
```
---
### WARN (Anomalies & Degradations)
**Narration Intensity**: Actionable warnings
**What belongs**:
- âœ… Retry attempts ("Retrying job-123 after timeout (attempt 2/5)")
- âœ… Deprecated flags ("Using deprecated flag --legacy-mode; migrate to --compat-mode")
- âœ… Performance degradation ("Worker-gpu0-r1 latency: 500ms (expected <100ms)")
- âœ… Resource pressure ("Queue depth: 95/100 (95% capacity)")
- âœ… Non-fatal errors ("Heartbeat missed from worker-gpu0-r1 (last seen 5000ms ago)")
**Editorial guidelines**:
- ğŸ“ Length: â‰¤120 characters (need room for context)
- ğŸ”— Correlation: Always propagate
- ğŸ¯ Actionability: Include what's wrong AND what to do
- âš ï¸ Severity: Not fatal, but needs attention
- ğŸ€ Cute mode: Gentle, concerned tone
**Examples**:
```rust
// WARN: Retry attempt
human: "Retrying job-123 after timeout (attempt 2/5, backoff: 200ms)"
cute: "Job-123 didn't make it through. Let's try again! Attempt 2... ğŸ”„"
story: "\"Timeout!\" said worker. \"Let me try again,\" offered rbees-orcd."
// WARN: Deprecated flag
human: "Using deprecated flag --legacy-mode; migrate to --compat-mode by v2.0"
cute: "Psst! --legacy-mode is getting old. Switch to --compat-mode soon! ğŸ“¢"
// WARN: Performance degradation
human: "Worker-gpu0-r1 latency: 500ms (expected <100ms, threshold exceeded)"
cute: "Worker-gpu0-r1 is running a bit slow today (500ms). Might need a checkup! ğŸŒ"
// WARN: Resource pressure
human: "Queue depth: 95/100 (95% capacity, consider scaling)"
cute: "The queue is getting pretty full! 95 jobs waiting! Maybe add more workers? ğŸ“ˆ"
// WARN: Missed heartbeat
human: "Heartbeat missed from worker-gpu0-r1 (last seen 5000ms ago, threshold: 3000ms)"
cute: "Haven't heard from worker-gpu0-r1 in 5 seconds. Are they okay? ğŸ˜Ÿ"
```
---
### ERROR (Operational Failures)
**Narration Intensity**: Rich context for diagnosis
**What belongs**:
- âœ… Failed operations with context ("VRAM allocation failed: requested 4096MB, only 2048MB available")
- âœ… Validation failures ("Job validation failed: model 'invalid-model' not found in catalog")
- âœ… Network errors ("Failed to connect to pool-managerd: connection refused (retry in 1s)")
- âœ… Resource exhaustion ("GPU0 out of memory: 8192MB allocated, 0MB free")
- âœ… Timeout errors ("Job-123 timed out after 30s (max: 30s, worker: worker-gpu0-r1)")
**Editorial guidelines**:
- ğŸ“ Length: â‰¤150 characters (need room for diagnosis)
- ğŸ”— Correlation: **ALWAYS** propagate (critical for debugging!)
- ğŸ¯ Diagnosis: Include what failed, why, and relevant context
- ğŸ”’ Redaction: Especially important for error details
- ğŸ€ Cute mode: Empathetic, helpful tone
**Examples**:
```rust
// ERROR: VRAM allocation failure
human: "VRAM allocation failed on GPU0: requested 4096MB, only 2048MB available"
cute: "Oh no! GPU0 doesn't have enough room (need 4GB, only 2GB free). ğŸ˜Ÿ"
story: "\"Do you have 4GB?\" asked rbees-orcd. \"No,\" replied GPU0 sadly, \"only 2GB free.\""
// ERROR: Validation failure
human: "Job validation failed: model 'gpt-5' not found in catalog (available: llama-7b, phi-3)"
cute: "Hmm, 'gpt-5' isn't in our catalog. We have llama-7b and phi-3 though! ğŸ”"
// ERROR: Network failure
human: "Failed to connect to pool-managerd at localhost:8080: connection refused (retry in 1s)"
cute: "Couldn't reach pool-managerd! They might be napping. Trying again in 1s... ğŸ˜´"
// ERROR: Resource exhaustion
human: "GPU0 out of memory: 8192MB allocated, 0MB free (cannot load model 'llama-13b')"
cute: "GPU0 is completely full! No room for llama-13b right now. ğŸ˜°"
// ERROR: Timeout
human: "Job-123 timed out after 30s (max: 30s, worker: worker-gpu0-r1, correlation: req-abc)"
cute: "Job-123 took too long (30s limit). Worker-gpu0-r1 might be stuck! â°"
```
---
### FATAL (Unrecoverable Errors)
**Narration Intensity**: Maximum context + state dump
**What belongs**:
- âœ… Panic-worthy errors ("CRITICAL: VRAM-only policy violated on GPU0: UMA detected. Aborting.")
- âœ… Data corruption ("CRITICAL: Seal verification failed for shard 'llama-7b': digest mismatch")
- âœ… Invariant violations ("CRITICAL: Worker registry corrupted: duplicate worker_id 'worker-gpu0-r1'")
- âœ… Unrecoverable resource failures ("CRITICAL: All GPUs failed health check. Cannot continue.")
- âœ… Security violations ("CRITICAL: Unauthorized access attempt detected. Shutting down.")
**Editorial guidelines**:
- ğŸ“ Length: Unlimited (this is a crisis, tell the whole story!)
- ğŸ”— Correlation: **ALWAYS** propagate
- ğŸ¯ Context: Include EVERYTHING (state, inputs, stack trace context)
- ğŸš¨ Severity: System cannot continue
- ğŸ€ Cute mode: Still gentle, but serious
**Examples**:
```rust
// FATAL: Policy violation
human: "CRITICAL: VRAM-only policy violated on GPU0: UMA detected. Worker startup aborted."
cute: "STOP! GPU0 shares memory with CPU (UMA) â€” we need dedicated VRAM! Shutting down. ğŸ›‘"
story: "\"UMA detected!\" cried worker. \"We can't continue,\" said rbees-orcd gravely. \"Abort.\""
// FATAL: Data corruption
human: "CRITICAL: Seal verification failed for shard 'llama-7b' on GPU0: digest mismatch (expected: abc123, got: def456)"
cute: "DANGER! llama-7b's safety seal is wrong! This could be corruption! Stopping everything! ğŸš¨"
// FATAL: Invariant violation
human: "CRITICAL: Worker registry corrupted: duplicate worker_id 'worker-gpu0-r1' (existing: GPU0, new: GPU1)"
cute: "PANIC! Two workers claim to be 'worker-gpu0-r1'! This should never happen! ğŸ˜±"
// FATAL: Total resource failure
human: "CRITICAL: All GPUs failed health check (GPU0: OOM, GPU1: driver error, GPU2: offline). Cannot continue."
cute: "All GPUs are down! GPU0 is out of memory, GPU1 has driver issues, GPU2 is offline. We can't run! ğŸ’”"
// FATAL: Security violation
human: "CRITICAL: Unauthorized access attempt from IP 192.168.1.100 (invalid token). Shutting down for security."
cute: "SECURITY ALERT! Someone tried to access us without permission! Shutting down to stay safe! ğŸ”’"
```
---
## ğŸ”Œ Our Custom Tracing Integration Architecture
### Our Decision: Build Our Own! ğŸ€
We're **building custom proc macros** that integrate with `tracing` as a backend, but with our unique features on top:
**What we're building**:
- âœ… Custom `#[trace_fn]` with auto-inferred actor
- âœ… Custom `#[narrate(...)]` with template interpolation
- âœ… Compile-time editorial enforcement (â‰¤100 chars, SVO validation)
- âœ… Cute/story modes built-in (first-class, not add-on)
- âœ… Conditional compilation (zero overhead in production)
- âœ…  integration
### Original Options We Considered:
#### Option 1: Narration as Tracing Fields (Embedded)
**How it works**: Narration fields become structured fields on tracing events.
```rust
tracing::info!(
    actor = "rbees-orcd",
    action = "accept",
    target = "job-123",
    correlation_id = "req-abc",
    human = "Accepted request; queued at position 3",
    cute = "Orchestratord welcomes job-123 to the queue! ğŸ«",
    "Narration event"
);
```
**Pros**:
- âœ… Single event stream (no duplication)
- âœ… Tracing subscriber handles formatting
- âœ… Natural RUST_LOG filtering
**Cons**:
- âŒ Couples narration to tracing (we lose independence)
- âŒ Harder to enforce editorial standards (tracing doesn't know our rules)
- âŒ Can't easily swap backends (we're stuck with tracing)
**Our take**: ğŸ˜¤ **We don't love this.** We lose editorial control.
---
#### Option 2: Narration Emits Tracing Events (Wrapper)
**How it works**: `narrate()` internally calls `tracing::event!()` with our fields.
```rust
pub fn narrate(fields: NarrationFields) {
    // Apply redaction, validation, editorial rules
    let sanitized = apply_editorial_standards(fields);
    // Emit as tracing event
    tracing::info!(
        actor = sanitized.actor,
        action = sanitized.action,
        target = sanitized.target,
        human = sanitized.human,
        cute = sanitized.cute,
        correlation_id = sanitized.correlation_id,
        "Narration"
    );
}
```
**Pros**:
- âœ… We maintain editorial control (redaction, validation, â‰¤100 char enforcement)
- âœ… Single event stream (no duplication)
- âœ… Tracing subscriber handles output formatting
- âœ… RUST_LOG filtering works naturally
**Cons**:
- âš ï¸ Narration depends on tracing (acceptable trade-off)
- âš ï¸ Need to map narration levels to tracing levels
**Our take**: ğŸ€ **This is our favorite!** We keep control, you get integration.
---
#### Option 3: Parallel Streams (Independent)
**How it works**: Narration and tracing are completely separate.
```rust
// Narration event
narrate(NarrationFields {
    actor: "rbees-orcd",
    action: "accept",
    target: "job-123".to_string(),
    human: "Accepted request".to_string(),
    ..Default::default()
});
// Separate tracing event
tracing::info!(job_id = "job-123", "Job accepted");
```
**Pros**:
- âœ… Complete independence (we can swap backends)
- âœ… Full editorial control
- âœ… Narration can have different output (JSON, NDJSON, etc.)
**Cons**:
- âŒ Duplicate events (one narration, one tracing)
- âŒ Two streams to correlate
- âŒ More complex for consumers
**Our take**: ğŸ˜¤ **Too much duplication.** Operators will hate this.
---
### ğŸ† Our Implementation: Custom Proc Macros + Tracing Backend
**Why we're building our own**:
- ğŸ’ Cute mode is our **brand** â€” needs to be first-class
- ğŸ­ Story mode is **unique** â€” no other library has it
- ğŸ¨ Editorial enforcement is **our standard** â€” compile-time validation
- ğŸ”’ Security is **built-in** â€” automatic redaction
- ğŸ“Š  are **our workflow** â€” seamless integration
- ğŸ’ Brand differentiation matters
**Implementation strategy**:
```rust
// bin/shared-crates/narration-macros/src/lib.rs
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, ItemFn};
/// Our custom #[trace_fn] proc macro
#[proc_macro_attribute]
pub fn trace_fn(_attr: TokenStream, item: TokenStream) -> TokenStream {
    let input = parse_macro_input!(item as ItemFn);
    // Auto-infer actor from module path!
    let actor = infer_actor_from_module_path();
    // Conditional compilation
    #[cfg(feature = "trace-enabled")]
    {
        // Generate full tracing code with auto-inferred actor
        let expanded = generate_trace_code(&input, &actor);
        return TokenStream::from(expanded);
    }
    #[cfg(not(feature = "trace-enabled"))]
    {
        // Production: return original function unchanged
        return TokenStream::from(quote! { #input });
    }
}
/// Our custom #[narrate(...)] proc macro with template interpolation
#[proc_macro_attribute]
pub fn narrate(attr: TokenStream, item: TokenStream) -> TokenStream {
    let input = parse_macro_input!(item as ItemFn);
    let args = parse_macro_input!(attr as NarrateArgs);
    // Validate at compile time!
    if args.human.len() > 100 {
        return syn::Error::new_spanned(
            &args.human,
            "human field exceeds 100 character limit (ORCH-3305)"
        ).to_compile_error().into();
    }
    // Generate code with template interpolation
    let expanded = generate_narration_with_templates(&input, &args);
    TokenStream::from(expanded)
}
```
```rust
// narration-core/src/lib.rs
use tracing::Level;
/// Emit narration as tracing event (used by our proc macros)
pub fn narrate(mut fields: NarrationFields) {
    // Apply editorial standards
    enforce_length_limits(&mut fields);  // â‰¤100 chars for INFO
    apply_redaction(&mut fields);         // Automatic secret redaction
    validate_correlation_id(&fields);     // Ensure tracking
    // Emit as tracing event with structured fields
    tracing::info!(
        actor = fields.actor,
        action = fields.action,
        target = %fields.target,
        human = %fields.human,
        cute = fields.cute.as_deref(),
        story = fields.story.as_deref(),
        correlation_id = fields.correlation_id.as_deref(),
        // ... all other fields
    );
}
```
**Benefits of our custom implementation**:
- âœ… **Auto-inferred actor** from module path (zero boilerplate!)
- âœ… **Template interpolation** for human/cute/story fields
- âœ… **Compile-time editorial enforcement** (â‰¤100 chars, SVO validation)
- âœ… **Cute/story modes built-in** (first-class, not add-on)
- âœ… **Conditional compilation** (code removed in production)
- âœ… We enforce â‰¤100 character limit before tracing sees it
- âœ… We apply redaction before tracing sees it
- âœ… We validate correlation IDs before tracing sees it
- âœ… RUST_LOG filtering works: `RUST_LOG=info` shows INFO+ narration
- âœ… Single event stream (no duplication)
- âœ… Tracing subscriber formats output (JSON, console, etc.)
- âœ… **Brand differentiation** (uniquely "us")
---
## ğŸš¨ Pitfalls We've Seen (And How to Avoid Them)
### Pitfall 1: Level Confusion
**Problem**: Teams use INFO for internal details, DEBUG for lifecycle events.
**Example (WRONG)**:
```rust
// âŒ This is DEBUG, not INFO!
tracing::info!("Loop iteration 47/100 processing worker-gpu2-r1");
// âŒ This is INFO, not DEBUG!
tracing::debug!("Job completed in 2500ms");
```
**Fix**: Use our level boundaries!
```rust
// âœ… CORRECT: DEBUG for internal flow
narrate_debug!(human = "Processing worker 47/100: worker-gpu2-r1 (status=idle)");
// âœ… CORRECT: INFO for lifecycle
narrate_info!(human = "Job completed in 2500 ms (150 tokens, 60 tokens/sec)");
```
---
### Pitfall 2: Missing Correlation IDs
**Problem**: Teams forget to propagate correlation IDs, breaking request tracing.
**Example (WRONG)**:
```rust
// âŒ No correlation_id!
narrate_info!(
    actor = "rbees-orcd",
    action = "dispatch",
    target = "job-123",
    human = "Dispatching job to worker"
);
```
**Fix**: ALWAYS propagate correlation IDs!
```rust
// âœ… CORRECT: Correlation ID propagated
narrate_info!(
    actor = "rbees-orcd",
    action = "dispatch",
    target = "job-123",
    correlation_id = req_id, // ALWAYS INCLUDE THIS!
    human = "Dispatching job to worker-gpu0-r1"
);
```
---
### Pitfall 3: Vague Error Messages
**Problem**: ERROR events lack context for diagnosis.
**Example (WRONG)**:
```rust
// âŒ Too vague!
narrate_error!(human = "Allocation failed");
```
**Fix**: Include what, why, and context!
```rust
// âœ… CORRECT: Rich context
narrate_error!(
    human = "VRAM allocation failed on GPU0: requested 4096MB, only 2048MB available",
    error_kind = "vram_exhaustion",
    requested_mb = 4096,
    available_mb = 2048,
    device = "GPU0"
);
```
---
### Pitfall 4: TRACE in Production
**Problem**: Teams enable TRACE in production, overwhelming logs.
**Example (WRONG)**:
```bash
# âŒ WRONG: TRACE will kill performance
export RUST_LOG=trace
```
**Fix**: Use INFO for production, DEBUG for incidents!
```bash
# âœ… CORRECT: INFO for production
export RUST_LOG=info
# âœ… CORRECT: DEBUG for incident investigation
export RUST_LOG=debug
# âœ… CORRECT: TRACE for specific module only
export RUST_LOG=info,llama_orch::worker::inference=trace
```
---
### Pitfall 5: Ignoring Length Limits
**Problem**: Teams write 200-character INFO messages that overwhelm operators.
**Example (WRONG)**:
```rust
// âŒ Way too long! (180 characters)
narrate_info!(
    human = "Accepted inference request for model 'llama-7b' with max_tokens=150, temperature=0.7, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.3, and queued at position 3"
);
```
**Fix**: Stay under 100 characters for INFO!
```rust
// âœ… CORRECT: Concise (72 characters)
narrate_info!(
    human = "Accepted request; queued at position 3 (ETA 420 ms) on pool 'default'",
    model_ref = "llama-7b",
    max_tokens = 150,
    // Other params as structured fields, not in human!
);
```
---
## ğŸ“ What We're Building (4-Week Plan)
### Implementation Timeline
**Week 1**: Core proc macro crate
- Custom `#[trace_fn]` with auto-inferred actor
- Custom `#[narrate(...)]` with template interpolation
- Conditional compilation support
**Week 2**: Narration core enhancements
- Add WARN/ERROR/FATAL levels
- Lightweight trace macros (trace_tiny!, trace_loop!, etc.)
- Secret redaction (regex-based, cached)
- Integrate with `tracing` backend
**Week 3**: Editorial enforcement
- Compile-time validation (â‰¤100 chars, SVO structure)
- Feature flags (trace-enabled, debug-enabled, cute-mode, production)
- Helpful compile errors for violations
**Week 4**: Integration & testing
- BDD tests for cute/story modes
-  integration
- Migrate services (rbees-orcd â†’ pool-managerd â†’ worker-orcd)
- Update CI/CD pipelines
### What We'll Deliver
#### 1. Editorial Annotations
- âœ… Level boundary validation ("This belongs in DEBUG, not INFO")
- âœ… Length limit enforcement ("This is 127 chars, needs to be â‰¤100")
- âœ… Correlation ID discipline ("Missing correlation_id here!")
- âœ… Context richness ("Add the error_kind and device fields")
#### 2. Cute Story Examples
For every WARN/ERROR case you define, we'll provide:
- ğŸ“¢ **human**: Professional, concise, context-rich
- ğŸ€ **cute**: Whimsical, empathetic, emoji-enhanced
- ğŸ­ **story**: Dialogue-based (when it makes sense)
#### 3. Integration Guidance
- ğŸ”Œ How to wire narration into your tracing subscriber
- ğŸšï¸ RUST_LOG patterns for different environments
- ğŸ§ª Testing strategies for each level
- ğŸ“Š Performance impact analysis
#### 4. Anti-Pattern Catalog
- ğŸš¨ Common mistakes we've seen
- âœ… Fixes with code examples
- ğŸ“‹ Checklist for code review
---
## ğŸ€ Example: WARN/ERROR Cute Stories (Built Into Our System!)
### WARN Examples
**Retry scenario**:
```rust
// WARN: Retry attempt
human: "Retrying job-123 after timeout (attempt 2/5, backoff: 200ms)"
cute: "Job-123 didn't make it through. Let's try again! Attempt 2... ğŸ”„"
story: "\"Timeout!\" said worker. \"Let me try again,\" offered rbees-orcd patiently."
```
**Performance degradation**:
```rust
// WARN: Slow worker
human: "Worker-gpu0-r1 latency: 500ms (expected <100ms, threshold exceeded)"
cute: "Worker-gpu0-r1 is running a bit slow today (500ms). Might need a checkup! ğŸŒ"
story: "\"You're slower than usual,\" observed pool-managerd. \"I know,\" sighed worker-gpu0-r1."
```
**Deprecated feature**:
```rust
// WARN: Deprecated flag
human: "Using deprecated flag --legacy-mode; migrate to --compat-mode by v2.0"
cute: "Psst! --legacy-mode is getting old. Switch to --compat-mode soon! ğŸ“¢"
story: "\"Still using --legacy-mode?\" asked rbees-orcd. \"Yeah, I should update,\" admitted the config."
```
### ERROR Examples
**VRAM exhaustion**:
```rust
// ERROR: Out of memory
human: "VRAM allocation failed on GPU0: requested 4096MB, only 2048MB available"
cute: "Oh no! GPU0 doesn't have enough room (need 4GB, only 2GB free). ğŸ˜Ÿ"
story: "\"Do you have 4GB?\" asked rbees-orcd. \"No,\" replied GPU0 sadly, \"only 2GB free.\""
```
**Model not found**:
```rust
// ERROR: Validation failure
human: "Job validation failed: model 'gpt-5' not found in catalog (available: llama-7b, phi-3)"
cute: "Hmm, 'gpt-5' isn't in our catalog. We have llama-7b and phi-3 though! ğŸ”"
story: "\"Do you have gpt-5?\" asked the client. \"No,\" replied rbees-orcd, \"but I have llama-7b and phi-3!\""
```
**Network failure**:
```rust
// ERROR: Connection refused
human: "Failed to connect to pool-managerd at localhost:8080: connection refused (retry in 1s)"
cute: "Couldn't reach pool-managerd! They might be napping. Trying again in 1s... ğŸ˜´"
story: "\"Hello?\" called rbees-orcd. Silence. \"I'll try again soon,\" it decided."
```
### FATAL Examples
**Policy violation**:
```rust
// FATAL: VRAM-only violation
human: "CRITICAL: VRAM-only policy violated on GPU0: UMA detected. Worker startup aborted."
cute: "STOP! GPU0 shares memory with CPU (UMA) â€” we need dedicated VRAM! Shutting down. ğŸ›‘"
story: "\"UMA detected!\" cried worker. \"We can't continue,\" said rbees-orcd gravely. \"Abort.\""
```
**Data corruption**:
```rust
// FATAL: Seal verification failed
human: "CRITICAL: Seal verification failed for shard 'llama-7b' on GPU0: digest mismatch (expected: abc123, got: def456)"
cute: "DANGER! llama-7b's safety seal is wrong! This could be corruption! Stopping everything! ğŸš¨"
story: "\"The seal doesn't match!\" gasped worker. \"Corruption?\" asked rbees-orcd. \"Possibly. Abort!\""
```
---
## âœ… Our Recommendations Summary
### 1. Level Mapping
âœ… **APPROVED** with minor updates:
- Add WARN, ERROR, FATAL to our spec (we're doing this now)
- Keep INFO as narration backbone (perfect!)
- TRACE for opt-in ultra-fine detail (exactly right)
### 2. Tracing Integration
ğŸ€ **RECOMMENDED**: Option 2 (Wrapper approach)
- Narration wraps tracing events
- We maintain editorial control
- Single event stream
- RUST_LOG filtering works naturally
### 3. Editorial Standards
ğŸ’ **ENFORCED**:
- INFO: â‰¤100 characters (strict)
- WARN: â‰¤120 characters (actionable context)
- ERROR: â‰¤150 characters (diagnostic context)
- FATAL: Unlimited (full state dump)
- ALL levels: Correlation IDs required
- ALL levels: Secret redaction enforced
### 4. Cute Mode
ğŸ€ **ALWAYS AVAILABLE**:
- Every level gets cute narration
- Even FATAL can be gentle
- Emoji-enhanced, empathetic tone
- Story mode for dialogue (optional)
---
## ğŸ“… Next Steps
### What We Need From You
1. **Draft Policy Document**
   - Send us your proposed logging level policy
   - Include example scenarios for each level
   - We'll provide annotated feedback within 48 hours
2. **Integration Requirements**
   - Which tracing subscriber are you using? (tracing-subscriber, tracing-bunyan, etc.)
   - What output format? (JSON, console, NDJSON?)
   - Any custom fields beyond our taxonomy?
3. **Rollout Plan**
   - Which services first? (rbees-orcd, pool-managerd, worker-orcd?)
   - Timeline for migration?
   - Backward compatibility needs?
### What We'll Provide
1. **Updated LOGGING_LEVELS.md**
   - Add WARN, ERROR, FATAL levels
   - Include tracing integration guidance
   - Provide cute examples for all levels
2. **Tracing Integration Module**
   - `narration-core/src/tracing_integration.rs`
   - Wrapper functions: `narrate_trace!()`, `narrate_debug!()`, etc.
   - Level mapping and editorial enforcement
3. **Editorial Review**
   - Annotated feedback on your draft policy
   - Cute story examples for WARN/ERROR cases
   - Anti-pattern catalog with fixes
4. **Migration Guide**
   - Step-by-step integration instructions
   - Code examples for each service
   - Testing strategies
---
## ğŸ’ Final Thoughts
We're **thrilled** that you're taking logging levels seriously and aligning them with narration intensity. Your proposed mapping is **excellent**, and we're excited to collaborate on making this the **most debuggable distributed system in existence**.
A few parting thoughts:
### What We Love â¤ï¸
- âœ… INFO as the narration backbone (FINALLY!)
- âœ… WARN/ERROR with rich context (music to our ears!)
- âœ… TRACE for opt-in ultra-fine detail (perfect boundary)
- âœ… Your commitment to correlation IDs (we're so proud!)
### What We're Excited About ğŸ‰
- ğŸ€ Cute mode at ALL levels (even FATAL!)
- ğŸ”Œ Tracing integration (we'll make it seamless)
- ğŸ“š Comprehensive examples (we'll provide dozens)
- ğŸš€ Rollout across all services (narration everywhere!)
### What We're Watching For ğŸ‘€
- âš ï¸ Level confusion (we'll catch it in review)
- âš ï¸ Missing correlation IDs (we'll enforce it)
- âš ï¸ Vague error messages (we'll demand context)
- âš ï¸ TRACE in production (we'll stop you!)
---
## ğŸ¤ Let's Do This!
We're building a **world-class, uniquely branded narration system** with:
- ğŸ€ Custom proc macros (`#[trace_fn]`, `#[narrate(...)]`)
- ğŸ­ Auto-inferred actors (from module path)
- ğŸ¨ Template interpolation (for human/cute/story)
- ğŸ”’ Compile-time editorial enforcement
- ğŸ“Š Conditional compilation (zero overhead in production)
- ğŸ’ Cute/story modes built-in (first-class!)
We have **ultimate editorial authority** over `human` fields, and we take that responsibility **seriously** (but adorably). We're building something that's **uniquely ours** because:
**Cuteness pays the bills!** ğŸ’•
Generic tracing is boring. We're making llama-orch logs the **most delightful debugging experience** in distributed systems.
---
**With love, sass, and the confidence that cuteness pays the bills,**  
**The Narration Core Team** ğŸ­ğŸ€
*P.S. â€” We're not using generic `tracing::instrument`. We're building a **cute, story-telling, editorially-enforced** narration system that's uniquely ours. Because boring is for other people. ğŸ’*
---
**Deliverables**:
- ğŸ“„ LOGGING_LEVELS.md (updated with WARN/ERROR/FATAL)
- ğŸ“„ ERGONOMIC_TRACING.md (custom proc macro design)
- ğŸ“„ CONDITIONAL_COMPILATION.md (zero overhead in production)
- ğŸ“„ EXISTING_SOLUTIONS.md (why we're building our own)
- ğŸ“„ FINAL_SUMMARY.md (complete 4-week plan)
- ğŸ“„ Custom proc macro crate (`observability-narration-macros`)
*May your levels be clear, your correlation IDs present, your actors be auto-inferred, and your narration be adorable! ğŸ€*
