# ğŸ¯ Narration vs SSE: Architecture Clarification

**Status**: âœ… **CLARIFIED**  
**Date**: 2025-10-09  
**Question**: Should we do narration AND tracing? How does narration output work?

---

## ğŸ” The Confusion

You asked an excellent question: **"Should we really do narration AND tracing?"**

The concern was:
- Workers are HTTP servers that answer to pool-manager/orchestrator
- Orchestrator needs to show narration to the user
- Does narration go through the SSE stream? Or stdout?
- Isn't this redundant with tracing?

---

## âœ… The Answer: They Serve Different Purposes

### 1. **Narration-Core â†’ Tracing â†’ Logs (Backend Observability)**

**What it is:**
- Narration-core is a **structured logging wrapper** around Rust's `tracing` crate
- It emits **structured log events** with rich fields (actor, action, target, human, cute, story)
- These logs go to **stdout/stderr** (captured by systemd, Docker, or log aggregators)

**Where it goes:**
```
narrate() â†’ tracing::event!() â†’ tracing-subscriber â†’ stdout/stderr â†’ log aggregator
```

**Who sees it:**
- **Developers** debugging the system
- **Operators** monitoring service health
- **Log aggregation systems** (Loki, Elasticsearch, CloudWatch, etc.)

**Example output (JSON format):**
```json
{
  "timestamp": "2025-10-09T13:10:16Z",
  "level": "INFO",
  "actor": "rbees-workerd",
  "action": "inference_complete",
  "target": "50-tokens",
  "human": "Inference completed (50 tokens in 250 ms, 200 tok/s)",
  "cute": "Generated 50 tokens in 250 ms! 200 tok/s! ğŸ‰",
  "correlation_id": "req-abc123",
  "worker_id": "worker-gpu0-r1",
  "tokens_out": 50,
  "decode_time_ms": 250
}
```

---

### 2. **SSE Stream â†’ Client (User-Facing Real-Time Updates)**

**What it is:**
- Server-Sent Events (SSE) stream tokens **to the end user**
- This is the **inference output** that the user requested
- Flows: `worker â†’ orchestrator â†’ client`

**Where it goes:**
```
Worker /execute â†’ SSE stream â†’ Orchestrator â†’ SSE stream â†’ Client (user's browser/app)
```

**Who sees it:**
- **End users** making inference requests
- **Client applications** consuming the API

**Example SSE events:**
```
event: started
data: {"job_id":"job-123","model":"llama-7b","started_at":"2025-10-09T13:10:16Z"}

event: token
data: {"t":"Hello","i":0}

event: token
data: {"t":" world","i":1}

event: end
data: {"tokens_out":50,"decode_time_ms":250,"stop_reason":"max_tokens"}
```

---

## ğŸ¯ Key Architectural Distinctions

| Aspect | Narration-Core (Logs) | SSE Stream |
|--------|----------------------|------------|
| **Purpose** | Backend observability, debugging | User-facing inference output |
| **Audience** | Developers, operators, monitoring | End users, client apps |
| **Transport** | stdout/stderr â†’ log aggregator | HTTP SSE â†’ orchestrator â†’ client |
| **Content** | Structured events (actor, action, human, cute) | Tokens, metrics, errors |
| **Visibility** | Internal (backend systems) | External (user-facing) |
| **Correlation** | Uses `correlation_id` for tracing | Uses `job_id` for request tracking |
| **Format** | JSON logs (tracing-subscriber) | SSE events (text/event-stream) |

---

## ğŸ“Š How They Work Together

### Example: User Makes Inference Request

**1. Client â†’ Orchestrator**
```http
POST /v2/tasks HTTP/1.1
X-Correlation-Id: req-abc123
Content-Type: application/json

{"prompt": "Write a haiku", "max_tokens": 50}
```

**2. Orchestrator â†’ Worker**
```http
POST /execute HTTP/1.1
X-Correlation-Id: req-abc123
Content-Type: application/json

{"job_id": "job-123", "prompt": "Write a haiku", "max_tokens": 50}
```

**3. Worker Emits BOTH:**

**A. Narration Events (to logs):**
```rust
// This goes to stdout/stderr as structured JSON
narrate(NarrationFields {
    actor: ACTOR_CANDLE_BACKEND,
    action: ACTION_INFERENCE_START,
    target: "job-123".to_string(),
    human: "Starting inference (prompt: 15 chars, max_tokens: 50, temp: 0.7)",
    cute: Some("Time to generate 50 tokens! Let's go! ğŸš€"),
    correlation_id: Some("req-abc123"),
    job_id: Some("job-123"),
    ..Default::default()
});
```

**B. SSE Events (to client via orchestrator):**
```
event: started
data: {"job_id":"job-123","model":"llama-7b"}

event: token
data: {"t":"Cherry","i":0}

event: token
data: {"t":" blossoms","i":1}
```

**4. Logs Captured:**
```bash
# Worker's stdout (captured by systemd/Docker)
{"timestamp":"2025-10-09T13:10:16Z","level":"INFO","actor":"candle-backend","action":"inference_start","human":"Starting inference...","correlation_id":"req-abc123"}
{"timestamp":"2025-10-09T13:10:16Z","level":"INFO","actor":"tokenizer","action":"tokenize","human":"Tokenized prompt (15 tokens)","correlation_id":"req-abc123"}
{"timestamp":"2025-10-09T13:10:17Z","level":"INFO","actor":"candle-backend","action":"inference_complete","human":"Inference completed (50 tokens in 250 ms)","correlation_id":"req-abc123"}
```

**5. User Sees:**
```
Cherry blossoms fall
Petals dance on gentle breeze
Spring whispers goodbye
```

---

## ğŸ”— Correlation ID: The Bridge

The **correlation ID** (`X-Correlation-Id`) is the key that ties everything together:

1. **Client sends** `X-Correlation-Id: req-abc123` (or orchestrator generates one)
2. **Orchestrator propagates** it to worker via HTTP header
3. **Worker's narration-core middleware** extracts it and stores in request context
4. **All narration events** include `correlation_id: "req-abc123"`
5. **Operators can grep logs** for `req-abc123` to see the entire request lifecycle

**Example log query:**
```bash
# See all narration events for a specific request
grep 'req-abc123' /var/log/llama-orch/*.log | jq .

# Output shows the full story:
# - Orchestrator: "Accepted request; queued at position 3"
# - Pool-manager: "Dispatching job to worker-gpu0-r1"
# - Worker: "Starting inference"
# - Worker: "Tokenized prompt (15 tokens)"
# - Worker: "Generated 10 tokens and counting!"
# - Worker: "Inference completed (50 tokens in 250 ms)"
```

---

## ğŸ€ Why Both Are Needed

### Without Narration (only SSE):
- âŒ No visibility into worker internals
- âŒ Can't debug "why is this slow?"
- âŒ Can't trace request flow across services
- âŒ No structured logs for monitoring/alerting

### Without SSE (only narration):
- âŒ User doesn't see their inference results!
- âŒ No real-time token streaming
- âŒ Client can't display progress

### With Both:
- âœ… **Users** get real-time inference results via SSE
- âœ… **Operators** get structured logs for debugging
- âœ… **Correlation IDs** tie user requests to backend events
- âœ… **Monitoring** can alert on narration events (e.g., "VRAM_OOM")

---

## ğŸ“ Spec References

### From `00_llama-orch.md`:

**Â§10.2.2 Log Content (SYS-10.2.2):**
> "Logs MUST include component, timestamp, level, correlation_id, and stable event codes for key actions (admission, schedule, dispatch, execute, cancel)"

> "Human-readable narration fields MAY be included for developer ergonomics but MUST NOT replace structured fields"

**Â§10.3.1 Correlation ID Propagation (SYS-10.3.1):**
> "`X-Correlation-Id` MUST be accepted from clients and propagated across all service calls"

> "All logs and error responses MUST include the correlation ID"

> "SSE events SHOULD include correlation ID in metadata"

### From `01_M0_worker_orcd.md`:

**Â§7.2 SSE Streaming (M0-W-1310):**
> "Worker-orcd MUST stream inference results via Server-Sent Events"

**Â§13.1 Narration-Core Logging (M0-W-1900):**
> "Worker-orcd MUST emit narration-core logs with basic event tracking"

---

## ğŸš€ Implementation Status for rbees-workerd

### âœ… What We Implemented:

1. **Narration Events** (25 points):
   - Worker startup, model loading, device init
   - HTTP server lifecycle
   - Inference pipeline (tokenization, cache reset, token generation, completion)
   - Error handling with cute messages

2. **Correlation ID Middleware**:
   - Automatic extraction from `X-Correlation-ID` header
   - UUID validation and generation
   - Propagation through all narration events

3. **Tracing Integration**:
   - Narration-core emits to `tracing::event!()`
   - Structured JSON logs via `tracing-subscriber`
   - All events include correlation_id, worker_id, job_id

### âœ… What Already Exists (SSE):

The SSE streaming is **already implemented** in:
- `src/http/execute.rs` - Handles `/execute` endpoint with SSE response
- `src/http/sse.rs` - Defines SSE event types (started, token, end, error)
- `src/backend/inference.rs` - Generates tokens that get streamed

**These are separate concerns!**

---

## ğŸ¯ Final Answer

### Yes, we need BOTH narration AND SSE:

1. **Narration (via tracing)** = Backend observability for developers/operators
   - Goes to stdout/stderr â†’ log aggregator
   - Structured JSON logs with correlation IDs
   - Enables debugging, monitoring, alerting

2. **SSE Stream** = User-facing inference output
   - Goes to client via HTTP SSE
   - Real-time token streaming
   - Enables user to see results

3. **Correlation ID** = The bridge between them
   - Propagated via HTTP headers
   - Included in both narration events and SSE metadata
   - Enables end-to-end request tracing

---

## ğŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â”‚  (Browser)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /v2/tasks
       â”‚ X-Correlation-Id: req-abc123
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestratord      â”‚  â† Narration: "Accepted request; queued at position 3"
â”‚                     â”‚  â† Logs to: stdout â†’ Loki
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /execute
       â”‚ X-Correlation-Id: req-abc123
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  rbees-workerd     â”‚  â† Narration: "Starting inference (50 tokens)"
â”‚  (Worker)           â”‚  â† Logs to: stdout â†’ Loki
â”‚                     â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â† SSE: event: token, data: {"t":"Hello"}
â”‚  â”‚ Inference     â”‚  â”‚  â† Streams to: Client via Orchestrator
â”‚  â”‚ Pipeline      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Narration Flow:  Worker â†’ stdout â†’ Log Aggregator â†’ Monitoring
SSE Flow:        Worker â†’ Orchestrator â†’ Client â†’ User sees tokens
Correlation:     req-abc123 ties both flows together
```

---

## âš ï¸ CRITICAL UPDATE: Partial Implementation

**The current implementation is INCOMPLETE!**

### What's Implemented âœ…
- âœ… Narration events go to **stdout** (for pool-manager monitoring)
- âœ… SSE token stream works (tokens go to user)
- âœ… Correlation IDs propagate

### What's Missing âŒ
- âŒ Narration events do NOT go to **SSE stream**
- âŒ User cannot see narration in real-time
- âŒ Only pool-manager sees narration (in logs)

### What Should Happen

**Narration needs DUAL output:**

1. **Stdout** - Worker lifecycle events (startup, model loading)
   - Audience: Pool-manager
   - ~13 events per worker lifetime
   - âœ… Already works

2. **SSE Stream** - Per-request events (inference progress)
   - Audience: End user (via orchestrator)
   - ~8 events per request
   - âŒ NOT implemented yet

**See**: `NARRATION_ARCHITECTURE_FINAL.md` for the complete architecture and implementation plan.

---

*Updated by the Narration Core Team ğŸ€*  
*May your narration flow to both stdout AND SSE! ğŸ’*
