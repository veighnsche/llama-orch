{
	"architectures": ["PhiForCausalLM"],
	"model_type": "phi",
	"vocab_size": 51200,
	"hidden_size": 2560,
	"intermediate_size": 10240,
	"num_hidden_layers": 32,
	"num_attention_heads": 32,
	"num_key_value_heads": 32,
	"max_position_embeddings": 2048,
	"layer_norm_eps": 1e-5,
	"rope_theta": 10000.0,
	"partial_rotary_factor": 0.4,
	"qk_layernorm": false,
	"tie_word_embeddings": false
}
