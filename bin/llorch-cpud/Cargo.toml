[package]
name = "llorch-cpud"
version = "0.1.0"
edition = "2021"
authors = ["TEAM CASCADE"]
description = "CPU-based GPT-2 inference worker daemon"

[workspace]
# This is a standalone workspace

[dependencies]
# VERIFIED REUSABLE: worker-crates (99% reusable, ~4,198 lines)
# Audit: WORKER_CRATES_REUSABILITY_AUDIT.md

# worker-common: 100% reusable (~1,100 lines)
# Provides: SamplingConfig, InferenceResult, WorkerError, startup callbacks
worker-common = { path = "../worker-crates/worker-common" }

# worker-http: 100% reusable (~771 lines)
# Provides: HTTP server, routes, SSE streaming, InferenceBackend trait
worker-http = { path = "../worker-crates/worker-http" }

# worker-tokenizer: 100% reusable (~1,200 lines)
# Provides: Pure Rust BPE tokenization, GGUF and HuggingFace support
worker-tokenizer = { path = "../worker-crates/worker-tokenizer" }

# worker-models: 100% reusable (~800 lines)
# Provides: GPTConfig, ModelAdapter trait, architecture configs
worker-models = { path = "../worker-crates/worker-models" }

# CPU tensor operations (NEW - ~500 lines to implement)
ndarray = "0.15"
ndarray-linalg = { version = "0.16", optional = true }

# Async runtime (already in worker-http)
tokio = { version = "1", features = ["full"] }
async-trait = "0.1"

# Utilities
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json"] }
clap = { version = "4", features = ["derive"] }
serde = { version = "1.0", features = ["derive"] }

[dev-dependencies]
# Testing utilities
approx = "0.5"

[features]
default = []
# Enable BLAS/LAPACK for faster matrix operations
blas = ["ndarray-linalg"]
# Enable performance benchmarking
benchmark = []

[[bin]]
name = "llorch-cpud"
path = "src/main.rs"

[lib]
name = "llorch_cpud"
path = "src/lib.rs"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
strip = true

[profile.dev]
opt-level = 0
debug = true
