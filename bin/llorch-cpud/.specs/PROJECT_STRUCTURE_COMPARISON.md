# Project Structure Comparison: Candle vs Mistral.rs vs llorch-cpud

**Date:** 2025-10-08  
**Purpose:** Understand architectural decisions across three Rust ML projects  
**Status:** Analysis

---

## Executive Summary

| Project | Purpose | Architecture | Key Insight |
|---------|---------|--------------|-------------|
| **Candle** | ML Framework (like PyTorch) | Library-first, multi-backend | Provides building blocks, not applications |
| **Mistral.rs** | Production Inference Server | Application-first, feature-rich | Complete inference solution with many models |
| **llorch-cpud** | Single-Model Worker Daemon | Worker-first, minimal | One model, one process, HTTP API |

---

## 1. Candle: The ML Framework

### Purpose
**"PyTorch for Rust"** - A machine learning framework, not an inference server.

### Structure
```
candle/
â”œâ”€â”€ candle-core/              # Tensor operations (like torch.Tensor)
â”‚   â”œâ”€â”€ backend.rs            # CPU/CUDA/Metal backends
â”‚   â”œâ”€â”€ tensor.rs             # Core tensor type
â”‚   â”œâ”€â”€ op.rs                 # Operations (matmul, conv, etc.)
â”‚   â””â”€â”€ device.rs             # Device abstraction
â”œâ”€â”€ candle-nn/                # Neural network layers (like torch.nn)
â”‚   â”œâ”€â”€ layer_norm.rs
â”‚   â”œâ”€â”€ linear.rs
â”‚   â”œâ”€â”€ conv.rs
â”‚   â””â”€â”€ ...
â”œâ”€â”€ candle-transformers/      # Transformer models (like transformers library)
â”‚   â”œâ”€â”€ models/               # 100+ model implementations
â”‚   â”‚   â”œâ”€â”€ llama.rs
â”‚   â”‚   â”œâ”€â”€ gpt2.rs
â”‚   â”‚   â”œâ”€â”€ mistral.rs
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ generation/           # Text generation utilities
â””â”€â”€ candle-examples/          # Example applications
    â”œâ”€â”€ llama/                # Example: Run Llama
    â”œâ”€â”€ gpt2/                 # Example: Run GPT-2
    â””â”€â”€ ...
```

### Key Insights

**1. Library, Not Application**
- Candle is a **framework** like PyTorch
- You build applications **on top** of Candle
- No HTTP server, no worker daemon
- Just provides tensor operations and model implementations

**2. Multi-Backend from Day 1**
- `candle-core` abstracts CPU/CUDA/Metal
- Same code runs on any backend
- Backend is a runtime choice, not compile-time

**3. Model Implementations are Examples**
- `candle-transformers/models/` has 100+ models
- These are **reference implementations**
- Users copy/modify for their needs
- Not production-ready servers

**4. VarBuilder Pattern**
- Uses `VarBuilder` for weight loading
- `.pp("layer")` = "push prefix" (like cd into directory)
- Matches PyTorch's `state_dict` naming exactly
- Example: `vb.pp("transformer").pp("h").pp("0")` â†’ `transformer.h.0.*`

**Why This Structure:**
- âœ… Reusable across many applications
- âœ… Easy to add new models
- âœ… Backend-agnostic
- âŒ Not a production server
- âŒ No HTTP API
- âŒ No worker management

---

## 2. Mistral.rs: The Production Server

### Purpose
**"Complete inference solution"** - Production-ready server with many features.

### Structure
```
mistral.rs/
â”œâ”€â”€ mistralrs-core/           # Core inference engine
â”‚   â”œâ”€â”€ models/               # 24 model families
â”‚   â”‚   â”œâ”€â”€ llama.rs
â”‚   â”‚   â”œâ”€â”€ mistral.rs
â”‚   â”‚   â”œâ”€â”€ phi3.rs
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ pipeline/             # Inference pipelines
â”‚   â”œâ”€â”€ engine/               # Request scheduling
â”‚   â”œâ”€â”€ kv_cache/             # KV cache management
â”‚   â”œâ”€â”€ attention/            # Attention implementations
â”‚   â”œâ”€â”€ layers.rs             # Common layers (90KB file!)
â”‚   â”œâ”€â”€ sampler.rs            # Sampling strategies
â”‚   â””â”€â”€ scheduler/            # Request scheduling
â”œâ”€â”€ mistralrs-vision/         # Vision model support
â”œâ”€â”€ mistralrs-quant/          # Quantization (ISQ, GGUF, GPTQ, AWQ)
â”œâ”€â”€ mistralrs-paged-attn/     # Paged attention (like vLLM)
â”œâ”€â”€ mistralrs-server/         # HTTP server + CLI
â”‚   â”œâ”€â”€ main.rs               # Server entry point
â”‚   â””â”€â”€ ...
â”œâ”€â”€ mistralrs-pyo3/           # Python bindings
â””â”€â”€ examples/                 # Usage examples
```

### Key Insights

**1. Application, Not Library**
- Mistral.rs is a **complete server**
- Has HTTP API, CLI, Python bindings
- Production-ready out of the box
- Users run it, not build on it

**2. Feature-Rich**
- Supports 24+ model families
- Quantization (GGUF, GPTQ, AWQ, FP8, HQQ)
- Paged attention (memory optimization)
- Vision models (multimodal)
- Diffusion models (image generation)
- Speech models
- LoRA/X-LoRA adapters
- Mixture of Experts (AnyMoE)
- Tool calling
- Streaming

**3. Monolithic Core**
- `layers.rs` is 90KB (2000+ lines)
- Everything in one place
- Not modular like Candle
- Optimized for completeness, not reusability

**4. Built on Candle**
- Uses `candle-core` for tensors
- Uses `candle-nn` for layers
- Adds production features on top
- VarBuilder pattern for weight loading

**5. Request Scheduling**
- Has `engine/` for request management
- Has `scheduler/` for batching
- Has `sequence.rs` for tracking requests
- Production-grade orchestration

**Why This Structure:**
- âœ… Complete solution
- âœ… Production-ready
- âœ… Many features
- âœ… Easy to deploy
- âŒ Hard to customize
- âŒ Monolithic
- âŒ Not a framework

---

## 3. llorch-cpud: The Minimal Worker

### Purpose
**"Single-model worker daemon"** - One model, one process, HTTP API, orchestrated by pool-managerd.

### Structure
```
llorch-cpud/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs               # HTTP server (uses worker-http)
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â””â”€â”€ cpu_backend.rs    # InferenceBackend impl
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ gpt2.rs           # ONE model (GPT-2)
â”‚   â”œâ”€â”€ layers/               # Pure implementations
â”‚   â”‚   â”œâ”€â”€ layer_norm.rs
â”‚   â”‚   â”œâ”€â”€ embedding.rs
â”‚   â”‚   â”œâ”€â”€ attention/        # Split into focused files
â”‚   â”‚   â”‚   â”œâ”€â”€ qkv.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ scores.rs
â”‚   â”‚   â”‚   â””â”€â”€ output.rs
â”‚   â”‚   â”œâ”€â”€ ffn.rs
â”‚   â”‚   â””â”€â”€ transformer.rs
â”‚   â”œâ”€â”€ cache/                # Top-level (room to grow)
â”‚   â”‚   â””â”€â”€ kv_cache.rs
â”‚   â””â”€â”€ tensor/
â”‚       â””â”€â”€ ops.rs            # CPU tensor operations
â””â”€â”€ tests/
    â””â”€â”€ checkpoint_*.rs       # Validation tests
```

### Key Insights

**1. Worker, Not Server**
- llorch-cpud is a **worker daemon**
- Spawned by pool-managerd
- One model per process
- HTTP API for orchestration
- Not user-facing

**2. Minimal and Focused**
- ONE model (GPT-2)
- ONE backend (CPU)
- ONE purpose (inference)
- No quantization, no vision, no diffusion
- Just text generation

**3. Checkpoint-Driven Development**
- 13 validation checkpoints
- Each checkpoint validates one component
- Reference implementations (tinygrad, Candle, Mistral.rs)
- Incremental validation

**4. Modular Layers**
- Each layer in its own file
- Attention split into 4 files (qkv, cache, scores, output)
- Cache is top-level (room for optimization)
- Clean separation

**5. Worker-Crates Pattern**
- Reuses worker-http, worker-common, worker-tokenizer, worker-models
- HTTP server is external (worker-http)
- Model implementation is internal (pure)
- Clear boundaries

**6. Single-Threaded**
- Uses `tokio::main(flavor = "current_thread")`
- No thread pool
- Sequential request processing
- Optimal for CPU-bound inference

**Why This Structure:**
- âœ… Minimal and focused
- âœ… Easy to validate (checkpoints)
- âœ… Modular layers
- âœ… Reuses infrastructure (worker-crates)
- âœ… Orchestrated by pool-managerd
- âŒ Only one model
- âŒ Only CPU
- âŒ Not standalone (needs pool-managerd)

---

## Architectural Comparison

### Candle: Framework Architecture

```
Application (your code)
    â†“
candle-transformers (models)
    â†“
candle-nn (layers)
    â†“
candle-core (tensors)
    â†“
Backend (CPU/CUDA/Metal)
```

**Philosophy:** Provide building blocks, let users build applications.

---

### Mistral.rs: Monolithic Architecture

```
Client (HTTP/Python)
    â†“
mistralrs-server (API)
    â†“
mistralrs-core (engine + models + layers)
    â†“
candle-core (tensors)
    â†“
Backend (CPU/CUDA/Metal)
```

**Philosophy:** Complete solution, batteries included.

---

### llorch-cpud: Worker Architecture

```
orchestratord (brain)
    â†“
pool-managerd (control plane)
    â†“
llorch-cpud (worker)
    â†“ uses
worker-http (HTTP server)
worker-common (types)
worker-tokenizer (tokenization)
worker-models (configs)
    â†“
llorch-cpud/layers (pure implementation)
    â†“
ndarray (CPU tensors)
```

**Philosophy:** Minimal worker, orchestrated by system.

---

## Key Differences

### 1. Scope

| Aspect | Candle | Mistral.rs | llorch-cpud |
|--------|--------|------------|-------------|
| **Purpose** | Framework | Server | Worker |
| **Models** | 100+ (examples) | 24+ (built-in) | 1 (GPT-2) |
| **Backends** | CPU/CUDA/Metal | CPU/CUDA/Metal | CPU only |
| **HTTP API** | No | Yes | Yes (via worker-http) |
| **Standalone** | No | Yes | No (needs pool-managerd) |

### 2. Layer Organization

**Candle:**
```
candle-nn/
â”œâ”€â”€ layer_norm.rs
â”œâ”€â”€ linear.rs
â”œâ”€â”€ conv.rs
â””â”€â”€ ...
```
- One file per layer type
- Reusable across models
- Generic implementations

**Mistral.rs:**
```
mistralrs-core/src/
â”œâ”€â”€ layers.rs (90KB!)
â””â”€â”€ models/
    â”œâ”€â”€ llama.rs
    â”œâ”€â”€ mistral.rs
    â””â”€â”€ ...
```
- Monolithic `layers.rs`
- Model-specific implementations
- Everything in one place

**llorch-cpud:**
```
src/layers/
â”œâ”€â”€ layer_norm.rs
â”œâ”€â”€ attention/
â”‚   â”œâ”€â”€ qkv.rs
â”‚   â”œâ”€â”€ scores.rs
â”‚   â””â”€â”€ output.rs
â”œâ”€â”€ ffn.rs
â””â”€â”€ transformer.rs
```
- One file per component
- Attention split into focused files
- Checkpoint-aligned

### 3. Cache Organization

**Candle:**
- No explicit cache module
- KV cache is part of model implementation
- Embedded in attention

**Mistral.rs:**
```
mistralrs-core/src/
â”œâ”€â”€ kv_cache/
â”‚   â”œâ”€â”€ normal.rs
â”‚   â”œâ”€â”€ paged.rs
â”‚   â””â”€â”€ ...
â””â”€â”€ paged_attention/
```
- Dedicated cache module
- Multiple cache strategies
- Paged attention support

**llorch-cpud:**
```
src/cache/
â”œâ”€â”€ mod.rs
â””â”€â”€ kv_cache.rs
```
- Top-level cache module
- Simple implementation for MVP
- Room to grow (paged attention later)

### 4. Weight Loading

**Candle:**
```rust
let vb = VarBuilder::from_safetensors(...);
let ln = layer_norm(size, vb.pp("ln_1"))?;
```
- VarBuilder pattern
- `.pp()` = push prefix
- Matches PyTorch naming

**Mistral.rs:**
```rust
let vb = ShardedVarBuilder::from_safetensors(...);
let ln = layer_norm(size, config, vb.pp("ln_1"))?;
```
- ShardedVarBuilder (for multi-GPU)
- Same pattern as Candle
- Adds sharding support

**llorch-cpud:**
```rust
// Load from GGUF
let weights = load_gguf(&model_path)?;
let ln = LayerNorm::new(weights.ln_1_weight, weights.ln_1_bias, 1e-5);
```
- Direct weight loading
- No VarBuilder (simpler)
- GGUF format only

---

## Why llorch-cpud is Different

### 1. Worker, Not Server

**Candle/Mistral.rs:**
- Standalone applications
- Users run them directly
- Self-contained

**llorch-cpud:**
- Worker daemon
- Spawned by pool-managerd
- Part of larger system

### 2. Single Model Focus

**Candle:**
- Framework for any model
- Users implement their models

**Mistral.rs:**
- 24+ models built-in
- Users choose at runtime

**llorch-cpud:**
- ONE model (GPT-2)
- Hardcoded at compile time
- Optimized for that model

### 3. Checkpoint-Driven

**Candle/Mistral.rs:**
- No explicit checkpoints
- Test by running inference
- Compare with reference

**llorch-cpud:**
- 13 validation checkpoints
- Each component validated separately
- Incremental development

### 4. Modular Layers

**Candle:**
- Layers in `candle-nn`
- Reusable across models

**Mistral.rs:**
- Monolithic `layers.rs`
- Everything in one file

**llorch-cpud:**
- Each layer in own file
- Attention split into 4 files
- Cache is top-level

### 5. Worker-Crates Pattern

**Candle/Mistral.rs:**
- Self-contained
- No external worker infrastructure

**llorch-cpud:**
- Uses worker-http for HTTP
- Uses worker-common for types
- Uses worker-tokenizer for tokenization
- Uses worker-models for configs
- Reuses infrastructure

---

## Lessons from Candle

### 1. VarBuilder Pattern
**What:** `.pp("layer")` pushes prefix, matches PyTorch naming

**Why:** Makes weight loading match PyTorch exactly

**llorch-cpud:** Doesn't use this (simpler, GGUF-only)

### 2. Backend Abstraction
**What:** `Device` enum (CPU/CUDA/Metal)

**Why:** Same code, multiple backends

**llorch-cpud:** CPU-only (no abstraction needed)

### 3. Modular Layers
**What:** `candle-nn` has reusable layers

**Why:** Build any model from components

**llorch-cpud:** Similar, but focused on GPT-2

---

## Lessons from Mistral.rs

### 1. Production Features
**What:** Paged attention, quantization, scheduling

**Why:** Production inference needs these

**llorch-cpud:** MVP first, add later

### 2. Monolithic Core
**What:** `layers.rs` is 90KB

**Why:** Everything in one place for performance

**llorch-cpud:** Opposite - split into files for clarity

### 3. Multiple Cache Strategies
**What:** `kv_cache/` has normal, paged, etc.

**Why:** Different use cases need different strategies

**llorch-cpud:** Simple cache now, room to grow

### 4. Request Scheduling
**What:** `engine/` and `scheduler/` for batching

**Why:** Production needs request management

**llorch-cpud:** Orchestratord handles this

---

## Why llorch-cpud's Structure Makes Sense

### 1. Worker-First Design
- Part of larger system (llama-orch)
- Orchestrated by pool-managerd
- Not standalone

### 2. Minimal and Focused
- ONE model (GPT-2)
- ONE backend (CPU)
- Easy to validate

### 3. Checkpoint-Driven
- Incremental validation
- Catch errors early
- Reference implementations

### 4. Modular Layers
- Easy to understand
- Easy to test
- Easy to validate

### 5. Reuses Infrastructure
- worker-http for HTTP
- worker-common for types
- worker-tokenizer for tokenization
- Focus on model, not infrastructure

### 6. Room to Grow
- Cache is top-level (can add paged attention)
- Layers are modular (can optimize)
- Single-threaded (can add multi-model later)

---

## Summary Table

| Aspect | Candle | Mistral.rs | llorch-cpud |
|--------|--------|------------|-------------|
| **Type** | Framework | Server | Worker |
| **Purpose** | Build ML apps | Run inference | Execute one model |
| **Models** | 100+ examples | 24+ built-in | 1 hardcoded |
| **Backends** | CPU/CUDA/Metal | CPU/CUDA/Metal | CPU only |
| **HTTP API** | No | Yes | Yes (worker-http) |
| **Standalone** | No | Yes | No |
| **Layer Org** | Modular | Monolithic | Modular |
| **Cache** | Embedded | Dedicated module | Top-level module |
| **Weight Loading** | VarBuilder | ShardedVarBuilder | Direct GGUF |
| **Validation** | Manual | Manual | 13 checkpoints |
| **Threading** | Multi | Multi | Single |
| **Orchestration** | None | Built-in | External (pool-managerd) |

---

## Conclusion

**Candle:** Framework for building ML applications  
**Mistral.rs:** Complete production inference server  
**llorch-cpud:** Minimal worker daemon for orchestrated system

**llorch-cpud's structure is correct because:**
- âœ… It's a worker, not a server
- âœ… It's minimal and focused
- âœ… It's checkpoint-driven
- âœ… It reuses infrastructure
- âœ… It's part of larger system

**Different purposes require different structures!**

---

Built by TEAM CASCADE ğŸŒŠ
