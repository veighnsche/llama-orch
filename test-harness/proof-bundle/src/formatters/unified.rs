//! Unified proof bundle report
//!
//! Single comprehensive markdown report containing all test evidence.

use crate::core::{TestSummary, TestStatus};

/// Generate unified proof bundle report
///
/// Combines executive summary, test breakdown, metadata, and failures into one document.
pub fn generate_unified_report(summary: &TestSummary) -> Result<String, String> {
    // Validate input
    if summary.total == 0 {
        return Err("Cannot generate report: no tests in summary".to_string());
    }
    
    let mut md = String::new();
    
    // Header
    md.push_str("# AUTOGENERATED: Proof Bundle\n");
    md.push_str("# Comprehensive Test Report\n\n");
    
    // Executive Summary Section
    md.push_str("## Executive Summary\n\n");
    md.push_str(&format!("**Date**: {}\n", chrono::Utc::now().format("%Y-%m-%d")));
    
    let status = if summary.pass_rate == 100.0 {
        "✅ 100.0% PASS RATE"
    } else if summary.pass_rate >= 95.0 {
        "⚠️ HIGH PASS RATE"
    } else if summary.pass_rate >= 80.0 {
        "⚠️ MODERATE PASS RATE"
    } else {
        "❌ LOW PASS RATE"
    };
    md.push_str(&format!("**Status**: {}\n\n", status));
    
    md.push_str("### Quick Facts\n\n");
    md.push_str(&format!("- **{} tests** executed\n", summary.total));
    md.push_str(&format!("- **{} passed** ({:.1}%)\n", summary.passed, summary.pass_rate));
    md.push_str(&format!("- **{} failed** ({:.1}%)\n", summary.failed, 100.0 - summary.pass_rate));
    md.push_str(&format!("- **{} ignored**\n", summary.ignored));
    md.push_str(&format!("- **Duration**: {:.2}s\n\n", summary.duration_secs));
    
    // Risk Assessment
    md.push_str("### Risk Assessment\n\n");
    if summary.failed == 0 {
        md.push_str("✅ **LOW RISK** — All tests passing\n\n");
        md.push_str("### Recommendation\n\n");
        md.push_str("**✅ APPROVED** — High confidence for deployment\n\n");
    } else {
        md.push_str("❌ **HIGH RISK** — Failures detected\n\n");
        md.push_str("### Recommendation\n\n");
        md.push_str("**❌ BLOCKED** — Address failures before deployment\n\n");
    }
    
    md.push_str("---\n\n");
    
    // Test Evidence Section (Metadata)
    md.push_str("## Test Evidence\n\n");
    md.push_str("Detailed evidence of what was tested, threats addressed, and failure modes prevented.\n\n");
    
    // Group by priority
    let mut critical = Vec::new();
    let mut high = Vec::new();
    let mut medium = Vec::new();
    let mut low = Vec::new();
    let mut other = Vec::new();
    
    for test in &summary.tests {
        if let Some(ref metadata) = test.metadata {
            match metadata.priority.as_deref() {
                Some("critical") => critical.push(test),
                Some("high") => high.push(test),
                Some("medium") => medium.push(test),
                Some("low") => low.push(test),
                _ => other.push(test),
            }
        } else {
            other.push(test);
        }
    }
    
    if !critical.is_empty() {
        md.push_str(&format!("### 🚨 Critical Tests ({})\n\n", critical.len()));
        for test in critical {
            format_detailed_test(&mut md, test);
        }
    }
    
    if !high.is_empty() {
        md.push_str(&format!("### ⚠️ High Priority Tests ({})\n\n", high.len()));
        for test in high {
            format_detailed_test(&mut md, test);
        }
    }
    
    if !medium.is_empty() {
        md.push_str(&format!("### 📋 Medium Priority Tests ({})\n\n", medium.len()));
        for test in medium {
            format_detailed_test(&mut md, test);
        }
    }
    
    if !low.is_empty() {
        md.push_str(&format!("### 📝 Low Priority Tests ({})\n\n", low.len()));
        for test in low {
            format_detailed_test(&mut md, test);
        }
    }
    
    if !other.is_empty() {
        md.push_str(&format!("### Other Tests ({})\n\n", other.len()));
        for test in other {
            format_detailed_test(&mut md, test);
        }
    }
    
    md.push_str("---\n\n");
    
    // Failures Section (if any)
    if summary.failed > 0 {
        md.push_str("## ❌ Failures\n\n");
        let failures: Vec<_> = summary.tests.iter()
            .filter(|t| t.status == TestStatus::Failed)
            .collect();
        
        for test in failures {
            md.push_str(&format!("### {} {}\n\n", "❌", test.name));
            
            // Show stderr (where cargo test output goes)
            if let Some(ref stderr) = test.stderr {
                if !stderr.is_empty() {
                    md.push_str("**Error Output:**\n```\n");
                    md.push_str(stderr);
                    md.push_str("\n```\n\n");
                }
            }
            
            // Show stdout if present
            if let Some(ref stdout) = test.stdout {
                if !stdout.is_empty() {
                    md.push_str("**Standard Output:**\n```\n");
                    md.push_str(stdout);
                    md.push_str("\n```\n\n");
                }
            }
        }
        md.push_str("---\n\n");
    }
    
    // Test Breakdown Section
    md.push_str("## Test Breakdown\n\n");
    
    let passed: Vec<_> = summary.tests.iter()
        .filter(|t| t.status == TestStatus::Passed)
        .collect();
    
    if !passed.is_empty() {
        md.push_str(&format!("### ✅ Passed ({})\n\n", passed.len()));
        for test in passed {
            md.push_str(&format!("- {}\n", test.name));
        }
        md.push_str("\n");
    }
    
    let ignored: Vec<_> = summary.tests.iter()
        .filter(|t| t.status == TestStatus::Ignored)
        .collect();
    
    if !ignored.is_empty() {
        md.push_str(&format!("### ⏭️ Ignored ({})\n\n", ignored.len()));
        for test in ignored {
            md.push_str(&format!("- {}\n", test.name));
        }
    }
    
    Ok(md)
}

fn format_detailed_test(md: &mut String, test: &crate::core::TestResult) {
    let status_emoji = match test.status {
        TestStatus::Passed => "✅",
        TestStatus::Failed => "❌",
        TestStatus::Ignored => "⏭️",
    };
    
    md.push_str(&format!("- {} **{}**", status_emoji, test.name));
    
    if let Some(ref metadata) = test.metadata {
        if let Some(ref spec) = metadata.spec {
            md.push_str(&format!(" `{}`", spec));
        }
        
        md.push_str("\n");
        
        // Show scenario (WHAT is being tested)
        if let Some(ref scenario) = metadata.scenario {
            md.push_str(&format!("  - **Scenario**: {}\n", scenario));
        }
        
        // Show threat (security/risk addressed)
        if let Some(ref threat) = metadata.threat {
            md.push_str(&format!("  - **Threat**: {}\n", threat));
        }
        
        // Show failure mode (what failure is prevented)
        if let Some(ref failure_mode) = metadata.failure_mode {
            md.push_str(&format!("  - **Prevents**: {}\n", failure_mode));
        }
        
        // Show edge case (boundary conditions)
        if let Some(ref edge_case) = metadata.edge_case {
            md.push_str(&format!("  - **Edge Case**: {}\n", edge_case));
        }
        
        // Show team and tags on same line
        let mut details = Vec::new();
        if let Some(ref team) = metadata.team {
            details.push(format!("@{}", team));
        }
        if !metadata.tags.is_empty() {
            details.push(metadata.tags.join(", "));
        }
        if !details.is_empty() {
            md.push_str(&format!("  - *{}*\n", details.join(" · ")));
        }
        
        md.push_str("\n");
    } else {
        md.push_str("\n\n");
    }
}
