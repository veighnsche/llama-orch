@startuml
' Draft architecture â€” llama-orch on-prem/hybrid

actor Client
rectangle Orchestratord as ORCH {
  [HTTP API] as API
  [SSE Stream] as SSE
}

package "Adapters" {
  [vLLM]
  [TGI]
  [llama.cpp]
}

node "GPU Pool" {
  [GPU-1]
  [GPU-2]
}

cloud Observability {
  [Logs]
  [Metrics]
  [Traces]
}

collections Secrets

Client --> API
API --> SSE
API --> Adapters
Adapters --> "GPU Pool"
ORCH --> Observability
ORCH --> Secrets

note right of ORCH
- Admission, sessions
- Deterministic SSE
- Cancel semantics
- Policy/guardrails
end note

note bottom of Observability
- Latency, throughput
- Uptime, queue
- Cost per 1K tokens
end note

@enduml
