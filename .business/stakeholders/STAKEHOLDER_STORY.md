# ğŸ rbee: The Complete Story

> **Building the future of AI orchestration, one bee at a time** ğŸ¯

**Pronunciation:** rbee (pronounced "are-bee")  
**Version:** 0.1.0 (Pre-Release)  
**Date:** 2025-10-10  
**Status:** Architecture Complete, M0 Implementation In Progress  
**License:** GPL-3.0-or-later (Free & Open Source, Copyleft)  
**Website:** https://rbee.dev

**ğŸ¯ PRIMARY TARGET AUDIENCE:** Developers who build with AI but don't want to depend on big AI providers.

**THE FEAR:** You're building complex codebases with AI assistance. What if OpenAI/Anthropic changes their models, shuts down, or changes pricing? Your codebase becomes unmaintainable without AI.

**THE SOLUTION:** rbee gives you a local AI infrastructure using ALL your home network hardware. Build your own AI coders from scratch with agentic API. Never depend on external providers again.

---

## Executive Summary

**rbee (pronounced "are-bee") is an OpenAI-compatible AI orchestration platform that lets you build your own AI infrastructure using ALL your home network hardware. Never depend on external AI providers again.**

**Core Problem We Solve:**

> **"I'm building complex codebases with AI assistance (Claude, GPT-4, etc.). But what happens when the AI provider changes their models, shuts down, or changes pricing? My codebase becomes unmaintainable without AI. I've created a dependency I can't control."**

**Our Solution:**

- **Independence:** Build AI coders that run on YOUR hardware
- **Control:** Your models, your rules, never change without permission
- **Agentic API:** Task-based API with SSE streaming for building AI agents
- **llama-orch-utils:** TypeScript library for building LLM pipelines
- **Home Network Power:** Use every GPU across all your computers
- **OpenAI-compatible:** Drop-in replacement, switch anytime
- **Multi-Modal AI:** LLMs, Stable Diffusion, TTS, embeddings (powered by Candle)
- **User-Scriptable Routing:** Write Rhai scripts to control how AI tasks are routed

**Our Solution:**
A complete AI orchestration ecosystem with 4 binaries (queen-rbee, [ai-type]-[backend]-worker-rbee, rbee-keeper, rbee-hive), web UI, SDK, marketplace integration, and support for multiple AI modalities (text, image, audio, embeddings).

**Why This Matters:**

Developers are scared of building heavy, complicated codebases with AI assistance because:
- **Dependency risk:** What if the AI provider changes or disappears?
- **Maintenance nightmare:** Can't maintain AI-generated code without AI
- **Cost uncertainty:** Pricing can change overnight
- **Loss of control:** External providers control your tooling

**rbee solves this:** Your own AI infrastructure, your own models, always available.

**Current Status (Oct 2025):**
- âœ… **42/62 BDD scenarios passing** (68% complete)
- âœ… **11 shared crates already built** (audit-logging, auth-min, input-validation, secrets-management, narration-core, deadline-propagation, gpu-info, and more)
- âœ… **GDPR compliance ready** (895 lines of audit-logging docs, 32 pre-defined event types)
- âœ… Backend detection system operational (CUDA, Metal, CPU)
- âœ… Multi-backend worker support
- âœ… OpenAI-compatible API (v1 completion endpoints)
- âœ… llama-orch-utils (TypeScript library for agentic AI)
- ğŸš§ **30-day plan to first revenue** (detailed day-by-day execution plan)
- ğŸš§ Lifecycle management in progress (daemon/hive/worker commands)

---

## What Makes Us Unique

### 1. **Multi-Modal AI Platform (Beyond LLMs)**

**Powered by Candle** (Rust ML framework), we support:

- **Text Generation** (LLMs): Llama, Qwen, Phi, GPT-style models
- **Image Generation**: Stable Diffusion, Stable Diffusion 3
- **Audio Synthesis**: Text-to-Speech (TTS) workers
- **Vector Embeddings**: text-embedding-ada-002 style workers
- **Multimodal**: Vision-language models (future)

**Different AI tasks, different protocols:**
- Text: SSE streaming (token-by-token)
- Images: JSON response with base64 or binary
- Audio: Binary stream (MP3, WAV)
- Embeddings: JSON response (vector arrays)

**Why This Matters:**
- One platform for ALL your AI needs
- Unified API across modalities
- Same orchestration logic for text, images, audio
- Extensible: Add new modalities without breaking existing ones

### 2. **User-Scriptable Routing (Rhai)**

**Write custom routing logic in Rhai** (embedded scripting language):

```rhai
// Route based on task type and priority
fn route_task(task, workers) {
    if task.type == "image-gen" && task.priority == "high" {
        // Route Stable Diffusion to CUDA GPUs
        return workers.filter(|w| w.capability == "image-gen" && w.backend == "cuda").least_loaded();
    } else if task.type == "text-gen" && task.model.contains("70b") {
        // Route large LLMs to multi-GPU setups
        return workers.filter(|w| w.gpus > 1).first();
    } else {
        // Route everything else to available workers
        return workers.least_loaded();
    }
}
```

**Marketplace for Routing Scripts:**
- Share your routing strategies
- Download community scripts
- Monetize clever routing algorithms
- **Example:** "Best latency for EU customers" script

**Why This Matters:**
- No recompilation neededâ€”update scripts at runtime
- Community-driven optimization
- Monetization opportunity for script creators

### 3. **Global GPU Marketplace (Revolutionary)**

**The Big Idea:** Connect to queen-rbees around the world who have idle GPUs available for tasks.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YOUR PLATFORM (Marketplace)                                   â”‚
â”‚                                                               â”‚
â”‚  Customer-Facing API: api.yourplatform.com                   â”‚
â”‚       â†“                                                       â”‚
â”‚  Your queen-rbee (Marketplace Engine)                        â”‚
â”‚       â”œâ”€â†’ Home Lab A (queen-rbee + 3 GPUs) [Netherlands]    â”‚
â”‚       â”œâ”€â†’ Home Lab B (queen-rbee + 2 GPUs) [Germany]        â”‚
â”‚       â””â”€â†’ Home Lab C (queen-rbee + 4 GPUs) [France]         â”‚
â”‚                                                               â”‚
â”‚  Customer pricing:  $100/hr (competitive market rate)        â”‚
â”‚  Provider payout:   $60/hr  (fair compensation)              â”‚
â”‚  Platform value:    $40/hr  (service, reliability, support)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Task-Based, Not Stable:**
- GPUs don't need to be always-on
- Perfect for idle capacity monetization
- Providers join/leave dynamically
- Platform matches tasks to available capacity

**SDK Makes It Easy:**
```typescript
// One line to switch from local to marketplace
const client = new Client('https://api.yourplatform.com', { apiKey: 'xxx' });

// Same API, global GPU access
const result = await client.enqueue({
  prompt: 'Generate an image of a cat',
  capability: 'image-gen',
  model: 'stable-diffusion-xl',
});
```

**Revenue Model:**
- 30-40% platform fee
- Providers monetize idle GPUs
- Customers get affordable, compliant inference
- Everyone wins

### 4. **EU-Native Compliance (GDPR)**
- Built-in audit logging and data residency enforcement
- GDPR-compliant endpoints (data export, deletion, consent tracking)
- EU-only worker filtering when compliance mode is enabled
- **Target:** B2B customers who need GDPR guarantees
- **Marketplace advantage:** Geo-verified EU-only providers

### 5. **Multi-Architecture Support**
- **NVIDIA CUDA** workers (VRAM-only policy for predictable performance)
- **Apple Metal** workers (unified memory architecture for M-series chips)
- **CPU** workers (fallback for models that don't require GPU)
- Extensible worker adapter system for future backends
- **Candle-powered:** Unified ML framework across all backends

### 6. **Test Reproducibility**
- Same model + same seed + temp=0 â†’ Same output (for testing validation)
- Proof bundle system captures seeds, transcripts, and metadata
- Enables deterministic testing for CI/CD pipelines
- **Note:** This is a testing tool, not a product guarantee (LLMs have inherent non-determinism)

### 7. **Web UI + SDK + CLI**
- **rbee-keeper (Web UI):** Visual management interface (primary interface)
- **rbee-keeper (CLI):** Command-line interface for power users (same functionality)
- **SDK (Rust/TypeScript):** Programmatic access for applications
- **All three** use the same queen-rbee HTTP API

### 8. **Clean Intelligence Hierarchy (The Bee Metaphor) ğŸ**
- **ğŸ‘‘ğŸ queen-rbee (The Brain):** Makes ALL intelligent decisions (scheduling, routing, admission)
  - SQLite registries: workers + beehives (with SSH details)
  - Rhai scripting engine for custom routing
  - Cascading shutdown coordinator
- **ğŸ¯ğŸ  rbee-hive (Hive Manager):** Executes commands, reports state (no policy decisions)
  - SQLite model catalog
  - Backend detection (CUDA, Metal, CPU)
  - Worker lifecycle management
- **ğŸğŸ’ª [ai-type]-[backend]-worker-rbee (Worker Bees):** Load models, generate tokens
  - Examples: llm-cuda-worker-rbee, sd-cuda-worker-rbee
  - Stateless executors with isolated memory contexts
- **ğŸ§‘â€ğŸŒ¾ğŸ rbee-keeper (Keeper Interface):** Web UI + CLI for managing the entire system
  - Configuration mode: setup add-node, install, list-nodes
  - Daemon mode: start/stop/status (in progress)
  - Inference mode: infer command

---

## Target Audience

### Primary Users (Ranked by Priority)

1. **Developers** (Priority 1)
   - **Use Case:** Develop applications with agentic API and home LLM inference
   - **Benefit:** Private inference on their own hardware with full control
   - **Pain Point:** Can't use cloud APIs due to privacy concerns or cost

2. **DevOps/SRE** (Priority 2)
   - **Use Case:** Set up private LLM inference clusters for teams/companies
   - **Benefit:** Multi-node orchestration with SSH-based management
   - **Pain Point:** Existing solutions are complex or cloud-only

3. **Startups** (Priority 3)
   - **Use Case:** Experiment with home LLM infrastructure using existing hardware
   - **Benefit:** Free (GPL) + run on whatever GPUs they have
   - **Pain Point:** Can't afford cloud inference costs at scale

### Secondary Users

- **Researchers:** Need reproducible results for testing and validation
- **Small Businesses:** Want GDPR-compliant private LLM hosting (EU market)
- **Homelabbers:** Run AI workloads on their existing GPU cluster

---

## The 4-Binary Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ queen-rbee (THE BRAIN - HTTP Daemon) [M1 - Not Built Yet]  â”‚
â”‚ â€¢ User-scriptable orchestration (Rhai scripting)            â”‚
â”‚ â€¢ Worker registry (SQLite)                                  â”‚
â”‚ â€¢ Routes inference requests via HTTP                        â”‚
â”‚ â€¢ Relays SSE token streams                                  â”‚
â”‚ Port: 8080                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST /execute (direct to worker)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ worker-rbee (EXECUTORS - HTTP Daemons) [M0 âœ… DONE]    â”‚
â”‚ â€¢ One worker per model (keeps model in VRAM/memory)        â”‚
â”‚ â€¢ Generates tokens via SSE streaming                        â”‚
â”‚ â€¢ Variants: cuda, metal, cpu                                â”‚
â”‚ Ports: 8001+                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Control Plane (CLIs):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rbee-keeper (USER INTERFACE - CLI) [M0 âœ… DONE]            â”‚
â”‚ â€¢ Manages queen-rbee lifecycle (start/stop/status)          â”‚
â”‚ â€¢ Configures SSH for remote machines                        â”‚
â”‚ â€¢ Manages hives and workers                                 â”‚
â”‚ â€¢ Future: Web UI                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ SSH control commands
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rbee-hive (POOL MANAGER - CLI) [M0 âœ… DONE]                â”‚
â”‚ â€¢ Model catalog (tracks downloaded models)                  â”‚
â”‚ â€¢ Worker spawning and lifecycle management                  â”‚
â”‚ â€¢ Backend detection (CUDA, Metal, CPU)                      â”‚
â”‚ â€¢ Orphan cleanup (detects and kills dead workers)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Critical Design Principle: Cascading Shutdown

**WHENEVER queen-rbee DIES, EVERYTHING DIES GRACEFULLY:**

```
queen-rbee dies (SIGTERM, crash, kill)
    â†“
ALL rbee-hive instances on ALL nodes die (via SSH SIGTERM)
    â†“
ALL workers on ALL nodes die (via shutdown command)
    â†“
ALL workers unload models and exit cleanly
    â†“
System is completely clean, no orphaned processes, no leaked VRAM
```

**Why This Matters:**
- No orphaned processes after tests
- No VRAM leaks
- Deterministic testing (every test starts from clean state)
- Production safety (graceful shutdown guarantees)

---

## How It Works: Complete Flow

### Development/Testing Mode (Ephemeral)

```bash
# User runs a one-off inference
rbee-keeper infer --node mac --model tinyllama --prompt "hello"
```

**What Happens:**
1. rbee-keeper spawns queen-rbee as child process (HTTP daemon on port 8080)
2. queen-rbee uses SSH to start rbee-hive on remote node (HTTP daemon on port 9200)
3. rbee-hive spawns worker-rbee (loads model into VRAM, HTTP daemon on port 8001)
4. Worker becomes ready, sends callback to rbee-hive (POST /v1/workers/ready)
5. rbee-hive notifies queen-rbee (POST /v1/orchestrator/worker-ready)
6. queen-rbee adds worker to global registry
7. queen-rbee routes inference request **directly** to worker (HTTP POST http://mac:8001/execute)
8. Worker streams tokens via SSE to queen-rbee
9. queen-rbee relays SSE stream to rbee-keeper stdout
10. User sees tokens appear in terminal
11. User presses Ctrl+C â†’ **cascading shutdown kills everything gracefully**

**Result:** Clean environment, no leftover processes, ready for next test

### Production Mode (Persistent)

```bash
# Operator starts daemons manually (one-time setup)
queen-rbee daemon &
ssh mac "rbee-hive daemon &"
ssh workstation "rbee-hive daemon &"

# Application uses SDK (HTTP client)
curl -X POST http://queen:8080/v2/tasks \
  -d '{"prompt": "hello", "model": "tinyllama"}'
```

**What Happens:**
1. Daemons are pre-started by operator
2. Workers stay alive, reused across requests (high performance)
3. rbee-keeper is NOT used (SDK talks directly to queen-rbee HTTP API)
4. Operator manages lifecycle manually

**Result:** High performance, worker reuse, production-grade

---

## Key Features

### 1. User-Scriptable Orchestration (Rhai)

Write custom orchestration logic without recompiling:

```rhai
// Custom scheduling policy
fn schedule_job(job, workers) {
    if job.priority == "high" {
        // Route high-priority to CUDA GPUs
        return workers.filter(|w| w.backend == "cuda").least_loaded();
    } else {
        // Route batch jobs to CPU
        return workers.filter(|w| w.backend == "cpu").first();
    }
}

// Custom admission control
fn should_admit(job, queue_depth) {
    if queue_depth > 100 {
        return false; // Reject if queue too deep
    }
    return true;
}
```

### 2. Multi-Backend Support

```bash
# CUDA worker (NVIDIA GPU)
worker-rbee --backend cuda --model tinyllama.gguf --gpu 0

# Metal worker (Apple M-series)
worker-rbee --backend metal --model tinyllama.gguf

# CPU worker (fallback)
worker-rbee --backend cpu --model tinyllama.gguf
```

### 3. EU Compliance Mode (GDPR)

```bash
export LLORCH_EU_AUDIT=true
queen-rbee daemon
```

**Enables:**
- Immutable audit log (all API calls recorded)
- GDPR endpoints: `/gdpr/export`, `/gdpr/delete`, `/gdpr/consent`
- Data residency enforcement (EU-only workers)
- Retention policies (auto-delete old data)

### 4. Test Reproducibility & Proof Bundles

```bash
LLORCH_RUN_ID=test-001 rbee-keeper infer --model tinyllama --prompt "hello"
```

**Output:**
```
.proof_bundle/integration/test-001/
â”œâ”€â”€ seeds.json         # Seeds used
â”œâ”€â”€ transcript.ndjson  # SSE events
â”œâ”€â”€ metadata.json      # Model ref, engine version, device
â””â”€â”€ result.txt         # Final output
```

**Focus:** Text generation workers (LLMs)

**Complete:**
- llm-cuda-worker-rbee (HTTP daemon, SSE streaming)
- rbee-hive (model catalog, worker spawning)
- **Backend detection system** (CUDA, Metal, CPU)
- **Registry schema** with backend capabilities
- rbee-keeper (CLI UI, basic commands)
- **Candle-powered:** Qwen, Phi, Llama models
- **54/62 BDD scenarios passing**

**In Progress (TEAM-053):**
- Lifecycle management (daemon start/stop/status)
- Cascading shutdown implementation
- SSH configuration management
- Worker cancellation endpoint

**Target:** 62+ scenarios passing by end of M0

### M1: Pool Manager Lifecycle (v0.2.0) - Q1 2026

**Focus:** Production-ready pool management
- rbee-hive as HTTP daemon (not CLI)
- Worker health monitoring (30s heartbeat)
- Idle timeout enforcement (5 minutes)
- Performance metrics emission

### ğŸ”® M2: Orchestrator Scheduling (v0.3.0) - Q2 2026

**Focus:** Intelligent orchestration + Rhai scripting

- queen-rbee HTTP daemon (orchestrator)
- **Rhai scripting engine** (user-defined orchestration)
- Worker registry (SQLite)
- Scheduling policies (priority, load balancing)
- **Web UI (frontend):** Visual management interface

### ğŸ”® M3: Multi-Modal Support (v0.4.0) - Q3 2026

**Focus:** Beyond LLMs - Images, Audio, Embeddings

- **Image generation workers:** Stable Diffusion, SD3 (Candle)
- **Audio generation workers:** TTS (Candle)
- **Embedding workers:** text-embedding-ada-002 style
- Protocol-aware orchestrator (SSE, JSON, Binary)
- Multi-modal routing in Rhai scripts
- Authentication (API keys, JWT)
- EU compliance mode (GDPR endpoints)

### ğŸ”® M4: Multi-GPU & Multi-Node (v0.5.0) - Q4 2026

**Focus:** Scale and performance

- Multi-GPU support (tensor parallelism)
- Multi-node coordination (distributed workers)
- Advanced scheduling (cross-node load balancing)
- **Platform mode:** Immutable Rhai scheduler for marketplace

### ğŸ”® M5: Platform Marketplace (v0.6.0) - 2027

**Focus:** Global GPU marketplace

- **Provider registration API:** Join the marketplace
- **Task-based billing:** Pay per task, not per hour
- **Federated placement:** Route to best provider
- **SLA enforcement:** Monitor uptime, latency
- **Provider dashboard:** Earnings, utilization, payouts
- **Customer dashboard:** Usage, costs, SLA reports
- **Billing integration:** Stripe, invoicing
- **Geo-verified routing:** EU-only enforcement

---

## Value Propositions by Audience

### For Developers

**Problem:** Cloud APIs are expensive, slow, or violate privacy

**Solution:** Run LLMs on your own hardware with full control

**Benefits:**
- Private inference (data never leaves your network)
- Free (GPL license, no usage fees)
- Multi-backend support (CUDA, Metal, CPU)
- Agentic API (OpenAI-compatible)

### For DevOps/SRE

**Problem:** Need to set up LLM inference for teams, but cloud solutions are complex

**Solution:** SSH-based orchestration for multi-node GPU clusters

**Benefits:**
- Homelab-friendly (SSH control plane)
- Multi-node support (distribute workloads)
- Clean architecture (easy to debug)
- Cascading shutdown (no orphaned processes)

### For Startups

**Problem:** Can't afford cloud inference costs at scale

**Solution:** Use existing hardware (dev machines, homelab) for free

**Benefits:**
- Zero cost (GPL license, run on any hardware)
- Experiment freely (no usage fees)
- Scale horizontally (add more GPUs as needed)
- Test reproducibility (validate model behavior)

### For EU B2B Customers

**Problem:** Need GDPR-compliant LLM hosting

**Solution:** Built-in EU compliance mode

**Benefits:**
- Audit logging (immutable trail)
- GDPR endpoints (data export, deletion, consent)
- Data residency enforcement (EU-only workers)
- Compliance by default (no extra work)

---

## Differentiators

### vs Cloud APIs (OpenAI, Anthropic)

**We win on:**
- âœ… Privacy (data never leaves your network)
- âœ… Cost (free, no usage fees)
- âœ… Control (choose which GPU runs which model)
- âœ… Customization (user-scriptable orchestration)

**They win on:**
- âœ… Ease of use (no infrastructure management)
- âœ… Model selection (access to latest models)

### vs Self-Hosted (Ollama, llama.cpp)

**We win on:**
- âœ… Multi-node orchestration
- âœ… User-scriptable policies (Rhai)
- âœ… Test reproducibility (proof bundles)
- âœ… EU compliance (GDPR built-in)

**They win on:**
- âœ… Simplicity (single binary)
- âœ… Maturity (battle-tested)

### vs Kubernetes (Ray, KServe)

**We win on:**
- âœ… Simplicity (SSH-based, no k8s cluster)
- âœ… Homelab-friendly (works on any Linux machine)
- âœ… Lower overhead (no k8s control plane)

**They win on:**
- âœ… Enterprise features (RBAC, namespaces)
- âœ… Ecosystem integration (Prometheus, Grafana)

---

## Technical Architecture Highlights

### Smart/Dumb Separation

**Design Principle:** ALL intelligence lives in one place (queen-rbee)

```
queen-rbee (THE BRAIN)
    â†“ makes decisions
    â†“ schedules jobs
    â†“ selects workers

rbee-hive (EXECUTOR)
    â†“ executes commands
    â†“ reports state

worker-rbee (EXECUTOR)
    â†“ loads model
    â†“ generates tokens
```

**Benefits:**
- Easy to debug (one place for scheduling bugs)
- Easy to customize (change scripts, no recompilation)
- Easy to test (executors are deterministic)

### Process Isolation & Memory Ownership

**Design Principle:** Each worker owns its memory context

```
Process 1: worker-rbee (CUDA GPU 0)
    â†“ CUDA context 0
    â†“ VRAM allocation isolated

Process 2: worker-rbee (CUDA GPU 1)
    â†“ CUDA context 1
    â†“ VRAM allocation isolated

Process 3: rbee-hive (CPU)
    â†“ NVML read-only queries
    â†“ No VRAM allocation
```

**Benefits:**
- No memory corruption across workers
- Clean VRAM lifecycle
- Standalone testing
- Hardware-specific optimizations

---

## Deployment Examples

### Example 1: Single Developer

```bash
# Start queen-rbee
queen-rbee daemon &

# Start rbee-hive
rbee-hive daemon &

# Spawn workers on different GPUs
rbee-hive worker spawn cuda --model llama-7b --gpu 0
rbee-hive worker spawn cuda --model mistral-7b --gpu 1

# Run inference
curl -X POST http://localhost:8080/v2/tasks \
  -d '{"prompt": "hello", "model": "llama-7b"}'
```

### Example 2: Multi-Node Team

```bash
# On orchestrator machine:
queen-rbee daemon &

# On remote machines:
ssh mac "rbee-hive daemon &"
ssh workstation "rbee-hive daemon &"

# Spawn workers:
ssh mac "rbee-hive worker spawn metal --model llama-7b"
ssh workstation "rbee-hive worker spawn cuda --model llama-70b"

# Team uses SDK
curl -X POST http://orchestrator:8080/v2/tasks \
  -d '{"prompt": "hello", "model": "llama-70b"}'
```

### Example 3: EU Compliance

```bash
# Enable EU audit mode
export LLORCH_EU_AUDIT=true
export LLORCH_AUDIT_LOG_PATH=/var/log/llorch/audit.log

queen-rbee daemon &

# All API calls are logged
# GDPR endpoints are enabled
# Workers are filtered to EU-only
```

---

## The Revolutionary Marketplace Model

### The Vision: Global Task-Based GPU Network

**Traditional GPU Rentals (Runpod, Vast.ai):**
- Rent entire servers by the hour
- Pay even when idle
- Complex setup (Docker, SSH)
- No orchestration

**Our Marketplace (Task-Based):**
- Pay per task, not per hour
- No idle costs
- One-line SDK integration
- Automatic orchestration

### How It Works

**For GPU Providers (Monetize Idle Capacity):**

```bash
# 1. Run your own queen-rbee locally
queen-rbee daemon &

# 2. Register with the platform marketplace
curl -X POST https://api.yourplatform.com/v2/platform/providers/register \
  -d '{
    "provider_id": "home-lab-123",
    "endpoint": "https://my-home-lab.example.com",
    "pricing": {"per_token": 0.0008, "per_hour": 4.0},
    "capacity": {"total_gpus": 6, "total_vram_gb": 144},
    "geo": {"country": "NL", "region": "EU"}
  }'

# 3. Start earning when customers submit tasks
# Your queen-rbee receives tasks, routes to your workers, you get paid
```

**For Customers (Access Global GPU Capacity):**

```typescript
// One line to access global marketplace
const client = new Client('https://api.yourplatform.com', { apiKey: 'xxx' });

// Submit task - platform routes to best provider
const result = await client.enqueue({
  prompt: 'Generate a cat image',
  capability: 'image-gen',
  model: 'stable-diffusion-xl',
});

// You pay per task, not per hour
// Platform handles routing, billing, SLA
```

**For Platform (Create Value):**

- **Match supply with demand:** Route tasks to available providers
- **Quality assurance:** Verify hardware, monitor uptime, enforce SLAs
- **Billing integration:** Handle payments, invoicing, provider payouts
- **Compliance:** EU-only routing, GDPR audit trails
- **Margin:** 30-40% platform fee covers infrastructure, support, reliability

### Revenue Projections

**Conservative Scenario (100 Customers):**
- Customer revenue: $50,000/month
- Provider payouts: $30,000/month
- **Platform revenue: $20,000/month ($240k/year)**

**Aggressive Scenario (1,000 Customers):**
- Customer revenue: $500,000/month
- Provider payouts: $300,000/month
- **Platform revenue: $200,000/month ($2.4M/year)**

**Enterprise Scenario (50 Large Customers):**
- Customer revenue: $500,000/month
- Provider payouts: $325,000/month
- **Platform revenue: $175,000/month ($2.1M/year)**

### Competitive Advantages

**vs Runpod/Vast.ai:**
- âœ… Task-based pricing (not hourly)
- âœ… No idle costs
- âœ… Automatic orchestration
- âœ… Multi-modal support (text, image, audio)
- âœ… EU compliance built-in

**vs Together.ai/Replicate:**
- âœ… Your own provider network
- âœ… Control margins (30-40%)
- âœ… EU data guarantees
- âœ… Test reproducibility support

**vs AWS Bedrock/Azure OpenAI:**
- âœ… 50-70% cheaper (home GPUs)
- âœ… EU-only by design
- âœ… Your API, not theirs
- âœ… Full stack control

### Marketplace Features (M5+)

**Provider Dashboard:**
- Real-time earnings tracking
- Capacity utilization graphs
- SLA compliance monitoring
- Payout history

**Customer Dashboard:**
- Usage analytics
- Cost tracking
- Provider selection preferences
- SLA reports

**Platform Features:**
- Dynamic pricing engine (supply/demand)
- SLA enforcement & monitoring
- Billing integration (Stripe)
- Provider reputation system
- Geo-verified EU-only routing

### Network Effects

**More providers** â†’ More capacity â†’ Better availability â†’ **More customers**  
**More customers** â†’ More demand â†’ Better utilization â†’ **More providers**

**Result:** Self-reinforcing ecosystem where everyone wins

---

## Business Model Evolution

### Current: Open Source (GPL-3.0)

- Free for everyone
- Copyleft (derivative works must be GPL)
- No usage fees, no restrictions
- Build community and prove product value

### Phase 1: Bootstrap (2026 - Year 1)

**Goal:** First paying customers, prove independence from big AI providers

**30-Day Plan to First Customer:**
- **Week 1 (Days 1-7):** Working end-to-end system (submit job â†’ worker executes â†’ tokens stream back)
- **Week 2 (Days 8-14):** EU compliance (GDPR endpoints, audit logs, basic web UI)
- **Week 3 (Days 15-21):** Marketing (landing page, outreach, 10 qualified leads)
- **Week 4 (Days 22-30):** Revenue (demos, close deal, onboard customer, â‚¬200 MRR)

**Key Advantage:** Already have 11 shared crates built (audit-logging, auth-min, etc.) â€” saves 5 days of development time!

**Milestones:**
- Month 1: 1 customer (â‚¬200 MRR) â€” **30-day plan in place**
- Month 3: 5 customers (â‚¬1,500 MRR)
- Month 6: 20 customers (â‚¬6,000 MRR)
- Month 12: 35 customers (â‚¬10,000 MRR)

**Features:**
- OpenAI-compatible API
- Agentic API for building AI coders
- llama-orch-utils (TypeScript library)
- Basic web UI
- EU compliance (GDPR) â€” **already 90% built!**

**Revenue:** â‚¬70K Year 1

### Phase 2: Scale (2027 - Year 2)

**Goal:** 100 customers, platform mode

**Milestones:**
- Q1: 50 customers (â‚¬15,000 MRR)
- Q2: 70 customers (â‚¬21,000 MRR)
- Q3: 85 customers (â‚¬25,500 MRR)
- Q4: 100 customers (â‚¬30,000 MRR)

**Features:**
- Platform mode (multi-tenant)
- Enterprise tier (dedicated instances)
- Advanced web UI
- Multi-modal (LLMs, SD, TTS, embeddings)

**Revenue:** â‚¬360K Year 2

### Phase 3: Marketplace (2028 - Year 3)

**Goal:** â‚¬1M+ annual revenue, GPU marketplace

**Milestones:**
- 200+ customers
- â‚¬83K+ MRR
- Marketplace with distributed providers

**Features:**
- GPU marketplace (provider network)
- Rhai script marketplace
- Enterprise features (SLAs, white-label)
- SOC2, ISO27001 certifications

**Revenue:** â‚¬1M+ Year 3

---

## Get Started

### Try It Today

```bash
git clone https://github.com/veighnsche/rbee
cd rbee
cargo build --release

# Run first inference
./target/release/rbee-keeper infer \
  --model tinyllama \
  --prompt "hello world"
```

### Join the Community

- **GitHub:** https://github.com/veighnsche/rbee
- **Discord:** (Coming soon)
- **Docs:** (Coming soon)

### Contribute

**We need:**
- Code reviewers (99% AI-generated, need human eyes)
- Security auditors (find vulnerabilities)
- Documentation writers
- Testers (multi-backend, multi-node)

---

## Contact

**Creator:** veighnsche (Vince)  
**GitHub:** https://github.com/veighnsche/rbee  
**License:** GPL-3.0-or-later

**Transparency Note:**
> This project is ~99% AI-generated (via Windsurf + Claude). I take full responsibility for architecture, specs, and any bugs. I'm seeking code reviewers to audit the codebase. If you're skeptical of vibe-coded projects, I understandâ€”but I hope the product speaks for itself.

**Security Issues:** See SECURITY.md for responsible disclosure

---

## Summary: The Complete Vision

### The Problem We're Solving

**Current State (Broken):**
- âŒ Cloud APIs are expensive and violate privacy
- âŒ Self-hosted solutions lack orchestration
- âŒ Can't easily pick which GPU runs which model
- âŒ Can't use remote computers for AI tasks
- âŒ No multi-modal support (LLMs only)
- âŒ No marketplace for idle GPU capacity
- âŒ Kubernetes is overkill for small teams
- âŒ No GDPR-native AI platforms exist

### Our Solution (Revolutionary)

**1. Multi-Modal AI Platform**
- âœ… Text generation (LLMs via Candle)
- âœ… Image generation (Stable Diffusion via Candle)
- âœ… Audio synthesis (TTS via Candle)
- âœ… Vector embeddings
- âœ… Unified API across all modalities

**2. User-Scriptable Routing (Rhai)**
- âœ… Write custom routing logic
- âœ… No recompilation needed
- âœ… Marketplace for routing scripts
- âœ… Monetize clever algorithms

**3. Global GPU Marketplace**
- âœ… Task-based pricing (not hourly)
- âœ… No idle costs
- âœ… Monetize idle GPU capacity
- âœ… One-line SDK integration
- âœ… 30-40% platform margins

**4. Complete Ecosystem**
- âœ… 4 binaries (queen-rbee, worker-rbee, rbee-keeper, rbee-hive)
- âœ… Web UI (visual management)
- âœ… SDK (Rust + TypeScript/WASM)
- âœ… CLI (power users)
- âœ… Multi-backend (CUDA, Metal, CPU)
- âœ… SSH-based control (homelab-friendly)
- âœ… EU compliance (GDPR built-in)
- âœ… Test reproducibility (proof bundles)

### The Business Model

**Phase 1 (2026):** Bootstrap - â‚¬70K Year 1 (35 customers)  
**Phase 2 (2027):** Scale - â‚¬360K Year 2 (100 customers)  
**Phase 3 (2028):** Marketplace - â‚¬1M+ Year 3 (200+ customers)

**Revenue Streams:**
1. SaaS subscriptions (â‚¬99-299/month per customer)
2. Enterprise tier (custom pricing, â‚¬2K+/month)
3. Marketplace fees (30-40% on transactions - Year 3)
4. Professional services (integrations, training - Year 2+)

**Pricing Tiers:**
- Starter: â‚¬99/month (500K tokens, basic features)
- Professional: â‚¬299/month (2M tokens, GDPR endpoints) â­ Most Popular
- Enterprise: Custom (â‚¬2K+/month, dedicated instances, SLAs)

### The Vision

**Short-term (2026 - Year 1):**
- Month 1: First paying customer (â‚¬200 MRR)
- Month 3: 5 customers (â‚¬1,500 MRR)
- Month 12: 35 customers (â‚¬10,000 MRR, â‚¬70K revenue)
- Prove Zed IDE integration works
- Launch llama-orch-utils for agentic AI development
- Establish EU compliance as competitive advantage

**Medium-term (2027 - Year 2):**
- 100 customers (â‚¬30,000 MRR, â‚¬360K revenue)
- Platform mode: multi-tenant with immutable Rhai scheduler
- Enterprise tier: dedicated instances
- Web UI for visual management

**Long-term (2028+ - Year 3+):**
- 200+ customers (â‚¬83K+ MRR, â‚¬1M+ revenue)
- GPU marketplace with distributed providers
- Home/Lab mode: custom Rhai scripts for self-hosters
- Multi-modal: LLMs, Stable Diffusion, TTS, embeddings
- Agentic AI development platform

### Why This Will Win

**Technical Moat:**
- Multi-modal support (not just LLMs)
- User-scriptable routing (Rhai)
- Task-based marketplace (not hourly)
- EU compliance built-in (GDPR native)
- Candle-powered (Rust performance)

**Business Moat:**
- Network effects (providers + customers)
- Programmable Rhai scheduler (platform + home modes)
- Test reproducibility (unique capability)
- GPL license (copyleft protection)

**Execution Moat:**
- Clean architecture (smart/dumb separation)
- Spec-driven development (RFC-2119)
- Proof bundle testing (deterministic validation)
- Vibe-coded but rigorous (99% AI-generated, human-reviewed)

### The Ask

**For Developers:**
- â­ Star the repo
- ğŸ§ª Try the platform
- ğŸ’¬ Give feedback
- ğŸ› Report bugs

**For Contributors:**
- ğŸ‘€ Review code (99% AI-generated, need human eyes)
- ğŸ”’ Audit security
- ğŸ“ Write docs
- ğŸ§ª Test multi-backend scenarios

**For GPU Providers:**
- ğŸ“ Sign up for marketplace waitlist (M5)
- ğŸ’° Monetize idle capacity
- ğŸŒ Join global network

**For Investors:**
- ğŸ“Š Review pitch deck
- ğŸ’¬ Schedule call
- ğŸš€ Join the revolution

---

## Quick Reference

**Pronunciation:** rbee (pronounced "are-bee")  
**Target Audience:** Developers who build with AI but fear provider dependency  
**The Fear:** Complex codebases become unmaintainable if provider changes/shuts down  
**The Solution:** Build your own AI infrastructure using home network hardware  
**Key Advantage:** 11 shared crates already built (saves 5 days of development)  
**30-Day Plan:** Week 1 (system), Week 2 (compliance), Week 3 (marketing), Week 4 (revenue)  
**Status:** 68% complete (42/62 BDD scenarios passing)  
**Year 1 Goal:** 35 customers, â‚¬10K MRR, â‚¬70K revenue  
**Current Version:** 0.1.0 (Pre-Release)  
**License:** GPL-3.0-or-later  
**Website:** https://rbee.dev  
**Repository:** https://github.com/veighnsche/llama-orch

---

Verified by Testing Team ğŸ”  
Guarded by auth-min Team ğŸ­  
Optimized by Performance Team â±ï¸  
Secured by Audit Logging Team ğŸ”’  
Narrated by Narration Core Team ğŸ€  
Crafted with love by Developer Experience Team ğŸ¨  
Orchestrated by Human ğŸ§‘â€ğŸŒ¾ğŸ  
Built by AI Engineering Teams ğŸğŸ’ªis not just open source.**

This is a **sustainable business model** with:
- 30-40% platform margins
- Network effects
- Platform mode for marketplace (immutable scheduler)
- Enterprise revenue streams

**This is not just vibe-coded.**

This is **rigorously architected** with:
- Spec-driven development (RFC-2119)
- Proof bundle testing
- Clean separation of concerns
- Human-reviewed architecture

**The future of AI infrastructure is:**
- Multi-modal (not just text)
- User-scriptable (not black box)
- Task-based (not hourly)
- EU-compliant (not US-centric)
- Community-driven (not vendor lock-in)

**We're building that future.** ğŸ

---

## ğŸ The Bee Metaphor

**Why bees?** Because our architecture mirrors a real beehive:

- **ğŸ‘‘ğŸ Queen Bee** - The brain, makes all decisions
- **ğŸ¯ğŸ  Hive** - The home, manages resources and workers
- **ğŸğŸ’ª Worker Bees** - The executors, do the actual work
- **ğŸ§‘â€ğŸŒ¾ğŸ Beekeeper** - The interface, manages the entire colony

Just like a real hive:
- Queen coordinates everything
- Hives provide structure and resources
- Workers execute tasks in parallel
- Keeper observes and manages from outside

**The result:** A harmonious, efficient, scalable system. ğŸ¯

---

*Last Updated: 2025-10-10*  
*Based on latest architecture specs and monetization strategy*  
*Comprehensive analysis of: CRITICAL_RULES.md, COMPONENT_RESPONSIBILITIES_FINAL.md, FINAL_ARCHITECTURE_SSH_CONTROL_HTTP_INFERENCE.md, MULTI_MODALITY_STREAMING_PLAN.md, monetization.md*
