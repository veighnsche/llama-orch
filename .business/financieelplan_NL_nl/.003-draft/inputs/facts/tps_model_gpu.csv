model_id,model_name,engine,precision,gpu,gpu_count,batch,input_tokens,output_tokens,throughput_tokens_per_sec,measurement_type,scenario_notes,source_tag
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,H100,1,1,512,512,92.72,single_stream,"Koyeb harness, shape 512x512",S1
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,A100,1,1,512,512,80.31,single_stream,"Koyeb harness, shape 512x512",S1
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,L40S,1,1,512,512,43.79,single_stream,"Koyeb harness, shape 512x512",S1
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,H100,1,32,512,512,2461.67,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,A100,1,32,512,512,1878.95,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,L40S,1,32,512,512,1124.41,aggregate,"Koyeb harness, shape 512x512, batch=32",S1

Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,H100,1,1,512,512,93.44,single_stream,"Koyeb harness, shape 512x512",S1
Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,A100,1,1,512,512,79.68,single_stream,"Koyeb harness, shape 512x512",S1
Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,L40S,1,1,512,512,45.81,single_stream,"Koyeb harness, shape 512x512",S1
Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,H100,1,32,512,512,1959.53,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,A100,1,32,512,512,1675.51,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
Qwen/Qwen2.5-7B-Instruct,Qwen2.5 7B,vLLM,BF16,L40S,1,32,512,512,956.48,aggregate,"Koyeb harness, shape 512x512, batch=32",S1

deepseek-ai/DeepSeek-R1-Distill-Llama-8B,DeepSeek R1 Distill Llama 8B,vLLM,BF16,H100,1,32,512,512,2442.23,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
deepseek-ai/DeepSeek-R1-Distill-Llama-8B,DeepSeek R1 Distill Llama 8B,vLLM,BF16,A100,1,32,512,512,1870.70,aggregate,"Koyeb harness, shape 512x512, batch=32",S1
deepseek-ai/DeepSeek-R1-Distill-Llama-8B,DeepSeek R1 Distill Llama 8B,vLLM,BF16,L40S,1,32,512,512,1116.65,aggregate,"Koyeb harness, shape 512x512, batch=32",S1

meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,H100,8,,,"",6067,aggregate,"Microsoft ‘Chat’ profile; Avg generation throughput",S2
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,H200,8,,,"",7464,aggregate,"Microsoft ‘Chat’ profile; Avg generation throughput",S2
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,A100 80G,8,,,"",2622,aggregate,"Microsoft ‘Chat’ profile; Avg generation throughput",S2
meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,BF16,A100 40G,8,,,"",2459,aggregate,"Microsoft ‘Chat’ profile; Avg generation throughput",S2

meta-llama/Llama-3.3-70B-Instruct,Llama 3.3 70B,TensorRT-LLM,FP8,H100,2,100,500,2000,3322.87,aggregate,"NVIDIA NIM; TP2, concurrency=100",S3

mistralai/Mixtral-8x7B-Instruct,Mixtral 8x7B,TensorRT-LLM,FP8,H100,2,~batch for RPS,,,"62.5",per_user_stream,"Streaming; mean 0.016s/token; ~38.4 RPS",S4

meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,FP16,A40,1,100,10000,~45000,1705.49,aggregate,"DatabaseMart method; 100 concurrent; 100 in / ~450 out",S5

mistralai/Mistral-7B-Instruct-v0.3,Mistral 7B,vLLM,GPTQ-4bit,RTX 4090,1,,,7850.4,aggregate,"Aphrodite engine (vLLM-powered), batch inference; quant=GPTQ 4-bit; concurrency/batch unspecified; URL=https://www.reddit.com/r/LocalLLaMA/comments/1anhz43/why_is_throughput_different_if_identical_gpu/",COMMUNITY

meta-llama/Llama-3.1-8B-Instruct,Llama 3.1 8B,vLLM,FP16,RTX 3090,1,,,1300,aggregate,"vLLM; 100 concurrent requests; worst-case p99 latency ~12.88 tok/s per stream; effective total >1300 tok/s; low token prompt; URL=https://www.reddit.com/r/LocalLLaMA/comments/1ettqkq/a_single_3090_can_serve_llama_3_to_thousands_of/",COMMUNITY
